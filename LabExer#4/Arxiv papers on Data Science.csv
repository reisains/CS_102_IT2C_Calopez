"","title","author","subject","abstract","meta"
"1","Low coordinate degree algorithms I: Universality of computational thresholds for hypothesis testing","Dmitriy Kunisky","Statistics Theory (math.ST)","We study when low coordinate degree functions (LCDF) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. These functions are a generalization, proposed in Hopkins' 2018 thesis but seldom studied since, of low degree polynomials (LDP), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. Instead of the orthogonal polynomial decompositions used in LDP calculations, our analysis of LCDF is based on the Efron-Stein or ANOVA decomposition, making it much more broadly applicable. By way of illustration, we prove channel universality for the success of LCDF in testing for the presence of sufficiently ""dilute"" random signals through noisy channels: the efficacy of LCDF depends on the channel only through the scalar Fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. As applications, we extend lower bounds against LDP for spiked matrix and tensor models under additive Gaussian noise to lower bounds against LCDF under general noisy channels. We also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. These results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps.","Tue, 12 Mar 2024 17:52:35 UTC (56 KB)"
"2","Simplified Tight Bounds for Monotone Minimal Perfect Hashing","Dmitry Kosolobov","Data Structures and Algorithms (cs.DS)","Given an increasing sequence of integers $x_1,\ldots,x_n$ from a universe $\{0,\ldots,u-1\}$, the monotone minimal perfect hash function (MMPHF) for this sequence is a data structure that answers the following rank queries: $rank(x) = i$ if $x = x_i$, for $i\in \{1,\ldots,n\}$, and $rank(x)$ is arbitrary otherwise. Assadi, Farach-Colton, and Kuszmaul recently presented at SODA'23 a proof of the lower bound $\Omega(n \min\{\log\log\log u, \log n\})$ for the bits of space required by MMPHF, provided $u \ge n 2^{2^{\sqrt{\log\log n}}}$, which is tight since there is a data structure for MMPHF that attains this space bound (and answers the queries in $O(\log u)$ time). In this paper, we close the remaining gap by proving that, for $u \ge (1+\epsilon)n$, where $\epsilon > 0$ is any constant, the tight lower bound is $\Omega(n \min\{\log\log\log \frac{u}{n}, \log n\})$, which is also attainable; we observe that, for all reasonable cases when $n < u < (1+\epsilon)n$, known facts imply tight bounds, which virtually settles the problem. Along the way we substantially simplify the proof of Assadi et al. replacing a part of their heavy combinatorial machinery by trivial observations. However, an important part of the proof still remains complicated. This part of our paper repeats arguments of Assadi et al. and is not novel. Nevertheless, we include it, for completeness, offering a somewhat different perspective on these arguments.","Tue, 12 Mar 2024 15:50:00 UTC (472 KB)"
"3","The Minimax Rate of HSIC Estimation for Translation-Invariant Kernels","Florian Kalinke, Zoltan Szabo","Statistics Theory (math.ST)","Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of $M\ge 2$ random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on $\mathbb R^d$ for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal O\!\left(n^{-1/2}\right)$. Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nyström-based one) on $\mathbb R^d$.","Tue, 12 Mar 2024 15:13:21 UTC (24 KB)"
"4","ProPML: Probability Partial Multi-label Learning","Łukasz Struski, Adam Pardyl, Jacek Tabor, Bartosz Zieliński","Machine Learning (cs.LG)","Partial Multi-label Learning (PML) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. In this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the PML setup. In contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. Furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.","Tue, 12 Mar 2024 12:40:23 UTC (1,739 KB)"
"5","Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance","Lijun Chang","Data Structures and Algorithms (cs.DS)","The concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. Although the problem of finding the maximum $k$-defective clique is NP-hard, several practical algorithms have been recently proposed in the literature, with kDC being the state of the art. kDC not only runs the fastest in practice, but also achieves the best time complexity. Specifically, it runs in $O^*(\gamma_k^n)$ time when ignoring polynomial factors; here, $\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $G$. In this paper, we propose the kDC-Two algorithm to improve the time complexity as well as practical performance. kDC-Two runs in $O^*( (\alpha\Delta)^{k+2} \gamma_{k-1}^\alpha)$ time when the maximum $k$-defective clique size $\omega_k(G)$ is at least $k+2$, and in $O^*(\gamma_{k-1}^n)$ time otherwise, where $\alpha$ and $\Delta$ are the degeneracy and maximum degree of $G$, respectively. In addition, with slight modification, kDC-Two also runs in $O^*( (\alpha\Delta)^{k+2} (k+1)^{\alpha+k+1-\omega_k(G)})$ time by using the degeneracy gap $\alpha+k+1-\omega_k(G)$ parameterization; this is better than $O^*( (\alpha\Delta)^{k+2}\gamma_{k-1}^\alpha)$ when $\omega_k(G)$ is close to the degeneracy-based upper bound $\alpha+k+1$. Finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. Extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude.","Tue, 12 Mar 2024 11:49:00 UTC (6,510 KB)"
"6","Controlling Delegations in Liquid Democracy","Shiri Alouf-Heffetz, Tanmay Inamdar, Pallavi Jain, Yash More, Nimrod Talmon","Computer Science and Game Theory (cs.GT)","In liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice. This results in a power structure in which certain agents possess more voting weight than others. As a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy. Here we formalize a certain kind of election control -- in which an external agent may change certain delegation arcs -- and study the computational complexity of the corresponding combinatorial problem.","Tue, 12 Mar 2024 11:42:42 UTC (4,111 KB)"
"7","Shining Light on Periodic Dominating Sets in Bounded-Treewidth Graphs","Jakob Greilhuber, Philipp Schepper, Philip Wellnitz","Data Structures and Algorithms (cs.DS)","For the vertex selection problem $(\sigma,\rho)$-DomSet one is given two fixed sets $\sigma$ and $\rho$ of integers and the task is to decide whether we can select vertices of the input graph, such that, for every selected vertex, the number of selected neighbors is in $\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\rho$. This framework covers Independent Set and Dominating Set for example. We investigate the case when $\sigma$ and $\rho$ are periodic sets with the same period $m\ge 2$, that is, the sets are two (potentially different) residue classes modulo $m$. We study the problem parameterized by treewidth and present an algorithm that solves in time $m^{tw} \cdot n^{O(1)}$ the decision, minimization and maximization version of the problem. This significantly improves upon the known algorithms where for the case $m \ge 3$ not even an explicit running time is known. We complement our algorithm by providing matching lower bounds which state that there is no $(m-\epsilon)^{pw} \cdot n^{O(1)}$ unless SETH fails. For $m = 2$, we extend these bound to the minimization version as the decision version is efficiently solvable.","Tue, 12 Mar 2024 11:01:43 UTC (370 KB)"
"8","Signed graphs in data sciences via communicability geometry","Fernando Diaz-Diaz, Ernesto Estrada","Metric Geometry (math.MG)","Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.","Tue, 12 Mar 2024 10:32:35 UTC (4,581 KB)"
"9","Automated Discovery of Anomalous Features in Ultra-Large Planetary Remote Sensing Datasets using Variational Autoencoders","Adam Lesnikowski, Valentin T. Bickel, Daniel Angerhausen","Earth and Planetary Astrophysics (astro-ph.EP)","The NASA Lunar Reconnaissance Orbiter (LRO) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually. Here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of LRO surface imagery representing the first planetary image anomaly detector. We give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites. Our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains. We publish our code and data along with this paper.","Tue, 12 Mar 2024 09:04:17 UTC (37,291 KB)"
"10","Learning-Augmented Algorithms with Explicit Predictors","Marek Elias, Haim Kaplan, Yishay Mansour, Shay Moran","Machine Learning (cs.LG)","Recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. These approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. In this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). In contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. In particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. Adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting. For each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance. We demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work.","Tue, 12 Mar 2024 08:40:21 UTC (66 KB)"
"11","Satisfiability to Coverage in Presence of Fairness, Matroid, and Global Constraints","Tanmay Inamdar, Pallavi Jain, Daniel Lokshtanov, Abhishek Sahu, Saket Saurabh, Anannya Upasana","Data Structures and Algorithms (cs.DS)","In MaxSAT with Cardinality Constraint problem (CC-MaxSAT), we are given a CNF-formula $\Phi$, and $k \ge 0$, and the goal is to find an assignment $\beta$ with at most $k$ variables set to true (also called a weight $k$-assignment) such that the number of clauses satisfied by $\beta$ is maximized. MaxCov can be seen as a special case of CC-MaxSAT, where the formula $\Phi$ is monotone, i.e., does not contain any negative literals. CC-MaxSAT and MaxCov are extremely well-studied problems in the approximation algorithms as well as parameterized complexity literature. Our first contribution is that the two problems are equivalent to each other in the context of FPT-Approximation parameterized by $k$ (approximation is in terms of number of clauses satisfied/elements covered). We give a randomized reduction from CC-MaxSAT to MaxCov in time $O(1/\epsilon)^{k} \cdot (m+n)^{O(1)}$ that preserves the approximation guarantee up to a factor of $1-\epsilon$. Furthermore, this reduction also works in the presence of fairness and matroid constraints. Armed with this reduction, we focus on designing FPT-Approximation schemes (FPT-ASes) for MaxCov and its generalizations. Our algorithms are based on a novel combination of a variety of ideas, including a carefully designed probability distribution that exploits sparse coverage functions. These algorithms substantially generalize the results in Jain et al. [SODA 2023] for CC-MaxSAT and MaxCov for $K_{d,d}$-free set systems (i.e., no $d$ sets share $d$ elements), as well as a recent FPT-AS for Matroid-Constrained MaxCov by Sellier [ESA 2023] for frequency-$d$ set systems.","Tue, 12 Mar 2024 05:24:16 UTC (94 KB)"
"12","The Primal Pathwidth SETH","Michael Lampis","Computational Complexity (cs.CC)","Motivated by the importance of dynamic programming (DP) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can Dominating Set be solved in time $(3-\epsilon)^{pw}n^{O(1)}$? (where $pw$ is the pathwidth) (ii) can Coloring be solved in time $pw^{(1-\epsilon)pw}n^{O(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\epsilon)k}$? Such questions are well-studied: in some cases the answer is No under the SETH, while in others coarse-grained lower bounds are known under the ETH. Even though questions such as the above seem ""morally equivalent"" as they all ask if a simple DP can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential FPT to XNLP-complete. This paper's main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise. We achieve this by putting forth a natural complexity assumption which we call the Primal Pathwidth-Strong Exponential Time Hypothesis (PP-SETH) and which states that 3-SAT cannot be solved in time $(2-\epsilon)^{pw}n^{O(1)}$, for any $\epsilon>0$, where $pw$ is the pathwidth of the primal graph of the input. We then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the PP-SETH, and hence to each other. This allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the SETH, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the PP-SETH is more plausible than the SETH.","Tue, 12 Mar 2024 01:21:22 UTC (299 KB)"
"13","Noisy Computing of the Threshold Function","Ziao Wang, Nadim Ghaddar, Banghua Zhu, Lele Wang","Data Structures and Algorithms (cs.DS)","Let $\mathsf{TH}_k$ denote the $k$-out-of-$n$ threshold function: given $n$ input Boolean variables, the output is $1$ if and only if at least $k$ of the inputs are $1$. We consider the problem of computing the $\mathsf{TH}_k$ function using noisy readings of the Boolean variables, where each reading is incorrect with some fixed and known probability $p \in (0,1/2)$. As our main result, we show that, when $k = o(n)$, it is both sufficient and necessary to use $$(1 \pm o(1)) \frac{n\log \frac{k}{\delta}}{D_{\mathsf{KL}}(p || 1-p)}$$ queries in expectation to compute the $\mathsf{TH}_k$ function with a vanishing error probability $\delta = o(1)$, where $D_{\mathsf{KL}}(p || 1-p)$ denotes the Kullback-Leibler divergence between $\mathsf{Bern}(p)$ and $\mathsf{Bern}(1-p)$ distributions. In particular, this says that $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{D_{\mathsf{KL}}(p || 1-p)}$ queries in expectation are both sufficient and necessary to compute the $\mathsf{OR}$ and $\mathsf{AND}$ functions of $n$ Boolean variables. Compared to previous work, our result tightens the dependence on $p$ in both the upper and lower bounds.","Tue, 12 Mar 2024 00:35:05 UTC (45 KB)"
"14","Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning","Fuseinin Mumuni, Alhassan Mumuni","Machine Learning (cs.LG)","We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.","Mon, 11 Mar 2024 18:11:00 UTC (2,816 KB)"
"15","Imaging of I Zw 18 by JWST: I. Strategy and First Results of Dusty Stellar Populations","Alec S. Hirschauer, Nicolas Crouzet, Nolan Habel, Laura Lenkić, Conor Nally, Olivia C. Jones, Giacomo Bortolini, Martha L. Boyer, Kay Justtanont Margaret Meixner, Göran Östlin, Gillian S. Wright, Ruyman Azzollini, Joris A. D. L. Blommaert, Bernhard Brandl, Leen Decin, Omnarayani Nayak, Pierre Royer, B. A. Sargent, Paul van der Werf","Astrophysics of Galaxies (astro-ph.GA)","We present a James Webb Space Telescope (JWST) imaging survey of I Zw 18, the archetypal extremely metal-poor, star-forming, blue compact dwarf galaxy. With an oxygen abundance of only $\sim$3% $Z_{\odot}$, it is among the lowest-metallicity systems known in the local universe, and is, therefore, an excellent accessible analog for the galactic building blocks which existed at early epochs of ionization and star formation. These JWST data provide a comprehensive infrared (IR) view of I Zw 18 with eight filters utilizing both NIRCam (F115W, F200W, F356W, and F444W) and MIRI (F770W, F1000W, F1500W, and F1800W) photometry, which we have used to identify key stellar populations that are bright in the near- and mid-IR. These data allow for a better understanding of the origins of dust and dust-production mechanisms in metal-poor environments by characterizing the population of massive, evolved stars in the red supergiant (RSG) and asymptotic giant branch (AGB) phases. In addition, it enables the identification of the brightest dust-enshrouded young stellar objects (YSOs), which provide insight into the formation of massive stars at extremely low metallicities typical of the very early universe. This paper provides an overview of the observational strategy and data processing, and presents first science results, including identifications of dusty AGB star, RSG, and bright YSO candidates. These first results assess the scientific quality of JWST data and provide a guide for obtaining and interpreting future observations of the dusty and evolved stars inhabiting compact dwarf star-forming galaxies in the local universe.","Mon, 11 Mar 2024 17:59:58 UTC (5,251 KB)"
"16","Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?","Egor Zverev, Sahar Abdelnabi, Mario Fritz, Christoph H. Lampert","Machine Learning (cs.LG)","Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed LLMs. Finally, we quantitatively demonstrate that all evaluated LLMs fail to achieve a high amount of separation, according to our measure. The source code and SEP dataset are openly accessible at this https URL.","Mon, 11 Mar 2024 15:48:56 UTC (181 KB)"
"17","SMC-Last Extracted Photometry","T. A. Kuchar, G. C. Sloan, D. R. Mizuno, Kathleen E. Kraemer, M. L. Boyer, Martin A. T. Groenewegen, O. C. Jones, F. Kemper, Iain McDonald, Joana M. Oliveira, Marta Sewiło, Sundar Srinivasan, Jacco Th. van Loon, Albert Zijlstra","Astrophysics of Galaxies (astro-ph.GA)","We present point-source photometry from the Spitzer Space Telescope's final survey of the Small Magellanic Cloud (SMC). We mapped 30 square degrees in two epochs in 2017, with the second extending to early 2018 at 3.6 and 4.5 microns using the Infrared Array Camera. This survey duplicates the footprint from the SAGE-SMC program in 2008. Together, these surveys cover a nearly 10 yr temporal baseline in the SMC. We performed aperture photometry on the mosaicked maps produced from the new data. We did not use any prior catalogs as inputs for the extractor in order to be sensitive to any moving objects (e.g., foreground brown dwarfs) and other transient phenomena (e.g., cataclysmic variables or FU Ori-type eruptions). We produced a point-source catalog with high-confidence sources for each epoch as well as combined-epoch catalog. For each epoch and the combined-epoch data, we also produced a more complete archive with lower-confidence sources. All of these data products will be available to the community at the Infrared Science Archive.","Mon, 11 Mar 2024 14:29:12 UTC (1,596 KB)"
"18","Societal and scientific impact of policy research: A large-scale empirical study of some explanatory factors using Altmetric and Overton","Pablo Dorta-González, Alejandro Rodríguez-Caro, María Isabel Dorta-González","Digital Libraries (cs.DL)","This study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals. We reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public. News and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents. Conversely, articles locked behind paywalls and those published under the full open access model (based on Altmetric data) have a lower chance of being policy-cited. Publication year and policy type show no significant influence. Our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy. Interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it. This highlights the need for improved communication strategies to ensure research informs policy decisions more effectively. This study provides valuable insights for researchers, policymakers, and science communicators. Researchers can tailor their dissemination efforts to reach policymakers through media channels. Policymakers can leverage these findings to identify research with higher policy relevance. Science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities.","Mon, 11 Mar 2024 13:39:46 UTC (1,443 KB)"
"19","Approximating Maximum Edge 2-Coloring by Normalizing Graphs","Tobias Mömke, Alexandru Popa, Aida Roshany-Tabrizi, Michael Ruderer, Roland Vincze","Discrete Mathematics (cs.DM)","In a simple, undirected graph G, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors. The problem maximum edge 2-coloring (ME2C) is to find an edge 2-coloring in a graph G with the goal to maximize the number of colors. For a relevant graph class, ME2C models anti-Ramsey numbers and it was considered in network applications. For the problem a 2-approximation algorithm is known, and if the input graph has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3. It is known that ME2C is APX-hard and that it is UG-hard to obtain an approximation ratio better than 1.5. We show that if the input graph has a perfect matching, there is a polynomial time 1.625-approximation and if the graph is claw-free or if the maximum degree of the input graph is at most three (i.e., the graph is subcubic), there is a polynomial time 1.5-approximation algorithm for ME2C","Mon, 11 Mar 2024 13:05:22 UTC (341 KB)"
"20","Untangling Gaussian Mixtures","Eva Fluck, Sandra Kiefer, Christoph Standke","Statistics Theory (math.ST)","Tangles were originally introduced as a concept to formalize regions of high connectivity in graphs. In recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs. This paper further explores the potential of tangles in data sets as a means for a formal study of clusters. Real-world data often follow a normal distribution. Accounting for this, we develop a quantitative theory of tangles in data sets drawn from Gaussian mixtures. To this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data. We provide explicit conditions under which tangles associated with the marginal Gaussian distributions exist asymptotically almost surely. This can be considered as a sufficient formal criterion for the separabability of clusters in the data.","Mon, 11 Mar 2024 12:42:31 UTC (136 KB)"
"21","Optimal Bounds for Distinct Quartics","Panagiotis Charalampopoulos, Paweł Gawrychowski, Samah Ghazawi","Data Structures and Algorithms (cs.DS)","A fundamental concept related to strings is that of repetitions. It has been extensively studied in many versions, from both purely combinatorial and algorithmic angles. One of the most basic questions is how many distinct squares, i.e., distinct strings of the form $UU$, a string of length $n$ can contain as fragments. It turns out that this is always $\mathcal{O}(n)$, and the bound cannot be improved to sublinear in $n$ [Fraenkel and Simpson, JCTA 1998]. Several similar questions about repetitions in strings have been considered, and by now we seem to have a good understanding of their repetitive structure. For higher-dimensional strings, the basic concept of periodicity has been successfully extended and applied to design efficient algorithms -- it is inherently more complex than for regular strings. Extending the notion of repetitions and understanding the repetitive structure of higher-dimensional strings is however far from complete. Quartics were introduced by Apostolico and Brimkov [TCS 2000] as analogues of squares in two dimensions. Charalampopoulos, Radoszewski, Rytter, Waleń, and Zuba [ESA 2020] proved that the number of distinct quartics in an $n\times n$ 2D string is $\mathcal{O}(n^2 \log^2 n)$ and that they can be computed in $\mathcal{O}(n^2 \log^2 n)$ time. Gawrychowski, Ghazawi, and Landau [SPIRE 2021] constructed an infinite family of $n \times n$ 2D strings with $\Omega(n^2 \log n)$ distinct quartics. This brings the challenge of determining asymptotically tight bounds. Here, we settle both the combinatorial and the algorithmic aspects of this question: the number of distinct quartics in an $n\times n$ 2D string is $\mathcal{O}(n^2 \log n)$ and they can be computed in the worst-case optimal $\mathcal{O}(n^2 \log n)$ time.","Mon, 11 Mar 2024 12:35:21 UTC (1,078 KB)"
"22","Balanced Substructures in Bicolored Graphs","P. S. Ardra, R. Krithika, Saket Saurabh, Roohani Sharma","Data Structures and Algorithms (cs.DS)","An edge-colored graph is said to be balanced if it has an equal number of edges of each color. Given a graph $G$ whose edges are colored using two colors and a positive integer $k$, the objective in the Edge Balanced Connected Subgraph problem is to determine if $G$ has a balanced connected subgraph containing at least $k$ edges. We first show that this problem is NP-complete and remains so even if the solution is required to be a tree or a path. Then, we focus on the parameterized complexity of Edge Balanced Connected Subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter. Towards this, we show that if a graph has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function. We use this result combined with dynamic programming algorithms based on color coding and representative sets to show that Edge Balanced Connected Subgraph and its variants are FPT. Further, using polynomial-time reductions to the Multilinear Monomial Detection problem, we give faster randomized FPT algorithms for the problems. In order to describe these reductions, we define a combinatorial object called relaxed-subgraph. We define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties. This object is defined in the spirit of branching walks known for the Steiner Tree problem and may be of independent interest.","Mon, 11 Mar 2024 10:52:22 UTC (34 KB)"
"23","Arborescences and Shortest Path Trees when Colors Matter","P. S. Ardra, Jasine Babu, Kritika Kashyap, R. Krithika, Sreejith K. Pallathumadam, Deepak Rajendraprasad","Data Structures and Algorithms (cs.DS)","Color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) graph and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color. Some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions. In this work, we study color-constrained arborescences and shortest path trees. Computing a color-constrained shortest path tree on weighted digraphs turns out to be NP-hard in general but polynomial-time solvable when all cycles have positive weight. This polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original graph. While finding color-constrained arborescence of digraphs is NP-hard in general, we give efficient algorithms when the input graph is acyclic. Consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed. Our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight. En route, we sight nice connections to colored matroids and color-constrained bases.","Mon, 11 Mar 2024 10:27:30 UTC (28 KB)"
"24","An Algorithm for Correct Computation of Reeb Spaces for PL Bivariate Fields","Amit Chattopadhyay, Yashwanth Ramamurthi, Osamu Saeki","Computational Geometry (cs.CG)","The Reeb space is a topological structure which is a generalization of the notion of the Reeb graph to multi-fields. Its effectiveness has been established in revealing topological features in data across diverse computational domains which cannot be identified using the Reeb graph or other scalar-topology-based methods. Approximations of Reeb spaces such as the Mapper and the Joint Contour Net have been developed based on quantization of the range. However, computing the topologically correct Reeb space dispensing the range-quantization is a challenging problem. In the current paper, we develop an algorithm for computing a correct net-like approximation corresponding to the Reeb space of a generic piecewise-linear (PL) bivariate field based on a multi-dimensional Reeb graph (MDRG). First, we prove that the Reeb space is homeomorphic to its MDRG. Subsequently, we introduce an algorithm for computing the MDRG of a generic PL bivariate field through the computation of its Jacobi set and Jacobi structure, a projection of the Jacobi set into the Reeb space. This marks the first algorithm for MDRG computation without requiring the quantization of bivariate fields. Following this, we compute a net-like structure embedded in the corresponding Reeb space using the MDRG and the Jacobi structure. We provide the proof of correctness and complexity analysis of our algorithm.","Mon, 11 Mar 2024 10:06:08 UTC (3,655 KB)"
"25","Fun Maximizing Search, (Non) Instance Optimality, and Video Games for Parrots","Jérémy Barbay","Data Structures and Algorithms (cs.DS)","Computerized Adaptive Testing (CAT) measures an examinee's ability while adapting to their level. Both too many questions and too many hard questions can make a test frustrating. Are there some CAT algorithms which can be proven to be theoretically better than others, and in which framework? We show that slightly extending the traditional framework yields a partial order on CAT algorithms. For uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the CAT problem.","Mon, 11 Mar 2024 09:47:21 UTC (3,596 KB)"
"26","FeatAug: Automatic Feature Augmentation From One-to-Many Relationship Tables","Danrui Qi, Weiling Zheng, Jiannan Wang","Machine Learning (cs.LG)","Feature augmentation from one-to-many relationship tables is a critical but challenging problem in ML model development. To augment good features, data scientists need to come up with SQL queries manually, which is time-consuming. Featuretools [1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables. It represents each feature as a group-by aggregation SQL query on relevant tables and can automatically generate these SQL queries. However, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios. To overcome this limitation, we propose FEATAUG, a new feature augmentation framework that automatically extracts predicate-aware SQL queries from one-to-many relationship tables. This extension is not trivial because considering predicates will exponentially increase the number of candidate queries. As a result, the original Featuretools framework, which materializes all candidate queries, will not work and needs to be redesigned. We formally define the problem and model it as a hyperparameter optimization problem. We discuss how the Bayesian Optimization can be applied here and propose a novel warm-up strategy to optimize it. To make our algorithm more practical, we also study how to identify promising attribute combinations for predicates. We show that how the beam search idea can partially solve the problem and propose several techniques to further optimize it. Our experiments on four real-world datasets demonstrate that FeatAug extracts more effective features compared to Featuretools and other baselines. The code is open-sourced at this https URL","Mon, 11 Mar 2024 01:44:14 UTC (1,442 KB)"
"27","Accelerating Sparse Tensor Decomposition Using Adaptive Linearized Representation","Jan Laukemann, Ahmed E. Helal, S. Isaac Geronimo Anderson, Fabio Checconi, Yongseok Soh, Jesmin Jahan Tithi, Teresa Ranadive, Brian J Gravelle, Fabrizio Petrini, Jee Choi","Distributed, Parallel, and Cluster Computing (cs.DC)","High-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis. To quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (TD) methods. However, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures. This study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. Our novel sparse tensor representation, Adaptive Linearized Tensor Order (ALTO), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. To demonstrate the efficacy of ALTO, we accelerate popular TD methods that compute the Canonical Polyadic Decomposition (CPD) model across a range of real-world sparse tensors. Additionally, we characterize the major execution bottlenecks of TD methods on multiple generations of the latest Intel Xeon Scalable processors, including Sapphire Rapids CPUs, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. Across a diverse set of real-world data sets, ALTO outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. Compared to the best mode-specific formats, which require multiple tensor copies, ALTO achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage.","Mon, 11 Mar 2024 00:30:25 UTC (929 KB)"
"28","Improved FPT Approximation Scheme and Approximate Kernel for Biclique-Free Max k-Weight SAT: Greedy Strikes Back","Pasin Manurangsi","Data Structures and Algorithms (cs.DS)","In the Max $k$-Weight SAT (aka Max SAT with Cardinality Constraint) problem, we are given a CNF formula with $n$ variables and $m$ clauses together with a positive integer $k$. The goal is to find an assignment where at most $k$ variables are set to one that satisfies as many constraints as possible. Recently, Jain et al. [SODA'23] gave an FPT approximation scheme (FPT-AS) with running time $2^{O\left(\left(dk/\epsilon\right)^d\right)} \cdot (n + m)^{O(1)}$ for Max $k$-Weight SAT when the incidence graph is $K_{d,d}$-free. They asked whether a polynomial-size approximate kernel exists. In this work, we answer this question positively by giving an $(1 - \epsilon)$-approximate kernel with $\left(\frac{d k}{\epsilon}\right)^{O(d)}$ variables. This also implies an improved FPT-AS with running time $(dk/\epsilon)^{O(dk)} \cdot (n + m)^{O(1)}$. Our approximate kernel is based mainly on a couple of greedy strategies together with a sunflower lemma-style reduction rule.","Sun, 10 Mar 2024 22:54:06 UTC (17 KB)"
"29","Revisiting Path Contraction and Cycle Contraction","R. Krithika, V. K. Kutty Malu, Prafullkumar Tale","Data Structures and Algorithms (cs.DS)","The Path Contraction and Cycle Contraction problems take as input an undirected graph $G$ with $n$ vertices, $m$ edges and an integer $k$ and determine whether one can obtain a path or a cycle, respectively, by performing at most $k$ edge contractions in $G$. We revisit these NP-complete problems and prove the following results. Path Contraction admits an algorithm running in $\mathcal{O}^*(2^{k})$ time. This improves over the current algorithm known for the problem [Algorithmica 2014]. Cycle Contraction admits an algorithm running in $\mathcal{O}^*((2 + \epsilon_{\ell})^k)$ time where $0 < \epsilon_{\ell} \leq 0.5509$ is inversely proportional to $\ell = n - k$. Central to these results is an algorithm for a general variant of Path Contraction, namely, Path Contraction With Constrained Ends. We also give an $\mathcal{O}^*(2.5191^n)$-time algorithm to solve the optimization version of Cycle Contraction. Next, we turn our attention to restricted graph classes and show the following results. Path Contraction on planar graphs admits a polynomial-time algorithm. Path Contraction on chordal graphs does not admit an algorithm running in time $\mathcal{O}(n^{2-\epsilon} \cdot 2^{o(tw)})$ for any $\epsilon > 0$, unless the Orthogonal Vectors Conjecture fails. Here, $tw$ is the treewidth of the input graph. The second result complements the $\mathcal{O}(nm)$-time, i.e., $\mathcal{O}(n^2 \cdot tw)$-time, algorithm known for the problem [Discret. Appl. Math. 2014].","Sun, 10 Mar 2024 19:20:11 UTC (309 KB)"
"30","Aqueous Solution Chemistry In Silico and the Role of Data Driven Approaches","Debarshi Banerjee, Khatereh Azizi, Colin K. Egan, Edward Danquah Donkor, Cesare Malosso, Solana Di Pino, Gonzalo Diaz Miron, Martina Stella, Giulia Sormani, Germaine Neza Hozana, Marta Monti, Uriel N. Morzan, Alex Rodriguez, Giuseppe Cassone, Asja Jelic, Damian Scherlis, Ali Hassanali","Chemical Physics (physics.chem-ph)","The use of computer simulations to study the properties of aqueous systems is, today more than ever, an active area of research. In this context, during the last decade there has been a tremendous growth in the use of data-driven approaches to develop more accurate potentials for water as well as to characterize its complexity in chemical and biological contexts. We highlight the progress, giving a historical context, on the path to the development of many-body and reactive potentials to model aqueous chemistry, including the role of machine learning strategies. We focus specifically on conceptual and methodological challenges along the way in performing simulations that seek to tackle problems in modeling the chemistry of aqueous solutions. In conclusion, we summarize our perspectives on the use and integration of advanced data-science techniques to provide chemical insights in physical chemistry and how this will influence computer simulations of aqueous systems in the future.","Sun, 10 Mar 2024 15:25:53 UTC (19,296 KB)"
"31","Steering a Fleet: Adaptation for Large-Scale, Workflow-Based Experiments","Jim Pruyne, Valerie Hayot-Sasson, Weijian Zheng, Ryan Chard, Justin M. Wozniak, Tekin Bicer, Kyle Chard, Ian T. Foster","Distributed, Parallel, and Cluster Computing (cs.DC)","Experimental science is increasingly driven by instruments that produce vast volumes of data and thus a need to manage, compute, describe, and index this data. High performance and distributed computing provide the means of addressing the computing needs; however, in practice, the variety of actions required and the distributed set of resources involved, requires sophisticated ""flows"" defining the steps to be performed on data. As each scan or measurement is performed by an instrument, a new instance of the flow is initiated resulting in a ""fleet"" of concurrently running flows, with the overall goal to process all the data collected during a potentially long-running experiment. During the course of the experiment, each flow may need to adapt its execution due to changes in the environment, such as computational or storage resource availability, or based on the progress of the fleet as a whole such as completion or discovery of an intermediate result leading to a change in subsequent flow's behavior. We introduce a cloud-based decision engine, Braid, which flows consult during execution to query their run-time environment and coordinate with other flows within their fleet. Braid accepts streams of measurements taken from the run-time environment or from within flow runs which can then be statistically aggregated and compared to other streams to determine a strategy to guide flow execution. For example, queue lengths in execution environments can be used to direct a flow to run computations in one environment or another, or experiment progress as measured by individual flows can be aggregated to determine the progress and subsequent direction of the flows within a fleet. We describe Braid, its interface, implementation and performance characteristics. We further show through examples and experience modifying an existing scientific flow how Braid is used to make adaptable flows.","Sun, 10 Mar 2024 03:46:13 UTC (12,799 KB)"
"32","Hierarchical Query Classification in E-commerce Search","Bing He, Sreyashi Nag, Limeng Cui, Suhang Wang, Zheng Li, Rahul Goutam, Zhen Li, Haiyang Zhang","Information Retrieval (cs.IR)","E-commerce platforms typically store and structure product information and search data in a hierarchy. Efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. The significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. The inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. To address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ''label hierarchy''. Additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. Extensive experiments demonstrate that our proposed method is better than state-of-the-art (SOTA) on the proprietary Amazon dataset, and comparable to SOTA on the public datasets of Web of Science and RCV1-V2. These results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.","Sat, 9 Mar 2024 21:55:55 UTC (207 KB)"
"33","ROME/REA: Three-year, Tri-color Timeseries Photometry of the Galactic Bulge","R.A. Street, E. Bachelet, Y. Tsapras, M.P.G. Hundertmark, V. Bozza, D.M. Bramich, A. Cassan, M. Dominik, R. Figuera Jaimes, K. Horne, S. Mao, A. Saha, J. Wambsganss, Weicheng Zang","Astrophysics of Galaxies (astro-ph.GA)","The ROME/REA (Robotic Observations of Microlensing Events/Reactive Event Assessment) Survey was a Key Project at Las Cumbres Observatory (hereafter LCO) which continuously monitored 20 selected fields (3.76 sq.deg.) in the Galactic Bulge throughout their seasonal visibility window over a three-year period, between March 2017 and March 2020. Observations were made in three optical passbands (SDSS-g', -r', -i'), and LCO's multi-site telescope network enabled the survey to achieve a typical cadence of $\sim$10\,hrs in i' and ~15 hrs in g' and r'. In addition, intervals of higher cadence (<1 hr) data were obtained during monitoring of key microlensing events within the fields. This paper describes the Difference Image Analysis data reduction pipeline developed to process these data, and the process for combining the photometry from LCO's three observing sites in the Southern Hemisphere. The full timeseries photometry for all 8 million stars, down to a limiting magnitude of i~18 mag is provided in the data release accompanying this paper, and samples of the data are presented for exemplar microlensing events, illustrating how the tri-band data are used to derive constraints on the microlensing source star parameters, a necessary step in determining the physical properties of the lensing object. The timeseries data also enables a wealth of additional science, for example in characterizing long-timescale stellar variability, and a few examples of the data for known variables are presented.","Sat, 9 Mar 2024 19:02:38 UTC (28,582 KB)"
"34","Hamiltonicity, Path Cover, and Independence Number: An FPT Perspective","Fedor V. Fomin, Petr A. Golovach, Danil Sagunov, Kirill Simonov","Data Structures and Algorithms (cs.DS)","The connection between Hamiltonicity and the independence numbers of graphs has been a fundamental aspect of Graph Theory since the seminal works of the 1960s. This paper presents a novel algorithmic perspective on these classical problems. Our contributions are twofold. First, we establish that a wide array of problems in undirected graphs, encompassing problems such as Hamiltonian Path and Cycle, Path Cover, Largest Linkage, and Topological Minor Containment are fixed-parameter tractable (FPT) parameterized by the independence number of a graph. To the best of our knowledge, these results mark the first instances of FPT problems for such parameterization. Second, we extend the algorithmic scope of the Gallai-Milgram theorem. The original theorem by Gallai and Milgram, asserts that for a graph G with the independence number \alpha(G), the vertex set of G can be covered by at most \alpha(G) vertex-disjoint paths. We show that determining whether a graph can be covered by fewer than \alpha(G) - k vertex-disjoint paths is FPT parameterized by k. Notably, the independence number parameterization, which describes graph's density, departs from the typical flow of research in parameterized complexity, which focuses on parameters describing graph's sparsity, like treewidth or vertex cover.","Sat, 9 Mar 2024 15:48:43 UTC (37 KB)"
"35","Approximate Bipartite $b$-Matching using Multiplicative Auction","Bhargav Samineni, S M Ferdous, Mahantesh Halappanavar, Bala Krishnamoorthy","Data Structures and Algorithms (cs.DS)","Given a bipartite graph $G(V= (A \cup B),E)$ with $n$ vertices and $m$ edges and a function $b \colon V \to \mathbb{Z}_+$, a $b$-matching is a subset of edges such that every vertex $v \in V$ is incident to at most $b(v)$ edges in the subset. When we are also given edge weights, the Max Weight $b$-Matching problem is to find a $b$-matching of maximum weight, which is a fundamental combinatorial optimization problem with many applications. Extending on the recent work of Zheng and Henzinger (IPCO, 2023) on standard bipartite matching problems, we develop a simple auction algorithm to approximately solve Max Weight $b$-Matching. Specifically, we present a multiplicative auction algorithm that gives a $(1 - \varepsilon)$-approximation in $O(m \varepsilon^{-1} \log \varepsilon^{-1} \log \beta)$ worst case time, where $\beta$ the maximum $b$-value. Although this is a $\log \beta$ factor greater than the current best approximation algorithm by Huang and Pettie (Algorithmica, 2022), it is considerably simpler to present, analyze, and implement.","Sat, 9 Mar 2024 03:28:31 UTC (688 KB)"
"36","Scalable $k$-clique Densest Subgraph Search","Xiaowei Ye, Miao Qiao, Rong-Hua Li, Qi Zhang, Guoren Wang","Data Structures and Algorithms (cs.DS)","In this paper, we present a collection of novel and scalable algorithms designed to tackle the challenges inherent in the $k$-clique densest subgraph problem (\kcdsp) within network analysis. We propose \psctl, a novel algorithm based on the Frank-Wolfe approach for addressing \kcdsp, effectively solving a distinct convex programming problem. \textcolor{black}{\psctl is able to approximate \kcdsp with near optimal guarantees.} The notable advantage of \psctl lies in its time complexity, which is independent of the count of $k$-cliques, resulting in remarkable efficiency in practical applications. Additionally, we present \spath, a sampling-based algorithm with the capability to handle networks on an unprecedented scale, reaching up to $1.8\times 10^9$ edges. By leveraging the \ccpath algorithm as a uniform $k$-clique sampler, \spath ensures the efficient processing of large-scale network data, accompanied by a detailed analysis of accuracy guarantees. Together, these contributions represent a significant advancement in the field of $k$-clique densest subgraph discovery. In experimental evaluations, our algorithms demonstrate orders of magnitude faster performance compared to the current state-of-the-art solutions.","Sat, 9 Mar 2024 03:08:29 UTC (35,692 KB)"
"37","Systematic assessment of various universal machine-learning interatomic potentials","Haochen Yu, Matteo Giantomassi, Giuliana Materzanini, Gian-Marco Rignanese","Materials Science (cond-mat.mtrl-sci)","Machine-learning interatomic potentials have revolutionized materials modeling at the atomic scale. Thanks to these, it is now indeed possible to perform simulations of \abinitio quality over very large time and length scales. More recently, various universal machine-learning models have been proposed as an out-of-box approach avoiding the need to train and validate specific potentials for each particular material of interest. In this paper, we review and evaluate five different universal machine-learning interatomic potentials (uMLIPs), all based on graph neural network architectures which have demonstrated transferability from one chemical system to another. The evaluation procedure relies on data both from a recent verification study of density-functional-theory implementations and from the Materials Project. Through this comprehensive evaluation, we aim to provide guidance to materials scientists in selecting suitable models for their specific research problems, offer recommendations for model selection and optimization, and stimulate discussion on potential areas for improvement in current machine-learning methodologies in materials science.","Fri, 8 Mar 2024 23:53:20 UTC (5,104 KB)"
"38","$\mathtt{tsGT}$: Stochastic Time Series Modeling With Transformer","Łukasz Kuciński, Witold Drzewakowski, Mateusz Olko, Piotr Kozakowski, Łukasz Maziarka, Marta Emilia Nowakowska, Łukasz Kaiser, Piotr Miłoś","Machine Learning (cs.LG)","Time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. Recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. In this paper, we go in a different direction by introducing $\mathtt{tsGT}$, a stochastic time series model built on a general-purpose transformer architecture. We focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. We show that $\mathtt{tsGT}$ outperforms the state-of-the-art models on MAD and RMSE, and surpasses its stochastic peers on QL and CRPS, on four commonly used datasets. We complement these results with a detailed analysis of $\mathtt{tsGT}$'s ability to model the data distribution and predict marginal quantile values.","Fri, 8 Mar 2024 22:59:41 UTC (1,482 KB)"
"39","Dynamic Convex Hulls for Simple Paths","Bruce Brewer, Gerth Stølting Brodal, Haitao Wang","Computational Geometry (cs.CG)","We consider the planar dynamic convex hull problem. In the literature, solutions exist supporting the insertion and deletion of points in poly-logarithmic time and various queries on the convex hull of the current set of points in logarithmic time. If arbitrary insertion and deletion of points are allowed, constant time updates and fast queries are known to be impossible. This paper considers two restricted cases where worst-case constant time updates and logarithmic time queries are possible. We assume all updates are performed on a deque (double-ended queue) of points. The first case considers the monotonic path case, where all points are sorted in a given direction, say horizontally left-to-right, and only the leftmost and rightmost points can be inserted and deleted. The second case assumes that the points in the deque constitute a simple path. Note that the monotone case is a special case of the simple path case. For both cases, we present solutions supporting deque insertions and deletions in worst-case constant time and standard queries on the convex hull of the points in $O(\log n)$ time, where $n$ is the number of points in the current point set. The convex hull of the current point set can be reported in $O(h+\log n)$ time, where $h$ is the number of edges of the convex hull. For the 1-sided monotone path case, where updates are only allowed on one side, the reporting time can be reduced to $O(h)$, and queries on the convex hull are supported in $O(\log h)$ time. All our time bounds are worst case. In addition, we prove lower bounds that match these time bounds, and thus our results are optimal. For a quick comparison, the previous best update bounds for the simple path problem were amortized $O(\log n)$ time by Friedman, Hershberger, and Snoeyink [SoCG 1989].","Fri, 8 Mar 2024 22:11:00 UTC (351 KB)"
"40","A VLBI Software Correlator for Fast Radio Transients","Calvin Leung, Shion Andrew, Kiyoshi W. Masui, Charanjot Brar, Tomas Cassanelli, Shami Chatterjee, Victoria Kaspi, Kholoud Khairy, Adam E. Lanman, Mattias Lazda, Juan Mena-Parra, Gavin Noble, Aaron B. Pearlman, Mubdi Rahman, Pranav Sanghavi","Instrumentation and Methods for Astrophysics (astro-ph.IM)","One major goal in fast radio burst science is to detect fast radio bursts (FRBs) over a wide field of view without sacrificing the angular resolution required to pinpoint them to their host galaxies. Wide-field detection and localization capabilities have already been demonstrated using connected-element interferometry; the CHIME/FRB Outriggers project will push this further using widefield cylindrical telescopes as widefield outriggers for very long baseline interferometry (VLBI). This paper describes an offline VLBI software correlator written in Python for the CHIME/FRB Outriggers project. It includes features well-suited to modern widefield instruments like multibeaming/multiple phase center correlation, pulse gating including coherent dedispersion, and a novel correlation algorithm based on the quadratic estimator formalism. This algorithm mitigates sensitivity loss which arises in instruments where the windowing and channelization is done outside the VLBI correlator at each station, which accounts for a 30 percent sensitivity drop away from the phase center. Our correlation algorithm recovers this sensitivity on both simulated and real data. As an end to end check of our software, we have written a preliminary pipeline for VLBI calibration and single-pulse localization, which we use in Lanman et al. (2024) to verify the astrometric accuracy of the CHIME/FRB Outriggers array.","Fri, 8 Mar 2024 19:11:39 UTC (7,856 KB)"
"41","An Intelligent Assistive System Based on Augmented Reality and Internet of Things for Patients with Alzheimer's Disease","Fatemeh Ghorbani","Human-Computer Interaction (cs.HC)","Independent life of the individuals suffering from Alzheimer's disease (AD) is compromised due to their memory loss. As a result, they depend on others to help them lead their daily life. In this situation, either the family members or the caregivers offer their help; they attach notes on every single object or take out the contents of a drawer to make those visible when they leave the patient alone. The aim of this thesis is to provide multi-level support and some helping means for AD patients and their family members through the integration of existing science and methods. This study reports results on an intelligent assistive (IA) system, achieved through the integration of Internet of Things (IoT), augmented reality (AR), and adaptive fuzzy decision-making methods. The proposed system has four main components; (1) a location and heading data stored in the local fog layer, (2) an AR device to make interactions with the AD patient, (3) a supervisory decision-maker to handle the direct and environmental interactions with the patient, (4) and a user interface for family or caregivers to monitor the patient's real-time situation and send reminders once required. The system operates in different modes, including automated and semi-automated. The first one helps the user complete the activities in their daily life by showing AR messages or making automatic changes. The second one allows manual changes after the real-time assessment of the user's cognitive state based on the AR game score. We provide further evidence that the accuracy, reliability and response time of the IA system are appropriate to be implemented in AD patients' homes. Moreover, the system response in the semi-automated mode causes less data loss than the automated mode, as the number of active devices decreases.","Wed, 21 Feb 2024 15:29:20 UTC (2,901 KB)"
"42","Understanding the Progression of Educational Topics via Semantic Matching","Tamador Alkhidir (1), Edmond Awad (2), Aamena Alshamsi (3) ((1) Curriculum Department, Ministry of Education, United Arab Emirates, (2) Department of Economics and Institute for Data Science and AI, University of Exeter, United Kingdom,(3) Heuristic World, Dubai, United Arab Emirates)","Computers and Society (cs.CY)","Education systems are dynamically changing to accommodate technological advances, industrial and societal needs, and to enhance students' learning journeys. Curriculum specialists and educators constantly revise taught subjects across educational grades to identify gaps, introduce new learning topics, and enhance the learning outcomes. This process is usually done within the same subjects (e.g. math) or across related subjects (e.g. math and physics) considering the same and different educational levels, leading to massive multi-layer comparisons. Having nuanced data about subjects, topics, and learning outcomes structured within a dataset, empowers us to leverage data science to better understand the progression of various learning topics. In this paper, Bidirectional Encoder Representations from Transformers (BERT) topic modeling was used to extract topics from the curriculum, which were then used to identify relationships between subjects, track their progression, and identify conceptual gaps. We found that grouping learning outcomes by common topics helped specialists reduce redundancy and introduce new concepts in the curriculum. We built a dashboard to avail the methodology to curriculum specials. Finally, we tested the validity of the approach with subject matter experts.","Sat, 10 Feb 2024 08:24:29 UTC (8,213 KB)"
"43","Nurses as agents for achieving Environmentally Sustainable Health Systems: A bibliometric analysis","Olga Maria Luque Alcaraz, Pilar Aparicio-Martínez, Antonio Gomera, Manuel Vaquero-Abellán","Computers and Society (cs.CY)","Objective: To analyze the current scientific knowledge and research lines focused on environmentally sustainable health systems, including the role of nurses. Background: There seem to be differences between creating interventions focused on environmentally sustainable health systems, including nurses, and the scarcity of research on this topic, framed on the Sustainable Development Goals. Methods: A bibliometric analysis was carried out, via three databases (Web of Science, Scopus, and Pubmed), and the guideline recommendations were followed to select bibliometric data. Results: The search resulted in 159 publications, significantly increasing the trends from 2017 to 2021 (p=0.028). The most relevant countries in this area were the United States of America, the United Kingdom, and Sweden. Also, the top articles were from relevant journals, indexed in Journal Citation Report, and the first and the second quartile linked to the nursing field and citations (p<0.001). Conclusion: Education is key to achieving environmentally sustainable health systems via institutions and policies. Implications for nursing management: There is a lack of experimental data and policies on achieving or maintaining environmentally sustainable health care systems, indicating that nurses have an important role and should be consulted and included in decision-making policies regarding sustainability in the healthcare systems.","Mon, 5 Feb 2024 12:14:04 UTC (4,234 KB)"
"44","The NIRSpec Wide GTO Survey","Michael V. Maseda, Anna de Graaff, Marijn Franx, Hans-Walter Rix, Stefano Carniani, Isaac Laseter, Ugne Dudzeviciute, Tim Rawle, Eleonora Parlanti, Santiago Arribas, Andrew J. Bunker, Alex J. Cameron, Stephane Charlot, Mirko Curti, Francesco D'Eugenio, Gareth C. Jones, Nimisha Kumari, Roberto Maiolino, Hannah Uebler, Aayush Saxena, Renske Smit, Chris Willott, Joris Witstok","Astrophysics of Galaxies (astro-ph.GA)","The Near-infrared Spectrograph (NIRSpec) on the James Webb Space Telescope is uniquely suited to studying galaxies in the distant Universe with its combination of multi-object capabilities and sensitivity over a large range in wavelength (0.6-5.3 microns). Here we present the NIRSpec Wide survey, part of the NIRSpec Instrument Science Team's Guaranteed Time Observations, using NIRSpec's microshutter array to obtain spectra of more than 3200 galaxies at $z>1$ at both low- and high-resolution ($R\approx100$ and 2700) for a total of 105 hours. With 31 pointings covering $\approx$320 arcmin$^2$ across the five CANDELS fields with exquisite ancillary photometry from the Hubble Space Telescope, the NIRSpec Wide survey represents a fast and efficient way of using JWST to probe galaxies in the early Universe. Pointing centers are determined to maximize the observability of the rarest, high-value sources. Subsequently, the microshutter configurations are optimized to observe the maximum number of ""census"" galaxies with a selection function based primarily on HST/F160W magnitude, photometric/slitless grism redshift, and predicted \ha\ flux tracing the bulk of the galaxy population at cosmic noon ($z_{\rm med}=2.0$). We present details on the survey strategy, the target selection, an outline of the motivating science cases, and discuss upcoming public data releases to the community.","Fri, 8 Mar 2024 18:26:38 UTC (3,200 KB)"
"45","Sparse dynamic network reconstruction through L1-regularization of a Lyapunov equation","Ian Xul Belaustegui, Marcela Ordorica Arango, Román Rossi-Pool, Naomi Ehrich Leonard, Alessio Franci","Systems and Control (eess.SY)","An important problem in many areas of science is that of recovering interaction networks from simultaneous time-series of many interacting dynamical processes. A common approach is to use the elements of the correlation matrix or its inverse as proxies of the interaction strengths, but the reconstructed networks are necessarily undirected. Transfer entropy methods have been proposed to reconstruct directed networks but the reconstructed network lacks information about interaction strengths. We propose a network reconstruction method that inherits the best of the two approaches by reconstructing a directed weighted network from noisy data under the assumption that the network is sparse and the dynamics are governed by a linear (or weakly-nonlinear) stochastic dynamical system. The two steps of our method are i) constructing an (infinite) family of candidate networks by solving the covariance matrix Lyapunov equation for the state matrix and ii) using L1-regularization to select a sparse solution. We further show how to use prior information on the (non)existence of a few directed edges to drastically improve the quality of the reconstruction.","Fri, 8 Mar 2024 17:07:37 UTC (2,163 KB)[v2] Tue, 12 Mar 2024 21:24:30 UTC (2,147 KB)"
"46","Advances of Deep Learning in Protein Science: A Comprehensive Survey","Bozhen Hu, Cheng Tan, Lirong Wu, Jiangbin Zheng, Jun Xia, Zhangyang Gao, Zicheng Liu, Fandi Wu, Guijun Zhang, Stan Z. Li","Biomolecules (q-bio.BM)","Protein representation learning plays a crucial role in understanding the structure and function of proteins, which are essential biomolecules involved in various biological processes. In recent years, deep learning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and representations from large-scale protein data. This comprehensive survey aims to provide an overview of the recent advances in deep learning techniques applied to protein science. The survey begins by introducing the developments of deep learning based protein models and emphasizes the importance of protein representation learning in drug discovery, protein engineering, and function annotation. It then delves into the fundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention models, and graph neural networks in modeling protein sequences, structures, and functions, and explores how these techniques can be used to extract meaningful features and capture intricate relationships within protein data. Next, the survey presents various applications of deep learning in the field of proteins, including protein structure prediction, protein-protein interaction prediction, protein function prediction, etc. Furthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses potential solutions and future directions for overcoming these challenges. This comprehensive survey provides a valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing the power of deep learning techniques. By consolidating the latest advancements and discussing potential avenues for improvement, this review contributes to the ongoing progress in protein research and paves the way for future breakthroughs in the field.","Fri, 8 Mar 2024 13:45:32 UTC (4,315 KB)"
"47","Interoperability of the Metaverse: A Digital Ecosystem Perspective Review","Liang Yang, Shi-Ting Ni, Yuyang Wang, Ao Yu, Jyh-An Lee, Pan Hui","Computers and Society (cs.CY)","The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. Interoperability, recognized as a major barrier to the Metaverse's full potential, is central to this debate. CoinMarketCap's report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent. Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process. Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem. Urs Gasser's framework from Harvard Law School, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. Incorporating this framework, we dissect literature for a comprehensive Metaverse interoperability overview. Our study seeks to establish benchmarks for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement.","Fri, 8 Mar 2024 10:38:01 UTC (7,107 KB)"
"48","Efficient Algorithms for Personalized PageRank Computation: A Survey","Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, Ji-Rong Wen","Data Structures and Algorithms (cs.DS)","Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes $s$ and $t$, the PPR value $\pi_s(t)$ equals the probability that an $\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way. As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.","Fri, 8 Mar 2024 10:19:34 UTC (12,011 KB)"
"49","Putting Language into Context Using Smartphone-Based Keyboard Logging","Florian Bemmann, Timo Koch, Maximilian Bergmann, Clemens Stachl, Daniel Buschek, Ramona Schoedel, Sven Mayer","Human-Computer Interaction (cs.HC)","While the study of language as typed on smartphones offers valuable insights, existing data collection methods often fall short in providing contextual information and ensuring user privacy. We present a privacy-respectful approach - context-enriched keyboard logging - that allows for the extraction of contextual information on the user's input motive, which is meaningful for linguistics, psychology, and behavioral sciences. In particular, with our approach, we enable distinguishing language contents by their channel (i.e., comments, messaging, search inputs). Filtering by channel allows for better pre-selection of data, which is in the interest of researchers and improves users' privacy. We demonstrate our approach on a large-scale six-month user study (N=624) of language use in smartphone interactions in the wild. Finally, we highlight the implications for research on language use in human-computer interaction and interdisciplinary contexts.","Fri, 8 Mar 2024 09:41:01 UTC (3,282 KB)"
"50","Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem","Ceyao Zhang, Renjie Li, Cheng Zhang, Zhaoyu Zhang, Feng Yin","Applied Physics (physics.app-ph)","Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. Advanced AI technologies, especially reinforcement learning (RL), have emerged as a powerful tool to augment and accelerate this inverse design process. By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch. However, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of RL approaches. Recently, sequential models, especially the Transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models. In this paper, we introduce a novel framework named PCSEL Inverse Design Transformer (PiT) that abstracts the inverse design of PCSEL as a sequence modeling problem. The central part of our PiT is a Transformer-based structure that leverages the past trajectories and current states to predict the current actions. Compared with the traditional RL approaches, PiT can output the optimal actions and achieve target PCSEL designs by leveraging offline data and conditioning on the desired return. Results demonstrate that PiT achieves superior performance and data efficiency compared to baselines.","Fri, 8 Mar 2024 08:38:50 UTC (2,445 KB)"
"51","Improving the open cluster census. III. Using cluster masses, radii, and dynamics to create a cleaned open cluster catalogue","Emily L. Hunt, Sabine Reffert","Astrophysics of Galaxies (astro-ph.GA)","The census of open clusters has exploded in size thanks to data from the Gaia satellite. However, it is likely that many of these reported clusters are not gravitationally bound, making the open cluster census impractical for many scientific applications. We test different physically motivated methods for distinguishing between bound and unbound clusters, using them to create a cleaned cluster catalogue. We derived completeness-corrected photometric masses for 6956 clusters from our earlier work. Then, we used these masses to compute the size of the Roche surface of these clusters (their Jacobi radius) and distinguish between bound and unbound clusters. We find that only 5647 (79%) of the clusters from our previous catalogue are compatible with bound open clusters, dropping to just 11% of clusters within 250 pc. 3530 open clusters are in a strongly cut high quality sample. The moving groups in our sample show different trends in their size as a function of age and mass, suggesting that they are unbound and undergoing different dynamical processes. Our cluster mass measurements constitute the largest catalogue of Milky Way cluster masses to date, which we also use for further science. Firstly, we inferred the mass-dependent completeness limit of the open cluster census, showing that the census is complete within 1.8 kpc only for objects heavier than 230 M$_\odot$. Next, we derived a completeness-corrected age and mass function for our open cluster catalogue, including estimating that the Milky Way contains a total of $1.3 \times 10^5$ open clusters, only ~4% of which are currently known. Finally, we show that most open clusters have mass functions compatible with the Kroupa initial mass function. We demonstrate Jacobi radii for distinguishing between bound and unbound star clusters, and publish an updated star cluster catalogue with masses and improved cluster classifications. (abridged)","Fri, 8 Mar 2024 08:27:23 UTC (4,952 KB)"
"52","Single Family Algebra Operation on ZDDs Leads To Exponential Blow-Up","Kengo Nakamura, Masaaki Nishino, Shuhei Denzumi","Data Structures and Algorithms (cs.DS)","Zero-suppressed binary decision diagram (ZDD) is a data structure to represent a family of (sub)sets compactly, and it can be used as a succinct index for a family of sets. To build ZDD representing a desired family of sets, there are many transformation operations that take ZDDs as inputs and output ZDD representing the resultant family after performing operations such as set union and intersection. However, except for some basic operations, the worst-time complexity of taking such transformation on ZDDs has not been extensively studied, and some contradictory statements about it have arisen in the literature. In this paper, we show that many transformation operations on ZDDs cannot be performed in worst-case polynomial time with respect to the size of input ZDDs. This refutes some of the folklore circulated in past literature and resolves an open problem raised by Knuth. Our results are stronger in that such blow-up of computational time occurs even when the ordering, which has a significant impact on the efficiency of treating ZDDs, is reasonable.","Fri, 8 Mar 2024 05:57:12 UTC (371 KB)"
"53","Data-Dependent LSH for the Earth Mover's Distance","Rajesh Jayaram, Erik Waingarten, Tian Zhang","Data Structures and Algorithms (cs.DS)","We give new data-dependent locality sensitive hashing schemes (LSH) for the Earth Mover's Distance ($\mathsf{EMD}$), and as a result, improve the best approximation for nearest neighbor search under $\mathsf{EMD}$ by a quadratic factor. Here, the metric $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ consists of sets of $s$ vectors in $\mathbb{R}^d$, and for any two sets $x,y$ of $s$ vectors the distance $\mathsf{EMD}(x,y)$ is the minimum cost of a perfect matching between $x,y$, where the cost of matching two vectors is their $\ell_p$ distance. Previously, Andoni, Indyk, and Krauthgamer gave a (data-independent) locality-sensitive hashing scheme for $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ when $p \in [1,2]$ with approximation $O(\log^2 s)$. By being data-dependent, we improve the approximation to $\tilde{O}(\log s)$. Our main technical contribution is to show that for any distribution $\mu$ supported on the metric $\mathsf{EMD}_s(\mathbb{R}^d, \ell_p)$, there exists a data-dependent LSH for dense regions of $\mu$ which achieves approximation $\tilde{O}(\log s)$, and that the data-independent LSH actually achieves a $\tilde{O}(\log s)$-approximation outside of those dense regions. Finally, we show how to ""glue"" together these two hashing schemes without any additional loss in the approximation. Beyond nearest neighbor search, our data-dependent LSH also gives optimal (distributional) sketches for the Earth Mover's Distance. By known sketching lower bounds, this implies that our LSH is optimal (up to $\mathrm{poly}(\log \log s)$ factors) among those that collide close points with constant probability.","Fri, 8 Mar 2024 04:35:55 UTC (136 KB)"
"54","A basic lower bound for property testing","Eldar Fischer","Data Structures and Algorithms (cs.DS)","An $\epsilon$-test for any non-trivial property (one for which there are both satisfying inputs and inputs of large distance from the property) should use a number of queries that is at least inversely proportional in $\epsilon$. However, to the best of our knowledge there is no reference proof for this intuition. Such a proof is provided here. It is written so as to not require any prior knowledge of the related literature, and in particular does not use Yao's method.","Fri, 8 Mar 2024 02:27:59 UTC (6 KB)"
"55","NP-Completeness for the Space-Optimality of Double-Array Tries","Hideo Bannai, Keisuke Goto, Shunsuke Kanda, Dominik Köppl","Data Structures and Algorithms (cs.DS)","Indexing a set of strings for prefix search or membership queries is a fundamental task with many applications such as information retrieval or database systems. A classic abstract data type for modelling such an index is a trie. Due to the fundamental nature of this problem, it has sparked much interest, leading to a variety of trie implementations with different characteristics. A trie implementation that has been well-used in practice is the double-array (trie) consisting of merely two integer arrays. While a traversal takes constant time per node visit, the needed space consumption in computer words can be as large as the product of the number of nodes and the alphabet size. Despite that several heuristics have been proposed on lowering the space requirements, we are unaware of any theoretical guarantees. In this paper, we study the decision problem whether there exists a double-array of a given size. To this end, we first draw a connection to the sparse matrix compression problem, which makes our problem NP-complete for alphabet sizes linear to the number of nodes. We further propose a reduction from the restricted directed Hamiltonian path problem, leading to NP-completeness even for logarithmic-sized alphabets.","Thu, 7 Mar 2024 23:41:01 UTC (787 KB)"
"56","A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets","Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset","Robotics (cs.RO)","This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower bounds for the MT-TSP than the ones from the MICP.","Thu, 7 Mar 2024 22:03:36 UTC (963 KB)[v2] Mon, 11 Mar 2024 03:47:04 UTC (403 KB)"
"57","Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data","Tracey Oellerich, Maria Emelianenko","Numerical Analysis (math.NA)","Conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. If the form of the underlying dynamical system is known, linear algebra and algebraic geometry methods can be used to identify the conservation laws. Our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. Building in part upon the ideas proposed in [arXiv:1811.00961], we develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. We demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. While we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.","Thu, 7 Mar 2024 20:48:18 UTC (1,551 KB)"
"58","Improved Lower Bound for Differentially Private Facility Location","Pasin Manurangsi","Data Structures and Algorithms (cs.DS)","We consider the differentially private (DP) facility location problem in the so called super-set output setting proposed by Gupta et al. [SODA 2010]. The current best known expected approximation ratio for an $\epsilon$-DP algorithm is $O\left(\frac{\log n}{\sqrt{\epsilon}}\right)$ due to Cohen-Addad et al. [AISTATS 2022] where $n$ denote the size of the metric space, meanwhile the best known lower bound is $\Omega(1/\sqrt{\epsilon})$ [NeurIPS 2019]. In this short note, we give a lower bound of $\tilde{\Omega}\left(\min\left\{\log n, \sqrt{\frac{\log n}{\epsilon}}\right\}\right)$ on the expected approximation ratio of any $\epsilon$-DP algorithm, which is the first evidence that the approximation ratio has to grow with the size of the metric space.","Thu, 7 Mar 2024 19:47:20 UTC (10 KB)"
"59","Corrective or Backfire: Characterizing and Predicting User Response to Social Correction","Bing He, Yingchen Ma, Mustaque Ahamad, Srijan Kumar","Social and Information Networks (cs.SI)","Online misinformation poses a global risk with harmful implications for society. Ordinary social media users are known to actively reply to misinformation posts with counter-misinformation messages, which is shown to be effective in containing the spread of misinformation. Such a practice is defined as ""social correction"". Nevertheless, it remains unknown how users respond to social correction in real-world scenarios, especially, will it have a corrective or backfire effect on users. Investigating this research question is pivotal for developing and refining strategies that maximize the efficacy of social correction initiatives. To fill this gap, we conduct an in-depth study to characterize and predict the user response to social correction in a data-driven manner through the lens of X (Formerly Twitter), where the user response is instantiated as the reply that is written toward a counter-misinformation message. Particularly, we first create a novel dataset with 55, 549 triples of misinformation tweets, counter-misinformation replies, and responses to counter-misinformation replies, and then curate a taxonomy to illustrate different kinds of user responses. Next, fine-grained statistical analysis of reply linguistic and engagement features as well as repliers' user attributes is conducted to illustrate the characteristics that are significant in determining whether a reply will have a corrective or backfire effect. Finally, we build a user response prediction model to identify whether a social correction will be corrective, neutral, or have a backfire effect, which achieves a promising F1 score of 0.816. Our work enables stakeholders to monitor and predict user responses effectively, thus guiding the use of social correction to maximize their corrective impact and minimize backfire effects. The code and data is accessible on this https URL.","Thu, 7 Mar 2024 19:06:42 UTC (603 KB)"
"60","Specifying and Verifying the Convergence Stairs of the Collatz Program","Ali Ebnenasir","Discrete Mathematics (cs.DM)","This paper presents an algorithmic method that, given a positive integer $j$, generates the $j$-th convergence stair containing all natural numbers from where the Collatz conjecture holds by exactly $j$ applications of the Collatz function. To this end, we present a novel formulation of the Collatz conjecture as a concurrent program, and provide the general case specification of the $j$-th convergence stair for any $j > 0$. The proposed specifications provide a layered and linearized orientation of Collatz numbers organized in an infinite set of infinite binary trees. To the best of our knowledge, this is the first time that such a general specification is provided, which can have significant applications in analyzing and testing the behaviors of complex non-linear systems. We have implemented this method as a software tool that generates the Collatz numbers of individual stairs. We also show that starting from any value in any convergence stair the conjecture holds. However, to prove the conjecture, one has to show that every natural number will appear in some stair; i.e., the union of all stairs is equal to the set of natural numbers, which remains an open problem.","Thu, 29 Feb 2024 17:36:54 UTC (949 KB)"
"61","SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions","Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun","Machine Learning (cs.LG)","We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only. Our result naturally generalizes to the setting of a hidden subspace. Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.","Thu, 7 Mar 2024 18:49:32 UTC (32 KB)"
"62","A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation","Ankit Pensia","Data Structures and Algorithms (cs.DS)","We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean $\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly}(k,\log d,1/\epsilon)$ samples. We also provide analogous results for robust sparse PCA. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant.","Thu, 7 Mar 2024 18:23:51 UTC (46 KB)"
"63","mmPlace: Robust Place Recognition with Intermediate Frequency Signal of Low-cost Single-chip Millimeter Wave Radar","Chengzhen Meng, Yifan Duan, Chenming He, Dequan Wang, Xiaoran Fan, Yanyong Zhang","Robotics (cs.RO)","Place recognition is crucial for tasks like loop-closure detection and re-localization. Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments. However, it encounters two challenges. Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data. Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV). We propose mmPlace, a robust place recognition system to address these challenges. Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features. Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV. We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage. The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations.","Thu, 7 Mar 2024 17:53:37 UTC (8,296 KB)"
"64","On $[1,2]$-Domination in Interval and Circle Graphs","Mohsen Alambardar Meybodi, Abolfazl Poureidi","Computational Complexity (cs.CC)","A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.","Thu, 7 Mar 2024 17:43:21 UTC (523 KB)"
"65","Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation","Palak Jain, Adam Smith, Connor Wagaman","Data Structures and Algorithms (cs.DS)","We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms. We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.","Thu, 7 Mar 2024 16:14:08 UTC (155 KB)"
"66","Optimizing Inventory Placement for a Downstream Online Matching Problem","Boris Epstein, Will Ma (Columbia University)","Data Structures and Algorithms (cs.DS)","We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform. We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement. On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate. We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss. On the experimental side, we extract real-world sequences of customer orders from publicly-available this http URL data and evaluate different combinations of placement and fulfillment procedures. Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory.","Thu, 7 Mar 2024 15:46:19 UTC (508 KB)"
"67","Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?","Dibyayan Chakraborty, Antoine Dailly, Florent Foucaud, Ralf Klasing","Data Structures and Algorithms (cs.DS)","In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs. A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.","Thu, 7 Mar 2024 15:35:36 UTC (34 KB)"
"68","A Simple and Near-Optimal Algorithm for Directed Expander Decompositions","Aurelio L. Sulser, Maximilian Probst Gutenberg","Data Structures and Algorithms (cs.DS)","In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times. Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors. At the same time, our algorithm is much simpler and more accessible than previous work. In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019). We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework.","Thu, 7 Mar 2024 14:40:07 UTC (47 KB)"
"69","Conflict and Fairness in Resource Allocation","Susobhan Bandopadhyay, Aritra Banik, Sushmita Gupta, Pallavi Jain, Abhishek Sahu, Saket Saurabh, Prafullkumar Tale","Computer Science and Game Theory (cs.GT)","In the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents' welfare is maximized. Motivated by job scheduling, interest in this problem dates back to the work of Deuermeyer et al. [SIAM J. on Algebraic Discrete Methods'82]. Recent works consider the compatibility between resources and assign only mutually compatible resources to an agent. We study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\it conflict graph} on the set of resources (where an edge denotes incompatibility). The goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources. Chiarelli et al. [Algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \NP-hard. In this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters.","Thu, 7 Mar 2024 06:57:20 UTC (352 KB)"
"70","Switching Classes: Characterization and Computation","Dhanyamol Antony, Yixin Cao, Sagartanu Pal, R.B. Sandeep","Data Structures and Algorithms (cs.DS)","In a graph, the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary graph class $\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $\mathcal{G}$, and prove that it is finite when $\mathcal{G}$ is minor-closed and omits at least one graph. For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.","Thu, 7 Mar 2024 06:55:42 UTC (52 KB)"
"71","Equivalence Testing: The Power of Bounded Adaptivity","Diptarka Chakraborty, Sourav Chakraborty, Gunjan Kumar, Kuldeep S. Meel","Data Structures and Algorithms (cs.DS)","Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance. Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester). Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization. Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\tilde{O}(\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage. Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity. Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\tilde{O}(\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing.","Thu, 7 Mar 2024 05:23:06 UTC (30 KB)"
"72","FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning","Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu","Machine Learning (cs.LG)","Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.","Thu, 7 Mar 2024 01:52:05 UTC (2,187 KB)"
"73","Optimal Scheduling of Graph States via Path Decompositions","Samuel J. Elman, Jason Gavriel, Ryan L. Mann","Quantum Physics (quant-ph)","We study the optimal scheduling of graph states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of graphs. We define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width. Our analysis shows that approximating the spatial cost of a graph is $\textsf{NP}$-hard, while for graphs with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule.","Thu, 7 Mar 2024 00:50:02 UTC (13 KB)"
"74","Understanding Biology in the Age of Artificial Intelligence","Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro Liò, Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig","Artificial Intelligence (cs.AI)","Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.","Wed, 6 Mar 2024 23:20:34 UTC (1,634 KB)"
"75","Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection","Jianfeng He, Hang Su, Jason Cai, Igor Shalyminov, Hwanjun Song, Saab Mansour","Computation and Language (cs.CL)","Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at \url{this https URL}.","Wed, 6 Mar 2024 22:06:23 UTC (1,350 KB)"
"76","A Sierpinski Triangle Data Structure for Efficient Array Value Update and Prefix Sum Calculation","Brent Harrison, Jason Necaise, Andrew Projansky, James D. Whitfield","Data Structures and Algorithms (cs.DS)","The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array. It allows both of these operations to be performed in $O(\log_2 N)$ time. Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\log_3 N)$ time instead. We show this order to be optimal by making use of a connection to quantum computing.","Wed, 6 Mar 2024 19:01:04 UTC (170 KB)"
"77","On Outer Bi-Lipschitz Extensions of Linear Johnson-Lindenstrauss Embeddings of Subsets of $\mathbb{R}^N$","Rafael Chiclana, Mark A. Iwen, Mark Philip Roach","Metric Geometry (math.MG)","The celebrated Johnson-Lindenstrauss lemma states that for all $\varepsilon \in (0,1)$ and finite sets $X \subseteq \mathbb{R}^N$ with $n>1$ elements, there exists a matrix $\Phi \in \mathbb{R}^{m \times N}$ with $m=\mathcal{O}(\varepsilon^{-2}\log n)$ such that \[ (1 - \varepsilon) \|x-y\|_2 \leq \|\Phi x-\Phi y\|_2 \leq (1+\varepsilon)\| x- y\|_2 \quad \forall\, x, y \in X.\] Herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the Johnson-Lindenstrauss lemma for finite sets. After a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $X \subseteq \mathbb{R}^N$, and then specialize our generalized results to the case where $X$ is a low-dimensional compact submanifold of $\mathbb{R}^N$. In particular, we prove the following generalization of the Johnson-Lindenstrauss lemma: For all $\varepsilon \in (0,1)$ and $X\subseteq\mathbb{R}^N$, there exists a terminal embedding $f: \mathbb{R}^N \longrightarrow \mathbb{R}^{m}$ such that $$(1 - \varepsilon) \| x - y \|_2 \leq \left\| f(x) - f(y) \right\|_2 \leq (1 + \varepsilon) \| x - y \|_2 \quad \forall \, x \in X ~{\rm and}~ \forall \, y \in \mathbb{R}^N.$$ Crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=\mathcal{O}(\varepsilon^{-2} \omega^2(S_X))$, where $\omega(S_X)$ is the Gaussian width of the set of unit secants of $X$, $S_X=\overline{\{(x-y)/\|x-y\|_2 \colon x \neq y \in X\}}$. Furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice.","Wed, 6 Mar 2024 18:04:45 UTC (1,133 KB)"
"78","On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps","Emily Clark, Chloe Georgiou, Katelyn Poon, Marek Chrobak","Data Structures and Algorithms (cs.DS)","In his 2018 paper, Herlihy introduced an atomic protocol for multi-party asset swaps across different blockchains. His model represents an asset swap by a directed graph whose nodes are the participating parties and edges represent asset transfers, and rational behavior of the participants is captured by a preference relation between a protocol's outcomes. Asset transfers between parties are achieved using smart contracts. These smart contracts are quite involved and they require storage and processing of a large number of paths in the swap digraph, limiting practical significance of his protocol. His paper also describes a different protocol that uses only standard hash time-lock contracts (HTLC's), but this simpler protocol applies only to some special types of digraphs. He left open the question whether there is a simple and efficient protocol for cross-chain asset swaps in arbitrary digraphs. Motivated by this open problem, we conducted a comprehensive study of \emph{HTLC-based protocols}, in which all asset transfers are implemented with HTLCs. Our main contribution is a full characterization of swap digraphs that have such protocols.","Wed, 6 Mar 2024 18:08:01 UTC (421 KB)"
"79","Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications","Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit Pensia, Kevin Tian","Numerical Analysis (math.NA)","The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood. Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.","Wed, 6 Mar 2024 18:07:20 UTC (91 KB)[v2] Thu, 7 Mar 2024 02:12:55 UTC (91 KB)"
"80","Digitality as a ""longue durèe"" historical phenomenon","Salvatore Spina","Computers and Society (cs.CY)","The digital age introduced the Digital Ecological Niche (DEN), revolutionizing human interactions. The advent of Digital History (DHy) has marked a methodological shift in historical studies, tracing its roots to Babbage and Lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" This evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by Babbage and Lovelace's contributions to mathematical linguistic systems, laying the groundwork for Computer Science. This field, central to 20th-century mainframe interaction through programming languages and formalization, situates Digital History within a broader historical context. Here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. Nonetheless, the extent to which computation and Turing machines can fully understand and interpret history remains a subject of debate.","Wed, 6 Mar 2024 17:18:37 UTC (344 KB)"
"81","Parameterized Algorithms for Balanced Cluster Edge Modification Problems","Jayakrishnan Madathil, Kitty Meeks","Data Structures and Algorithms (cs.DS)","We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity. A graph $G$ is a cluster graph if every connected component of $G$ is a clique. In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a graph $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges. In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster graph should not exceed a given threshold. Depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems. We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels. Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-Means Clustering.","Wed, 6 Mar 2024 16:19:35 UTC (72 KB)"
"82","In the Search of Optimal Tree Networks: Hardness and Heuristics","Maxim Buzdalov, Pavel Martynov, Sergey Pankratov, Vitaly Aksenov, Stefan Schmid","Networking and Internet Architecture (cs.NI)","Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve. These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters. In this work, we consider networks with one of the most common topologies~ -- a binary tree. We show that finding an optimal demand-aware binary tree network is NP-hard. Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads.","Wed, 6 Mar 2024 14:15:01 UTC (140 KB)"
"83","Largest common subgraph of two forests","Dieter Rautenbach, Florian Werner","Data Structures and Algorithms (cs.DS)","A common subgraph of two graphs $G_1$ and $G_2$ is a graph that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given graphs $G_1$ and $G_2$ that is of maximum possible size ${\rm lcs}(G_1,G_2)$. This natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths. We present a simple $4$-approximation algorithm for forests, and, for every fixed $\epsilon\in (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${\rm lcs}(F_1,F_2)-\epsilon n$ edges. Restricted to instances with ${\rm lcs}(F_1,F_2)\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(\log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions.","Wed, 6 Mar 2024 13:25:41 UTC (18 KB)"
"84","Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training","Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas","Cryptography and Security (cs.CR)","Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.","Wed, 6 Mar 2024 10:25:36 UTC (1,049 KB)"
"85","Studying ECG signals using nonlinear oscillators and Genetic Algorithm","Sourav Chowdhury, Apratim Ghosal, Suparna Roychowhury, Indranath Chaudhuri","Medical Physics (physics.med-ph)","Cardiovascular diseases are the leading cause of death and disability in the world and thus their detection is extremely important as early as possible so that it can be prognosed and managed appropriately. Hence, electrophysiological models dealing with cardiac conduction are critically important in the field of interdisciplinary sciences. The primary aim of this paper is to reproduce a normal sinus rhythm ECG waveform which will act as the baseline for fitting and then fit any clinical ECG waveform that does not deviate much from normal sinus rhythm. To reproduce the ECG, we modeled the pacemaker complex using three coupled van der Pol (VDP) oscillators with appropriate delays to generate the action potentials. These action potentials are responsible for the excitation of the non-pacemaker cells of the atria and ventricles whose electrical activity gets recorded as the ECG signal. The ECG signal is composed of a periodic set of individual waves corresponding to atrial and ventricular contraction and relaxation. These waves are modeled with the help of four FitzHugh-Nagumo (FHN) equations with impulses corresponding to the action potentials generated by the pacemaker cells. After the successful reproduction of a normal sinus rhythm ECG, we have developed a framework where we have used genetic algorithm (GA) to fit a given clinical ECG data with parameters belonging to the above mentioned system of delay differential equations (DDEs). The GA framework has enabled us to fit ECG data representing different cardiac conditions reasonably well. We aim to use this work to get a better understanding of the cardiac conduction system and cardiovascular diseases which will help humanity in the future.","Wed, 6 Mar 2024 10:11:39 UTC (7,566 KB)"
"86","Graph Visualization for Blockchain Data","Marcell Dietl, Andre Gemünd, Daniel Oeltz, Felix M. Thiele, Christian Werner","Data Structures and Algorithms (cs.DS)","In this report, we introduce a novel approach to visualize extremely large graphs efficiently. Our method combines two force-directed algorithms, Kamada-Kawai and ForceAtlas2, to handle different graph components based on their node count. Additionally, we suggest utilizing the Fast Multipole method to enhance the speed of ForceAtlas2. Although initially designed for analyzing bitcoin transaction graphs, for which we present results here, this algorithm can also be applied to other crypto currency transaction graphs or graphs from diverse domains.","Wed, 6 Mar 2024 07:19:53 UTC (15,983 KB)"
"87","Double Exponential Lower Bound for Telephone Broadcast","Prafullkumar Tale","Data Structures and Algorithms (cs.DS)","Consider the Telephone Broadcast problem in which an input is a connected graph $G$ on $n$ vertices, a source vertex $s \in V(G)$, and a positive integer $t$. The objective is to decide whether there is a broadcast protocol from $s$ that ensures that all the vertices of $G$ get the message in at most $t$ rounds. We consider the broadcast protocol where, in a round, any node aware of the message can forward it to at most one of its neighbors. As the number of nodes aware of the message can at most double at each round, for a non-trivial instance we have $n \le 2^t$. Hence, the brute force algorithm that checks all the permutations of the vertices runs in time $2^{2^{\calO(t)}} \cdot n^{\calO(1)}$. As our first result, we prove this simple algorithm is the best possible in the following sense. Telephone Broadcast does not admit an algorithm running in time $2^{2^{o(t)}} \cdot n^{\calO(1)}$, unless the Ð fails. To the best of our knowledge, this is only the fourth example of \NP-Complete problem that admits a double exponential lower bound when parameterized by the solution size. It also resolves the question by Fomin, Fraigniaud, and Golovach [WG 2023]. In the same article, the authors asked whether the problem is \FPT\ when parameterized by the feedback vertex set number of the graph. We answer this question in the negative. Telephone Broadcast, when restricted to graphs of the feedback vertex number one, and hence treewidth of two, is \NP-\complete. We find this a relatively rare example of problems that admit a polynomial-time algorithm on trees but is \NP-\complete\ on graphs of treewidth two.","Wed, 6 Mar 2024 07:08:38 UTC (2,424 KB)"
"88","Undergraduate data science education: Who has the microphone and what are they saying?","Mine Dogucu, Sinem Demirci, Harry Bendekgey, Federica Zoe Ricci, Catalina M. Medina","Other Statistics (stat.OT)","The presence of data science has been profound in the scientific community in almost every discipline. An important part of the data science education expansion has been at the undergraduate level. We conducted a systematic literature review to (1) specify current evidence and knowledge gaps in undergraduate data science education and (2) inform policymakers and data science educators/practitioners about the present status of data science education research. The majority of the publications in data science education that met our search criteria were available open-access. Our results indicate that data science education research lacks empirical data and reproducibility. Not all disciplines contribute equally to the field of data science education. Computer science and data science as a separate field emerge as the leading contributors to the literature. In contrast, fields such as statistics, mathematics, as well as other fields closely related to data science exhibit a limited presence in studies. We recommend that federal agencies and researchers 1) invest in empirical data science education research; 2) diversify research efforts to enrich the spectrum of types of studies; 3) encourage scholars in key data science fields that are currently underrepresented in the literature to contribute more to research and publications.","Wed, 6 Mar 2024 00:49:08 UTC (49 KB)"
"89","Fine-Grained Privacy Guarantees for Coverage Problems","Laxman Dhulipala, George Z. Li","Data Structures and Algorithms (cs.DS)","We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under differential privacy. In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application. Our main result is an $\epsilon$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(k/\epsilon))$-approximation with high probability. Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\Omega(k/\epsilon)$ is necessary under edge-differential privacy. Via group privacy properties, this implies a new algorithm for $\epsilon$-node differentially private Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(fk/\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. When $f\ll k$, this improves over the best known algorithm for Max Cover under pure (node) differential privacy, which obtains an $(1-1/e,\tilde{O}(k^2/\epsilon))$-approximation.","Tue, 5 Mar 2024 21:40:10 UTC (50 KB)"
"90","Deep Configuration Performance Learning: A Systematic Survey and Taxonomy","Jingzhi Gong, Tao Chen","Software Engineering (cs.SE)","Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: this https URL.","Tue, 5 Mar 2024 21:05:16 UTC (1,311 KB)"
"91","Maintaining Light Spanners via Minimal Updates","Hadi Khodabandeh, David Eppstein","Computational Geometry (cs.CG)","We study the problem of maintaining a lightweight bounded-degree $(1+\varepsilon)$-spanner of a dynamic point set in a $d$-dimensional Euclidean space, where $\varepsilon>0$ and $d$ are arbitrary constants. In our fully-dynamic setting, points are allowed to be inserted as well as deleted, and our objective is to maintain a $(1+\varepsilon)$-spanner that has constant bounds on its maximum degree and its lightness (the ratio of its weight to that of the minimum spanning tree), while minimizing the recourse, which is the number of edges added or removed by each point insertion or deletion. We present a fully-dynamic algorithm that handles point insertion with amortized constant recourse and point deletion with amortized $O(\log\Delta)$ recourse, where $\Delta$ is the aspect ratio of the point set.","Tue, 5 Mar 2024 19:45:11 UTC (27 KB)"
"92","Shuffling Momentum Gradient Algorithm for Convex Optimization","Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen","Optimization and Control (math.OC)","The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.","Tue, 5 Mar 2024 18:19:02 UTC (395 KB)"
"93","Jovian sodium nebula and Io plasma torus S$^+$ and brightnesses 2017 -- 2023: insights into volcanic vs. sublimation supply","Jeffrey P. Morgenthaler (1), Carl A. Schmidt (2), Marissa F. Vogt (1), Nicholas M. Schneider (3), Max Marconi (1) ((1) Planetary Science Institute, (2) Center for Space Physics Boston University, (3) University Of Colorado, Boulder)","Earth and Planetary Astrophysics (astro-ph.EP)","We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date. The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations. Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state. The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape. We suggest that geologic activity, likely involving volcanic plumes, drives escape.","Tue, 5 Mar 2024 17:16:37 UTC (1,423 KB)"
"94","The Exchange Problem","Mohit Garg, Suneel Sarswat","Data Structures and Algorithms (cs.DS)","Auctions are widely used in exchanges to match buy and sell requests. Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. In this work, we study the algorithmic complexity of the problems arising from these matching tasks. We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\log n)$ time to match $n$ requests. For dynamic price matching, we establish a lower bound of $\Omega(n \log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.","Tue, 5 Mar 2024 15:30:24 UTC (20 KB)"
"95","Cover Edge-Based Novel Triangle Counting","David A. Bader, Fuhuan Li, Zhihui Du, Palina Pauliuchenka, Oliver Alvarado Rodriguez, Anant Gupta, Sai Sri Vastav Minnal, Valmik Nahata, Anya Ganeshan, Ahmet Gundogdu, Jason Lew","Data Structures and Algorithms (cs.DS)","Listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality. In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set. Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the Graph500 Benchmark and the MIT/Amazon/IEEE Graph Challenge. We implement 22 sequential algorithms for performance evaluation and comparison. At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs. In our estimate from massive-scale Graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x. Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.","Tue, 5 Mar 2024 14:24:48 UTC (546 KB)"
"96","Space Complexity of Euclidean Clustering","Xiaoyi Zhu, Yuxiang Tian, Lingxiao Huang, Zengfeng Huang","Computational Geometry (cs.CG)","The $(k, z)$-Clustering problem in Euclidean space $\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\varepsilon$, remains unclear in existing literature. This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.","Tue, 5 Mar 2024 13:49:32 UTC (1,557 KB)[v2] Wed, 6 Mar 2024 02:05:36 UTC (797 KB)"
"97","Hamiltonian Property Testing","Andreas Bluhm, Matthias C. Caro, Aadil Oufkir","Quantum Physics (quant-ph)","Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2^{n/2})$ many queries and $\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.","Tue, 5 Mar 2024 13:44:28 UTC (276 KB)"
"98","Improving the quality of individual-level online information tracking: challenges of existing approaches and introduction of a new content- and long-tail sensitive academic solution","Silke Adam, Mykola Makhortykh, Michaela Maier, Viktor Aigenseer, Aleksandra Urman, Teresa Gil Lopez, Clara Christner, Ernesto de León, Roberto Ulloa","Computers and Society (cs.CY)","This article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues. To overcome some of these problems, the article introduces a new academic tracking solution, WebTrack, an open source tracking tool maintained by a major European research institution. The design logic, the interfaces and the backend requirements for WebTrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed. Finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through WebTrack leads to new innovative shifts in the processing of tracking data. As WebTrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news.","Tue, 5 Mar 2024 12:49:25 UTC (422 KB)"
"99","DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting","Hao Wu, Haomin Wen, Guibin Zhang, Yutong Xia, Kai Wang, Yuxuan Liang, Yu Zheng, Kun Wang","Artificial Intelligence (cs.AI)","The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.","Tue, 5 Mar 2024 12:31:24 UTC (7,643 KB)"
"100","Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects","Kinga Skorupska, Rafał Stryjek, Izabela Wierzbowska, Piotr Bebas, Maciej Grzeszczuk, Piotr Gago, Jarosław Kowalski, Maciej Krzywicki, Jagoda Lazarek, Wiesław Kopeć","Human-Computer Interaction (cs.HC)","Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.","Tue, 5 Mar 2024 12:13:27 UTC (6,621 KB)"
"101","A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails","Amit Attia, Tomer Koren","Machine Learning (cs.LG)","This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.","Tue, 5 Mar 2024 11:38:20 UTC (11 KB)"
"102","HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents","Sam Yu-Te Lee, Kwan-Liu Ma","Human-Computer Interaction (cs.HC)","Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.","Tue, 5 Mar 2024 08:15:57 UTC (7,662 KB)"
"103","Neural Fractional Differential Equations","C. Coelho, M. Fernanda P. Costa, L.L. Ferrás","Machine Learning (cs.LG)","Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours. This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions. Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data. This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.","Tue, 5 Mar 2024 07:45:29 UTC (629 KB)"
"104","Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators","Yehonatan Fridman, Guy Tamir, Uri Steinitz, Gal Oren","Distributed, Parallel, and Cluster Computing (cs.DC)","Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations. This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs. We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases.","Tue, 5 Mar 2024 07:43:24 UTC (80 KB)[v2] Tue, 12 Mar 2024 04:46:37 UTC (1 KB) (withdrawn)"
"105","DGAP: Efficient Dynamic Graph Analysis on Persistent Memory","Abdullah Al Raqibul Islam, Dong Dai","Data Structures and Algorithms (cs.DS)","Dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. To accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices. Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together. In light of this, we propose DGAP, a framework for efficient dynamic graph analysis on persistent memory. Unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) graph structure with new designs for persistent memory to construct the framework. Specifically, DGAP introduces a \textit{per-section edge log} to reduce write amplification on persistent memory; a \textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\times$ better graph update performance and up to $3.77\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as XPGraph, LLAMA, and GraphOne.","Tue, 5 Mar 2024 05:40:45 UTC (5,442 KB)"
"106","The Influence of Validation Data on Logical and Scientific Interpretations of Forensic Expert Opinions","Steven P. Lund, Hari Iyer","Applications (stat.AP)","Forensic experts use specialized training and knowledge to enable other members of the judicial system to make better informed and more just decisions. Factfinders, in particular, are tasked with judging how much weight to give to experts' reports and opinions. Many references describe assessing evidential weight from the perspective of a forensic expert. Some recognize that stakeholders are each responsible for evaluating their own weight of evidence. Morris (1971, 1974, 1977) provided a general framework for recipients to update their own uncertainties after learning an expert's opinion. Although this framework is normative under Bayesian axioms and several forensic scholars advocate the use of Bayesian reasoning, few resources describe its application in forensic science. This paper addresses this gap by examining how recipients can combine principles of science and Bayesian reasoning to evaluate their own likelihood ratios for expert opinions. This exercise helps clarify how an expert's role depends on whether one envisions recipients to be logical and scientific or deferential. Illustrative examples with an expert's opinion expressed as a categorical conclusion, likelihood ratio, or range of likelihood ratios, or with likelihood ratios from multiple experts, each reveal the importance and influence of validation data for logical recipients' interpretations.","Tue, 5 Mar 2024 05:22:05 UTC (2,352 KB)"
"107","Algorithms for Galois Words: Detection, Factorization, and Rotation","Diptarama Hendrian, Dominik Köppl, Ryo Yoshinaka, Ayumi Shinohara","Data Structures and Algorithms (cs.DS)","Lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [Bannai+, SIAM J. Comput.'17]. We can determine Lyndon words, factorize a word into Lyndon words in lexicographically decreasing order, and find the Lyndon rotation of a word, all in linear time within constant additional working space. A recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of Lyndon words. In particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed. While a Lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a Galois word exhibits the same property by exchanging the lexicographic order with the alternating order. Unfortunately, this exchange has a large impact on the properties Galois words exhibit, which makes it a nontrivial task to translate results from Lyndon words to Galois words. Up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of Duval's algorithm are possible for computing the Galois factorization or the Galois rotation. Here, we affirm this conjecture as follows. Given a word $T$ of length $n$, we can determine whether $T$ is a Galois word, in $O(n)$ time with constant additional working space. Within the same complexities, we can also determine the Galois rotation of $T$, and compute the Galois factorization of $T$ online. The last result settles Open Problem~1 in [Dolce et al., TCS'2019] for Galois words.","Tue, 5 Mar 2024 03:59:20 UTC (202 KB)"
"108","On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication","Yang P. Liu","Data Structures and Algorithms (cs.DS)","We study connections between the problem of fully dynamic $(1-\epsilon)$-approximate maximum bipartite matching, and the dual $(1+\epsilon)$-approximate vertex cover problem, with the online matrix-vector ($\mathsf{OMv}$) conjecture which has recently been used in several fine-grained hardness reductions. We prove that there is an online algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $n^{1-c}\epsilon^{-C}$ time for constants $c, C > 0$ for fully dynamic updates if and only if the $\mathsf{OMv}$ conjecture is false. Similarly, we prove that there is an online algorithm that maintains a $(1-\epsilon)$-approximate maximum matching in amortized $n^{1-c}\epsilon^{-C}$ time if and only if there is a nontrivial algorithm for another dynamic problem, which we call dynamic approximate $\mathsf{OMv}$, that has seemingly no matching structure. This provides some evidence against achieving amortized sublinear update times for approximate fully dynamic matching and vertex cover. Leveraging these connections, we obtain faster algorithms for approximate fully dynamic matching in both the online and offline settings. 1. We give a randomized algorithm that with high probability maintains a $(1-\epsilon)$-approximate bipartite matching and $(1+\epsilon)$-approximate vertex cover in fully dynamic graphs, in amortized $O(\epsilon^{-O(1)} \frac{n}{2^{\Omega(\sqrt{\log n})}})$ update time. Our algorithm leverages fast algorithms for $\mathsf{OMv}$ due to Larsen-Williams [SODA 2017]. 2. We give a randomized offline algorithm for $(1-\epsilon)$-approximate maximum matching with amortized runtime $O(n^{.58}\epsilon^{-O(1)})$ by using fast matrix multiplication, significantly improving over the runtimes achieved via online algorithms. We also give an offline algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $O(n^{.723}\epsilon^{-O(1)})$ time.","Tue, 5 Mar 2024 01:33:09 UTC (39 KB)"
"109","Born Accessible Data Science and Visualization Courses: Challenges of Developing Curriculum to be Taught by Blind Instructors to Blind Students","JooYoung Seo, Sile O'Modhrain, Yilin Xia, Sanchita Kamath, Bongshin Lee, James M. Coughlan","Human-Computer Interaction (cs.HC)","While recent years have seen a growing interest in accessible visualization tools and techniques for blind people, little attention is paid to the learning opportunities and teaching strategies of data science and visualization tailored for blind individuals. Whereas the former focuses on the accessibility issues of data visualization tools, the latter is concerned with the learnability of concepts and skills for data science and visualization. In this paper, we present novel approaches to teaching data science and visualization to blind students in an online setting. Taught by blind instructors, nine blind learners having a wide range of professional backgrounds participated in a two-week summer course. We describe the course design, teaching strategies, and learning outcomes. We also discuss the challenges and opportunities of teaching data science and visualization to blind students. Our work contributes to the growing body of knowledge on accessible data science and visualization education, and provides insights into the design of online courses for blind students.","Tue, 5 Mar 2024 00:49:13 UTC (1,278 KB)"
"110","A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing","Yu Wang","Computation and Language (cs.CL)","The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.","Mon, 4 Mar 2024 21:51:11 UTC (1,091 KB)"
"111","End-to-end variational quantum sensing","Benjamin MacLellan, Piotr Roztocki, Stefanie Czischek, Roger G. Melko","Quantum Physics (quant-ph)","Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering. Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging. Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice. Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively. The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ansätze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation. End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors.","Mon, 4 Mar 2024 19:00:04 UTC (428 KB)"
"112","nimCSO: A Nim package for Compositional Space Optimization","Adam M. Krajewski, Arindam Debnath, Wesley F. Reinhart, Allison M. Beese, Zi-Kui Liu","Materials Science (cond-mat.mtrl-sci)","nimCSO is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning. Making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. Such spaces are encountered, for instance, in materials science, where datasets on Compositionally Complex Materials (CCMs) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions. At its core, nimCSO leverages the metaprogramming ability of the Nim language (this http URL) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. As demonstrated in this paper, nimCSO reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution. It is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability. All configuration is done with a simple human-readable YAML config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills.","Mon, 4 Mar 2024 18:59:45 UTC (1,691 KB)"
"113","Contract Design for Pandora's Box","Martin Hoefer, Conrad Schecker, Kevin Schewior","Computer Science and Game Theory (cs.GT)","We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979. In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios. A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.","Mon, 4 Mar 2024 18:53:27 UTC (28 KB)"
"114","Statistical Query Lower Bounds for Learning Truncated Gaussians","Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis","Data Structures and Algorithms (cs.DS)","We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension and Gaussian surface are small. As a corollary of our construction, it also follows that the complexity of the previously known algorithm for this task is qualitatively best possible.","Mon, 4 Mar 2024 18:30:33 UTC (43 KB)"
"115","Astronomy in Colombia: a bibliometric perspective","Sofía Guevara-Montoya, Felipe Ortiz-Ferreira, María Paula Silva-Arévalo, Paola A. Niño-Muñoz, Jaime E. Forero-Romero","Digital Libraries (cs.DL)","In Colombia, astronomical research is experiencing accelerated growth. In order to better understand its evolution and current state, we conducted a bibliometric study using data from the Astrophysics Data System (ADS) and Web of Science (WoS). In ADS, we identified 422 peer-reviewed publications from 1980, the year of the first publication, until 2023, which was the cutoff date for our study. Among the 25 Colombian institutions identified as participants in at least one publication, the contributions of four universities stand out: Universidad de los Andes, Universidad Nacional de Colombia, Universidad Industrial de Santander, and Universidad de Antioquia, with 104, 78, 68, and 67 publications, respectively. By cross-referencing information from ADS and WoS, we found that the areas with the greatest impact in publications are threefold: high-energy and fundamental physics, stars and stellar physics, and galaxies and cosmology. Globally, according to WoS, Colombia ranks 52nd in the number of peer-reviewed publications between 2019 and 2023, and fifth in Latin America. Additionally, we identified three highly cited publications (top 1% worldwide) belonging to the field of observational cosmology. When analyzing countries with equal or greater bibliographic production, we estimate that Colombian production is approximately four times lower than expected considering its population and GDP.","Mon, 4 Mar 2024 17:41:50 UTC (1,315 KB)"
"116","Constraint Satisfaction Problems with Advice","Suprovat Ghoshal, Konstantin Makarychev, Yury Makarychev","Data Structures and Algorithms (cs.DS)","We initiate the study of algorithms for constraint satisfaction problems with ML oracle advice. We introduce two models of advice and then design an approximation algorithm for Max Cut and Max 2-Lin in these models.","Mon, 4 Mar 2024 17:00:31 UTC (14 KB)"
"117","Matching Algorithms in the Sparse Stochastic Block Model","Anna Brandenberger, Byron Chin, Nathan S. Sheffield, Divya Shyamal","Data Structures and Algorithms (cs.DS)","The stochastic block model (SBM) is a generalization of the Erdős--Rényi model of random graphs that describes the interaction of a finite number of distinct communities. In sparse Erdős--Rényi graphs, it is known that a linear-time algorithm of Karp and Sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such graphs in terms of solutions to an ODE. We provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the Karp--Sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general SBM instances. We also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive. We show that the competitive ratio lower bound of 0.837 found by Mastin and Jaillet for the Erdős--Rényi case is tight whenever the expected degrees in all communities are equal. We propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality.","Mon, 4 Mar 2024 15:50:17 UTC (2,480 KB)"
"118","Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks","Emre Anakok, Pierre Barbillon, Colin Fontaine, Elisa Thebault","Machine Learning (stat.ML)","We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias.","Mon, 4 Mar 2024 13:12:02 UTC (1,260 KB)"
"119","Faster MEM-finding in $O (r + \bar{r} + g)$ space","Travis Gagie","Data Structures and Algorithms (cs.DS)","Suppose we are given a text $T [1..n]$, a straight-line program with $g$ rules for $T$ and an assignment of tags to the characters in $T$ such that the Burrows-Wheeler Transform of $T$ has $r$ runs, the Burrows-Wheeler Transform of the reverse of $T$ has $\bar{r}$ runs and the tag array -- the list of tags in the lexicographic order of the suffixes starting at the characters the tags are assigned to -- has $t$ runs. If the alphabet size is at most polylogarithmic in $n$ then there is an $O (r + \bar{r} + g + t)$-space index for $T$ such that when we are given a pattern $P [1..m]$ we can compute the maximal exact matches (MEMs) of $P$ with respect to $T$ in $O (m)$ time plus $O (\log n)$ time per MEM and then list the distinct tags assigned to the first characters of occurrences of that MEM in constant time per tag listed, all correctly with high probability.","Mon, 4 Mar 2024 13:04:32 UTC (5 KB)"
"120","Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties","Tobias Dornheim, Sebastian Schwalbe, Maximilian Böhme, Zhandos Moldabekov, Jan Vorberger, Panagiotis Tolias","Computational Physics (physics.comp-ph)","We present extensive new \emph{ab initio} path integral Monte Carlo (PIMC) results for a variety of structural properties of warm dense hydrogen and beryllium. To deal with the fermion sign problem -- an exponential computational bottleneck due to the antisymmetry of the electronic thermal density matrix -- we employ the recently proposed [\textit{J.~Chem.~Phys.}~\textbf{157}, 094112 (2022); \textbf{159}, 164113 (2023)] $\xi$-extrapolation method and find excellent agreement with exact direct PIMC reference data where available. This opens up the intriguing possibility to study a gamut of properties of light elements and potentially material mixtures over a substantial part of the warm dense matter regime, with direct relevance for astrophysics, material science, and inertial confinement fusion research.","Mon, 4 Mar 2024 12:23:05 UTC (1,064 KB)"
"121","Towards Deterministic Algorithms for Constant-Depth Factors of Constant-Depth Circuits","Mrinal Kumar, Varun Ramanathan, Ramprasad Saptharishi, Ben Lee Volk","Computational Complexity (cs.CC)","We design a deterministic subexponential time algorithm that takes as input a multivariate polynomial $f$ computed by a constant-depth circuit over rational numbers, and outputs a list $L$ of circuits (of unbounded depth and possibly with division gates) that contains all irreducible factors of $f$ computable by constant-depth circuits. This list $L$ might also include circuits that are spurious: they either do not correspond to factors of $f$ or are not even well-defined, e.g. the input to a division gate is a sub-circuit that computes the identically zero polynomial. The key technical ingredient of our algorithm is a notion of the pseudo-resultant of $f$ and a factor $g$, which serves as a proxy for the resultant of $g$ and $f/g$, with the advantage that the circuit complexity of the pseudo-resultant is comparable to that of the circuit complexity of $f$ and $g$. This notion, which might be of independent interest, together with the recent results of Limaye, Srinivasan and Tavenas, helps us derandomize one key step of multivariate polynomial factorization algorithms - that of deterministically finding a good starting point for Newton Iteration for the case when the input polynomial as well as the irreducible factor of interest have small constant-depth circuits.","Mon, 4 Mar 2024 12:09:16 UTC (43 KB)"
"122","Random Generation of Git Graphs","Julien Courtiel (GREYC), Martin Pépin (GREYC)","Data Structures and Algorithms (cs.DS)","Version Control Systems, such as Git and Mercurial, manage the history of a project as a Directed Acyclic Graph encoding the various divergences and synchronizations happening in its life cycle. A popular workflow in the industry, called the feature branch workflow, constrains these graphs to be of a particular shape: a unique main branch, and non-interfering feature branches. Here we focus on the uniform random generation of those graphs with n vertices, including k on the main branch, for which we provide three algorithms, for three different use-cases. The first, based on rejection, is efficient when aiming for small values of k (more precisely whenever k = O($\sqrt$ n)). The second takes as input any number k of commits in the main branch, but requires costly precalculation. The last one is a Boltzmann generator and enables us to generate very large graphs while targeting a constant k/n ratio. All these algorithms are linear in the size of their outputs.","Mon, 4 Mar 2024 10:01:00 UTC (187 KB)"
"123","The Canadian Traveller Problem on outerplanar graphs","Laurent Beaudou, Pierre Bergé, Vsevolod Chernyshev, Antoine Dailly, Yan Gerard, Aurélie Lagoutte, Vincent Limouzy, Lucas Pastor","Data Structures and Algorithms (cs.DS)","We study the PSPACE-complete $k$-Canadian Traveller Problem, where a weighted graph $G=(V,E,\omega)$ with a source $s\in V$ and a target $t\in V$ are given. This problem also has a hidden input $E_* \subsetneq E$ of cardinality at most $k$ representing blocked edges. The objective is to travel from $s$ to $t$ with the minimum distance. At the beginning of the walk, the blockages $E_*$ are unknown: the traveller discovers that an edge is blocked when visiting one of its endpoints. Online algorithms, also called strategies, have been proposed for this problem and assessed with the competitive ratio, i.e. the ratio between the distance actually traversed by the traveller divided by the distance we would have traversed knowing the blockages in advance. Even though the optimal competitive ratio is $2k+1$ even on unit-weighted planar graphs of treewidth 2, we design a polynomial-time strategy achieving competitive ratio $9$ on unit-weighted outerplanar graphs. This value $9$ also stands as a lower bound for this family of graphs as we prove that, for any $\varepsilon > 0$, no strategy can achieve a competitive ratio $9-\varepsilon$. Finally, we show that it is not possible to achieve a constant competitive ratio (independent of $G$ and $k$) on weighted outerplanar graphs.","Mon, 4 Mar 2024 09:29:29 UTC (51 KB)[v2] Fri, 8 Mar 2024 14:58:14 UTC (51 KB)"
"124","Schema-Based Query Optimisation for Graph Databases","Chandan Sharma (TYREX), Pierre Genevès (TYREX), Nils Gesbert (TYREX), Nabil Layaïda (TYREX)","Databases (cs.DB)","Recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. Graph data often come with schema information that describe how nodes and edges are organized. We propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. We show that this schema information can be useful in order to improve the performance when evaluating acylic recursive graph queries. Furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.","Mon, 4 Mar 2024 09:19:50 UTC (1,437 KB)"
"125","Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication","Matthias Bentert, Klaus Heeger, Tomohiro Koana","Data Structures and Algorithms (cs.DS)","We study the computational complexity of several polynomial-time-solvable graph problems parameterized by vertex integrity, a measure of a graph's vulnerability to vertex removal in terms of connectivity. Vertex integrity is the smallest number $\iota$ such that there is a set $S$ of $\iota' \le \iota$ vertices such that every connected component of $G-S$ contains at most $\iota-\iota'$ vertices. It is known that the vertex integrity lies between the well-studied parameters vertex cover number and tree-depth. Alon and Yuster [ESA 2007] designed algorithms for graphs with small vertex cover number using fast matrix multiplications. We demonstrate that fast matrix multiplication can also be effectively used when parameterizing by vertex integrity $\iota$ by developing efficient algorithms for problems including an $O(\iota^{\omega-1}n)$-time algorithm for computing the girth of a graph, randomized $O(\iota^{\omega - 1}n)$-time algorithms for Maximum Matching and for finding any induced four-vertex subgraph except for a clique or an independent set, and an $O(\iota^{(\omega-1)/2}n^2) \subseteq O(\iota^{0.687} n^2)$-time algorithm for All-Pairs Shortest Paths. These algorithms can be faster than previous algorithms parameterized by tree-depth, for which fast matrix multiplication is not known to be effective.","Mon, 4 Mar 2024 08:36:15 UTC (33 KB)"
"126","Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search","Lars Gottesbüren, Laxman Dhulipala, Rajesh Jayaram, Jakub Lacki","Data Structures and Algorithms (cs.DS)","We consider the fundamental problem of decomposing a large-scale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving shards, so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a routing algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines. In this paper, we design simple and highly efficient routing methods, and prove strong theoretical guarantees on their performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with any partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated partitioning method. In particular, our new routing methods enable the use of balanced graph partitioning, which is a high-quality partitioning method without a naturally associated routing algorithm. Thus, we provide the first methods for routing using balanced graph partitioning that are extremely fast to train, admit low latency, and achieve high recall. We provide a comprehensive evaluation of our full partitioning and routing pipeline on billion-scale datasets, where it outperforms existing scalable partitioning methods by significant margins, achieving up to 2.14x higher QPS at 90% recall$@10$ than the best competitor.","Mon, 4 Mar 2024 07:37:09 UTC (1,410 KB)"
"127","K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy","Henan Sun, Zhengyu Wu, Rong-Hua Li, Guoren Wang, Zening Li","Cryptography and Security (cs.CR)","(p,q)-clique enumeration on a bipartite graph is critical for calculating clustering coefficient and detecting densest subgraph. It is necessary to carry out subgraph enumeration while protecting users' privacy from any potential attacker as the count of subgraph may contain sensitive information. Most recent studies focus on the privacy protection algorithms based on edge LDP (Local Differential Privacy). However, these algorithms suffer a large estimation error due to the great amount of required noise. In this paper, we propose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p, q)-clique enumeration with a small estimation error, where a k-stars is a star-shaped graph with k nodes connecting to one node. The effectiveness of edge LDP relies on its capacity to obfuscate the existence of an edge between the user and his one-hop neighbors. This is based on the premise that a user should be aware of the existence of his one-hop neighbors. Similarly, we can apply this premise to k-stars as well, where an edge is a specific genre of 1-stars. Based on this fact, we first propose the k-stars neighboring list to enable our algorithm to obfuscate the existence of k-stars with Warner' s RR. Then, we propose the absolute value correction technique and the k-stars sampling technique to further reduce the estimation error. Finally, with the two-round user-collector interaction mechanism, we propose our k-stars LDP algorithm to count the number of (p, q)-clique while successfully protecting users' privacy. Both the theoretical analysis and experiments have showed the superiority of our algorithm over the algorithms based on edge LDP.","Mon, 4 Mar 2024 07:30:10 UTC (3,763 KB)"
"128","Structured methods for parameter inference and uncertainty quantification for mechanistic models in the life sciences","Michael J. Plank, Matthew J. Simpson","Quantitative Methods (q-bio.QM)","Parameter inference and uncertainty quantification are important steps when relating mathematical models to real-world observations, and when estimating uncertainty in model predictions. However, methods for doing this can be computationally expensive, particularly when the number of unknown model parameters is large. The aim of this study is to develop and test an efficient profile likelihood-based method, which takes advantage of the structure of the mathematical model being used. We do this by identifying specific parameters that affect model output in a known way, such as a linear scaling. We illustrate the method by applying it to three caricature models from different areas of the life sciences: (i) a predator-prey model from ecology; (ii) a compartment-based epidemic model from health sciences; and, (iii) an advection-diffusion-reaction model describing transport of dissolved solutes from environmental science. We show that the new method produces results of comparable accuracy to existing profile likelihood methods, but with substantially fewer evaluations of the forward model. We conclude that our method could provide a much more efficient approach to parameter inference for models where a structure approach is feasible. Code to apply the new method to user-supplied models and data is provided via a publicly accessible repository.","Mon, 4 Mar 2024 02:13:58 UTC (289 KB)"
"129","Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa","L. Crow, S. J. Watts","Machine Learning (stat.ML)","The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification algorithm gives the confusion matrix and Kappa. Theory and methods are discussed in detail and then applied to Monte Carlo data and real datasets. Four very different real datasets - Breast Cancer, Coronary Heart Disease, Bankruptcy, and Particle Identification - are analysed, with both continuous and discrete values, and their classification performance compared to the expected theoretical limit. In all cases this analysis shows that the algorithms could not have performed any better due to the underlying probability density functions for the two classes. Important lessons are learnt on how to predict the performance of algorithms for imbalanced data using training datasets that are approximately balanced. Machine learning is very powerful but classification performance ultimately depends on the quality of the data and the relevance of the variables to the problem.","Sun, 3 Mar 2024 17:36:42 UTC (4,596 KB)"
"130","Approximations and Hardness of Packing Partially Ordered Items","Ilan Doron-Arad, Guy Kortsarz, Joseph Naor, Baruch Schieber, Hadas Shachnai","Data Structures and Algorithms (cs.DS)","Motivated by applications in production planning and storage allocation in hierarchical databases, we initiate the study of covering partially ordered items (CPO). Given a capacity $k \in \mathbb{Z}^+$, and a directed graph $G=(V,E)$ where each vertex has a size in $\{0,1, \ldots,k\}$, we seek a collection of subsets of vertices $S_1, \ldots, S_m$ that cover all the vertices, such that for any $1 \leq j \leq m$, the total size of vertices in $S_j$ is bounded by $k$, and there are no edges from $V \setminus S_j$ to $S_j$. The objective is to minimize the number of subsets $m$. CPO is closely related to the rule caching problem (RCP) that is of wide interest in the networking area. The input for RCP is a directed graph $G=(V,E)$, a profit function $p:V \rightarrow \mathbb{Z}_{0}^+$, and $k \in \mathbb{Z}^+$. The output is a subset $S \subseteq V$ of maximum profit such that $|S| \leq k$ and there are no edges from $V \setminus S$ to $S$. Our main result is a $2$-approximation algorithm for CPO on out-trees, complemented by an asymptotic $1.5$-hardness of approximation result. We also give a two-way reduction between RCP and the densest $k$-subhypergraph problem, surprisingly showing that the problems are equivalent w.r.t. polynomial-time approximation within any factor $\rho \geq 1$. This implies that RCP cannot be approximated within factor $|V|^{1-\eps}$ for any fixed $\eps>0$, under standard complexity assumptions. Prior to this work, RCP was just known to be strongly NP-hard. We further show that there is no EPTAS for the special case of RCP where the profits are uniform, assuming Gap-ETH. Since this variant admits a PTAS, we essentially resolve the complexity status of this problem.","Sun, 3 Mar 2024 17:17:08 UTC (227 KB)"
"131","Local weak convergence and its applications","Sayan Banerjee, Shankar Bhamidi, Jianan Shen, Seth Parker Young","Probability (math.PR)","Motivated in part by understanding average case analysis of fundamental algorithms in computer science, and in part by the wide array of network data available over the last decade, a variety of random graph models, with corresponding processes on these objects, have been proposed over the last few years. The main goal of this paper is to give an overview of local weak convergence, which has emerged as a major technique for understanding large network asymptotics for a wide array of functionals and models. As opposed to a survey, the main goal is to try to explain some of the major concepts and their use to junior researchers in the field and indicate potential resources for further reading.","Sun, 3 Mar 2024 15:43:19 UTC (9,601 KB)"
"132","Piggybacking astronomical hazard investigations on scientific Big Data missions","Gijs A. Verdoes Kleijn, Teymoor Saifollahi, Rees Williams, Oscar Stolk, Georg Feulner","Instrumentation and Methods for Astrophysics (astro-ph.IM)","Current and upcoming large optical and near-infrared astronomical surveys have fundamental science as their primary drivers. To cater to those, these missions scan large fractions of the entire sky at multiple wavelengths and epochs. These aspects make these data sets also valuable for investigations into astronomical hazards for life on Earth. The Netherlands Research School for Astronomy (NOVA) is a partner in several optical / near-infrared surveys. In this paper we focus on the astronomical hazard value for two sets of those: the surveys with the OmegaCAM wide-field imager at the VST and with the Euclid Mission. For each of them we provide a brief overview of the astronomical survey hardware, the data and the information systems. We present first results related to the astronomical hazard investigations. We evaluate to what extent the existing functionality of the information systems covers the needs for the astronomical hazard investigations","Sun, 3 Mar 2024 15:38:17 UTC (5,236 KB)"
"133","An Authentic Algorithm for Ciphering and Deciphering Called Latin Djokovic","Diogen Babuc","Computational Engineering, Finance, and Science (cs.CE)","The question that is a motivation of writing is how many devote themselves to discovering something in the world of science where much is discerned and revealed, but at the same time, much is unknown. The insightful elements of this algorithm are the ciphering and deciphering algorithms of Playfair, Caesar, and Vigenere. Only a few of their main properties are taken and modified, with the aim of forming a specific functionality of the algorithm called Latin Djokovic. Specifically, a string is entered as input data. A key k is given, with a random value between the values a and b = a + 3. The obtained value is stored in a variable with the aim of being constant during the run of the algorithm. In correlation to the given key, the string is divided into several groups of substrings, and each substring has a length of k characters. The next step involves encoding each substring from the list of existing substrings. Encoding is performed using the basis of the Caesar algorithm, i.e., shifting with k characters. However, that k is incremented by 1 when moving to the next substring in that list. When the value of k becomes greater than b + 1, it will return to its initial value. The algorithm is executed, following the same procedure, until the last substring in the list is traversed. Using this polyalphabetic method, ciphering and deciphering of strings are achieved. The algorithm also works for a 100-character string. The x character is not used when the number of characters in a substring is incompatible with the expected length. The algorithm is simple to implement, but it is questionable if it works better than the other methods from the point of view of execution time and storage space.","Sun, 3 Mar 2024 09:59:21 UTC (601 KB)"
"134","Fusion of Gaussian Processes Predictions with Monte Carlo Sampling","Marzieh Ajirak, Daniel Waxman, Fernando Llorente, Petar M. Djuric","Machine Learning (cs.LG)","In science and engineering, we often work with models designed for accurate prediction of variables of interest. Recognizing that these models are approximations of reality, it becomes desirable to apply multiple models to the same data and integrate their outcomes. In this paper, we operate within the Bayesian paradigm, relying on Gaussian processes as our models. These models generate predictive probability density functions (pdfs), and the objective is to integrate them systematically, employing both linear and log-linear pooling. We introduce novel approaches for log-linear pooling, determining input-dependent weights for the predictive pdfs of the Gaussian processes. The aggregation of the pdfs is realized through Monte Carlo sampling, drawing samples of weights from their posterior. The performance of these methods, as well as those based on linear pooling, is demonstrated using a synthetic dataset.","Sun, 3 Mar 2024 04:21:21 UTC (770 KB)"
"135","Euclidean distance compression via deep random features","Brett Leroux, Luis Rademacher","Computational Geometry (cs.CG)","Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\varphi_\ell$ that compress point sets in the following way. For a point set $S$, the map $\varphi_\ell:\mathbb{R}^d \to N^{-1/2}\{-1,1\}^N$ has the property that storing $\varphi_\ell(S)$ (a \emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\pm \epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\epsilon$. The maps $\varphi_\ell$ are the $\ell$-fold composition of a certain type of random feature mapping. Moreover, we determine how large $N$ needs to be as a function of $\epsilon$ and other parameters of the point set. Compared to existing techniques, our maps offer several advantages. The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\mathbb{R}^k$ with $k =\Theta(\epsilon^{-2}\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\pm \epsilon)$ error with high probability. The main advantage of our maps $\varphi_\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\{-1,1\}^N$ and so there is no additional step needed to convert the sketch to bits. For some range of parameters, our maps $\varphi_\ell$ produce sketches which require fewer bits of storage space.","Sat, 2 Mar 2024 22:24:31 UTC (21 KB)"
"136","The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models","Stephanie Eckman, Barbara Plank, Frauke Kreuter","Human-Computer Interaction (cs.HC)","Whether future AI models make the world safer or less safe for humans rests in part on our ability to efficiently collect accurate data from people about what they want the models to do. However, collecting high quality data is difficult, and most AI/ML researchers are not trained in data collection methods. The growing emphasis on data-centric AI highlights the potential of data to enhance model performance. It also reveals an opportunity to gain insights from survey methodology, the science of collecting high-quality survey data. In this position paper, we summarize lessons from the survey methodology literature and discuss how they can improve the quality of training and feedback data, which in turn improve model performance. Based on the cognitive response process model, we formulate specific hypotheses about the aspects of label collection that may impact training data quality. We also suggest collaborative research ideas into how possible biases in data collection can be mitigated, making models more accurate and human-centric.","Sat, 2 Mar 2024 13:34:43 UTC (795 KB)"
"137","A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications","Adrian Dumitrescu","Data Structures and Algorithms (cs.DS)","We revisit the algorithmic problem of finding a triangle in a graph: We give a randomized combinatorial algorithm for triangle detection in a given $n$-vertex graph with $m$ edges running in $O(n^{7/3})$ time, or alternatively in $O(m^{4/3})$ time. This may come as a surprise since it invalidates several conjectures in the literature. In particular, - the $O(n^{7/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection based on matrix multiplication running in $O(n^\omega) = O(n^{2.372})$ time, due to Itai and Rodeh (1978). - the $O(m^{4/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection in sparse graphs based on matrix multiplication running in $O(m^{2\omega/(\omega+1)})= O(m^{1.407})$ time due to Alon, Yuster, and Zwick (1997). - the $O(n^{7/3})$ time algorithm for triangle detection leads to a $O(n^{25/9} \log{n})$ time combinatorial algorithm for $n \times n$ Boolean matrix multiplication, by a reduction of V. V. Williams and R.~R.~Williams (2018).This invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014). - the $O(m^{4/3})$ runtime invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014) that any combinatorial algorithm for triangle detection requires $m^{3/2 - o(1)}$ time. - as a direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the $k$-clique problem, surpassing an almost $40$ years old algorithm of Ne{š}et{ř}il and Poljak (1985). This result strongly disproves the combinatorial $k$-clique conjecture. - as another direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the \textsc{Max-Cut} problem, surpassing an almost $20$ years old algorithm of R.~R.~Williams (2005).","Sat, 2 Mar 2024 04:01:26 UTC (23 KB)[v2] Tue, 5 Mar 2024 15:14:48 UTC (1 KB) (withdrawn)"
"138","Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice","Karan Shah, Julie Butler, Alexis Knaub, Anıl Zenginoğlu, William Ratcliff, Mohammad Soltanieh-ha","Physics Education (physics.ed-ph)","With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.","Fri, 1 Mar 2024 20:21:42 UTC (4,431 KB)"
"139","ClassInSight: Designing Conversation Support Tools to Visualize Classroom Discussion for Personalized Teacher Professional Development","Tricia J. Ngoon, S Sushil, Angela Stewart, Ung-Sang Lee, Saranya Venkatraman, Neil Thawani, Prasenjit Mitra, Sherice Clarke, John Zimmerman, Amy Ogan","Human-Computer Interaction (cs.HC)","Teaching is one of many professions for which personalized feedback and reflection can help improve dialogue and discussion between the professional and those they serve. However, professional development (PD) is often impersonal as human observation is labor-intensive. Data-driven PD tools in teaching are of growing interest, but open questions about how professionals engage with their data in practice remain. In this paper, we present ClassInSight, a tool that visualizes three levels of teachers' discussion data and structures reflection. Through 22 reflection sessions and interviews with 5 high school science teachers, we found themes related to dissonance, contextualization, and sustainability in how teachers engaged with their data in the tool and in how their professional vision, the use of professional expertise to interpret events, shifted over time. We discuss guidelines for these conversational support tools to support personalized PD in professions beyond teaching where conversation and interaction are important.","Fri, 1 Mar 2024 20:09:44 UTC (1,147 KB)"
"140","Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights","Yuan Wang, Lokesh Kumar Sambasivan, Mingang Fu, Prakhar Mehrotra","Artificial Intelligence (cs.AI)","Generative AI applications, such as ChatGPT or DALL-E, have shown the world their impressive capabilities in generating human-like text or image. Diving deeper, the science stakeholder for those AI applications are Deep Generative Models, a.k.a DGMs, which are designed to learn the underlying distribution of the data and generate new data points that are statistically similar to the original dataset. One critical question is raised: how can we leverage DGMs into morden retail supply chain realm? To address this question, this paper expects to provide a comprehensive review of DGMs and discuss their existing and potential usecases in retail supply chain, by (1) providing a taxonomy and overview of state-of-the-art DGMs and their variants, (2) reviewing existing DGM applications in retail supply chain from a end-to-end view of point, and (3) discussing insights and potential directions on how DGMs can be further utilized on solving retail supply chain problems.","Thu, 29 Feb 2024 21:03:46 UTC (58 KB)"
"141","The ArgusSpec Prototype: Autonomous Spectroscopic Follow-up of Flares Detected by Large Array Telescopes","Nathan W. Galliher, Thomas Procter, Nicholas M. Law, Hank Corbett, Ward S. Howard, Alan Vasquez Soto, Ramses Gonzalez, Lawrence Machia, Jonathan Carney, William J. Marshall","Instrumentation and Methods for Astrophysics (astro-ph.IM)","ArgusSpec is a prototype autonomous spectroscopic follow-up instrument designed to characterize flares detected by the Argus Pathfinder telescope array by taking short exposure (30 s) broadband spectra (370 - 750 nm) at low resolutions (R~150 at 500 nm). The instrument is built from consumer off-the-shelf astronomical equipment, assembled inside a shipping container, and deployed alongside the Argus Pathfinder at a dark sky observing site in Western North Carolina. The \$35k prototype ArgusSpec was designed, built, and deployed in under a year, largely from existing parts, and has been operating on-sky since March 2023. With current hardware and software, the system is capable of receiving an observation, slewing, performing autonomous slit acquisition, and beginning data acquisition within an average of 32 s. With Argus Pathfinder's 1-second-cadence survey reporting alerts of rising sources within 2 s of onset, ArgusSpec can reach new targets well within a minute of the start of the event. As built, ArgusSpec can observe targets down to a 20$\sigma$ limiting magnitude of $m_V$~13 at 30 s cadence with an optical resolution of R~150 (at 500 nm). With automated rapid acquisition demonstrated, later hardware upgrades will significantly improve the limiting magnitude, and potentially enable deep spectroscopy by the coaddition of data from an array of ArgusSpec systems. ArgusSpec's primary science driver is the characterization of the blackbody evolution of flares from nearby M-dwarfs. Large flares emitted by these stars could have significant impacts on the potential habitability of any orbiting exoplanets, but our current understanding of these events is in large part built on observations from a handful of active stars. ArgusSpec will characterize large numbers of flares, building a spectroscopic library of the most extreme events from a wide variety of stellar masses and ages.","Thu, 29 Feb 2024 19:00:00 UTC (15,403 KB)"
"142","Virtual Reality for Understanding Artificial-Intelligence-driven Scientific Discovery with an Application in Quantum Optics","Philipp Schmidt, Sören Arlt, Carlos Ruiz-Gonzalez, Xuemei Gu, Carla Rodríguez, Mario Krenn","Human-Computer Interaction (cs.HC)","Generative Artificial Intelligence (AI) models can propose solutions to scientific problems beyond human capability. To truly make conceptual contributions, researchers need to be capable of understanding the AI-generated structures and extracting the underlying concepts and ideas. When algorithms provide little explanatory reasoning alongside the output, scientists have to reverse-engineer the fundamental insights behind proposals based solely on examples. This task can be challenging as the output is often highly complex and thus not immediately accessible to humans. In this work we show how transferring part of the analysis process into an immersive Virtual Reality (VR) environment can assist researchers in developing an understanding of AI-generated solutions. We demonstrate the usefulness of VR in finding interpretable configurations of abstract graphs, representing Quantum Optics experiments. Thereby, we can manually discover new generalizations of AI-discoveries as well as new understanding in experimental quantum optics. Furthermore, it allows us to customize the search space in an informed way - as a human-in-the-loop - to achieve significantly faster subsequent discovery iterations. As concrete examples, with this technology, we discover a new resource-efficient 3-dimensional entanglement swapping scheme, as well as a 3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our results show the potential of VR for increasing a human researcher's ability to derive knowledge from graph-based generative AI that, which is a common abstract data representation used in diverse fields of science.","Tue, 20 Feb 2024 17:48:01 UTC (12,146 KB)"
"143","Cognitive Bias in High-Stakes Decision-Making with LLMs","Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He","Artificial Intelligence (cs.AI)","Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type.","Sun, 25 Feb 2024 02:35:56 UTC (7,208 KB)"
"144","A framework for understanding data science","Michael L Brodie","Other Statistics (stat.OT)","The objective of this research is to provide a framework with which the data science community can understand, define, and develop data science as a field of inquiry. The framework is based on the classical reference framework (axiology, ontology, epistemology, methodology) used for 200 years to define knowledge discovery paradigms and disciplines in the humanities, sciences, algorithms, and now data science. I augmented it for automated problem-solving with (methods, technology, community). The resulting data science reference framework is used to define the data science knowledge discovery paradigm in terms of the philosophy of data science addressed in previous papers and the data science problem-solving paradigm, i.e., the data science method, and the data science problem-solving workflow, both addressed in this paper. The framework is a much called for unifying framework for data science as it contains the components required to define data science. For insights to better understand data science, this paper uses the framework to define the emerging, often enigmatic, data science problem-solving paradigm and workflow, and to compare them with their well-understood scientific counterparts, scientific problem-solving paradigm and workflow.","Wed, 14 Feb 2024 15:55:40 UTC (4,403 KB)"
"145","An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots","Tobias Haubold, Petra Linke","Robotics (cs.RO)","As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines. In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of virtual worlds and data scientists even work on the same files. We want to contribute to that topic by describing an approach where data scientists don't require knowledge about the simulation software. Our approach uses the standalone simulation software Webots, the Robot Operating System to communicate with simulated robots as well as the simulation software itself and container technology to separate the simulation from the model development environment. We put emphasize on the APIs the data scientists work with and the use of a standalone simulation software in unattended training pipelines. We show the parts that are specific to the Robotino and the robot task to learn.","Tue, 6 Feb 2024 12:08:01 UTC (188 KB)"
"146","The Probability to Hit Every Bin with a Linear Number of Balls","Stefan Walzer","Probability (math.PR)","Assume that $2n$ balls are thrown independently and uniformly at random into $n$ bins. We consider the unlikely event $E$ that every bin receives at least one ball, showing that $\Pr[E] = \Theta(b^n)$ where $b \approx 0.836$. Note that, due to correlations, $b$ is not simply the probability that any single bin receives at least one ball. More generally, we consider the event that throwing $\alpha n$ balls into $n$ bins results in at least $d$ balls in each bin.","Fri, 1 Mar 2024 18:32:34 UTC (59 KB)"
"147","Scalable Learning of Item Response Theory Models","Susanne Frick, Amer Krivošija, Alexander Munteanu","Machine Learning (cs.LG)","Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data.","Fri, 1 Mar 2024 17:12:53 UTC (393 KB)"
"148","Optimal Control of Underdamped Systems: An Analytic Approach","Julia Sanders, Marco Baldovin, Paolo Muratore-Ginanneschi","Statistical Mechanics (cond-mat.stat-mech)","Optimal control theory deals with finding protocols to steer a system between assigned initial and final states, such that a trajectory-dependent cost function is minimized. The application of optimal control to stochastic systems is an open and challenging research frontier, with a spectrum of applications ranging from stochastic thermodynamics, to biophysics and data science. Among these, the design of nanoscale electronic components motivates the study of underdamped dynamics, leading to practical and conceptual difficulties. In this work, we develop analytic techniques to determine protocols steering finite time transitions at minimum dissipation for stochastic underdamped dynamics. For transitions between Gaussian states, we prove that optimal protocols satisfy a Lyapunov equation, a central tool in stability analysis of dynamical systems. For transitions between states described by general Maxwell-Boltzmann distributions, we introduce an infinite-dimensional version of the Poincaré-Linstedt multiscale perturbation theory around the overdamped limit. This technique fundamentally improves the standard multiscale expansion. Indeed, it enables the explicit computation of momentum cumulants, whose variation in time is a distinctive trait of underdamped dynamics and is directly accessible to experimental observation. Our results allow us to numerically study cost asymmetries in expansion and compression processes and make predictions for inertial corrections to optimal protocols in the Landauer erasure problem at the nanoscale.","Fri, 1 Mar 2024 17:10:51 UTC (2,279 KB)"
"149","Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number","Pascal Koiran, Subhayan Saha","Data Structures and Algorithms (cs.DS)","We study symmetric tensor decompositions, i.e., decompositions of the form $T = \sum_{i=1}^r u_i^{\otimes 3}$ where $T$ is a symmetric tensor of order 3 and $u_i \in \mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent. This implies $r \leq n$,that is, the decomposition of T is undercomplete. We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric tensor that has an undercomplete decomposition.Then given some $T'$ close to $T$, an accuracy parameter $\varepsilon$, and an upper bound B on the condition number of the tensor, output vectors $u'_i$ such that $||u_i - u'_i|| \leq \varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are: 1) We provide the first algorithm for this problem that runs in linear time in the size of the input tensor. More specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\varepsilon =$ 1/poly(n) and B = poly(n). 2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\frac{1}{\varepsilon}$) in the input tensor. 3) We present a smoothed analysis of the condition number of the tensor decomposition problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs. Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented tensor obtained from the input tensor by a change of basis.","Fri, 1 Mar 2024 16:22:39 UTC (39 KB)"
"150","Nearest-Neighbours Estimators for Conditional Mutual Information","Jake Witter, Conor Houghton","Information Theory (cs.IT)","The conditional mutual information quantifies the conditional dependence of two random variables. It has numerous applications; it forms, for example, part of the definition of transfer entropy, a common measure of the causal relationship between time series. It does, however, require a lot of data to estimate accurately and suffers the curse of dimensionality, limiting its application in machine learning and data science. However, the Kozachenko-Leonenko approach can address this problem: it is possible, in this approach to define a nearest-neighbour estimator which depends only on the distance between data points and not on the dimension of the data. Furthermore, the bias can be calculated analytically for this estimator. Here this estimator is described and is tested on simulated data.","Fri, 1 Mar 2024 14:28:00 UTC (319 KB)"
