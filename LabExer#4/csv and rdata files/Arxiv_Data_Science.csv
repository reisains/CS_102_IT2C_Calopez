"","title","author","subject","abstract","meta"
"1","The NIRSpec Wide GTO Survey","Michael V. Maseda, Anna de Graaff, Marijn Franx, Hans-Walter Rix, Stefano Carniani, Isaac Laseter, Ugne Dudzeviciute, Tim Rawle, Eleonora Parlanti, Santiago Arribas, Andrew J. Bunker, Alex J. Cameron, Stephane Charlot, Mirko Curti, Francesco D'Eugenio, Gareth C. Jones, Nimisha Kumari, Roberto Maiolino, Hannah Uebler, Aayush Saxena, Renske Smit, Chris Willott, Joris Witstok","Astrophysics of Galaxies (astro-ph.GA)","The Near-infrared Spectrograph (NIRSpec) on the James Webb Space Telescope is uniquely suited to studying galaxies in the distant Universe with its combination of multi-object capabilities and sensitivity over a large range in wavelength (0.6-5.3 microns). Here we present the NIRSpec Wide survey, part of the NIRSpec Instrument Science Team's Guaranteed Time Observations, using NIRSpec's microshutter array to obtain spectra of more than 3200 galaxies at $z>1$ at both low- and high-resolution ($R\approx100$ and 2700) for a total of 105 hours. With 31 pointings covering $\approx$320 arcmin$^2$ across the five CANDELS fields with exquisite ancillary photometry from the Hubble Space Telescope, the NIRSpec Wide survey represents a fast and efficient way of using JWST to probe galaxies in the early Universe. Pointing centers are determined to maximize the observability of the rarest, high-value sources. Subsequently, the microshutter configurations are optimized to observe the maximum number of ""census"" galaxies with a selection function based primarily on HST/F160W magnitude, photometric/slitless grism redshift, and predicted \ha\ flux tracing the bulk of the galaxy population at cosmic noon ($z_{\rm med}=2.0$). We present details on the survey strategy, the target selection, an outline of the motivating science cases, and discuss upcoming public data releases to the community.","Fri, 8 Mar 2024 18:26:38 UTC (3,200 KB)"
"2","Sparse dynamic network reconstruction through L1-regularization of a Lyapunov equation","Ian Xul Belaustegui, Marcela Ordorica Arango, Román Rossi Pool, Naomi Ehrich Leonard, Alessio Franci","Systems and Control (eess.SY)","An important problem in many areas of science is that of recovering interaction networks from simultaneous time-series of many interacting dynamical processes. A common approach is to use the elements of the correlation matrix or its inverse as proxies of the interaction strengths, but the reconstructed networks are necessarily undirected. Transfer entropy methods have been proposed to reconstruct directed networks but the reconstructed network lacks information about interaction strengths. We propose a network reconstruction method that inherits the best of the two approaches by reconstructing a directed weighted network from noisy data under the assumption that the network is sparse and the dynamics are governed by a linear (or weakly-nonlinear) stochastic dynamical system. The two steps of our method are i) constructing an (infinite) family of candidate networks by solving the covariance matrix Lyapunov equation for the state matrix and ii) using L1-regularization to select a sparse solution. We further show how to use prior information on the (non)existence of a few directed edges to drastically improve the quality of the reconstruction.","Fri, 8 Mar 2024 17:07:37 UTC (2,163 KB)"
"3","Advances of Deep Learning in Protein Science: A Comprehensive Survey","Bozhen Hu, Cheng Tan, Lirong Wu, Jiangbin Zheng, Jun Xia, Zhangyang Gao, Zicheng Liu, Fandi Wu, Guijun Zhang, Stan Z. Li","Biomolecules (q-bio.BM)","Protein representation learning plays a crucial role in understanding the structure and function of proteins, which are essential biomolecules involved in various biological processes. In recent years, deep learning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and representations from large-scale protein data. This comprehensive survey aims to provide an overview of the recent advances in deep learning techniques applied to protein science. The survey begins by introducing the developments of deep learning based protein models and emphasizes the importance of protein representation learning in drug discovery, protein engineering, and function annotation. It then delves into the fundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention models, and graph neural networks in modeling protein sequences, structures, and functions, and explores how these techniques can be used to extract meaningful features and capture intricate relationships within protein data. Next, the survey presents various applications of deep learning in the field of proteins, including protein structure prediction, protein-protein interaction prediction, protein function prediction, etc. Furthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses potential solutions and future directions for overcoming these challenges. This comprehensive survey provides a valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing the power of deep learning techniques. By consolidating the latest advancements and discussing potential avenues for improvement, this review contributes to the ongoing progress in protein research and paves the way for future breakthroughs in the field.","Fri, 8 Mar 2024 13:45:32 UTC (4,315 KB)"
"4","Interoperability of the Metaverse: A Digital Ecosystem Perspective Review","Liang Yang, Shi-Ting Ni, Yuyang Wang, Ao Yu, Jyh-An Lee, Pan Hui","Computers and Society (cs.CY)","The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. Interoperability, recognized as a major barrier to the Metaverse's full potential, is central to this debate. CoinMarketCap's report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent. Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process. Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem. Urs Gasser's framework from Harvard Law School, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. Incorporating this framework, we dissect literature for a comprehensive Metaverse interoperability overview. Our study seeks to establish benchmarks for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement.","Fri, 8 Mar 2024 10:38:01 UTC (7,107 KB)"
"5","Efficient Algorithms for Personalized PageRank Computation: A Survey","Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, Ji-Rong Wen","Data Structures and Algorithms (cs.DS)","Personalized PageRank (PPR) is a traditional measure for node proximity on large graphs. For a pair of nodes $s$ and $t$, the PPR value $\pi_s(t)$ equals the probability that an $\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way. As a generalization of Google's celebrated PageRank centrality, PPR has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. Despite numerous studies devoted to PPR over the decades, efficient computation of PPR remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. In this paper, we recap several frequently used techniques for PPR computation and conduct a comprehensive survey of various recent PPR algorithms from an algorithmic perspective. We classify these approaches based on the types of queries they address and review their methodologies and contributions. We also discuss some representative algorithms for computing PPR on dynamic graphs and in parallel or distributed environments.","Fri, 8 Mar 2024 10:19:34 UTC (12,011 KB)"
"6","Putting Language into Context Using Smartphone-Based Keyboard Logging","Florian Bemmann, Timo Koch, Maximilian Bergmann, Clemens Stachl, Daniel Buschek, Ramona Schoedel, Sven Mayer","Human-Computer Interaction (cs.HC)","While the study of language as typed on smartphones offers valuable insights, existing data collection methods often fall short in providing contextual information and ensuring user privacy. We present a privacy-respectful approach - context-enriched keyboard logging - that allows for the extraction of contextual information on the user's input motive, which is meaningful for linguistics, psychology, and behavioral sciences. In particular, with our approach, we enable distinguishing language contents by their channel (i.e., comments, messaging, search inputs). Filtering by channel allows for better pre-selection of data, which is in the interest of researchers and improves users' privacy. We demonstrate our approach on a large-scale six-month user study (N=624) of language use in smartphone interactions in the wild. Finally, we highlight the implications for research on language use in human-computer interaction and interdisciplinary contexts.","Fri, 8 Mar 2024 09:41:01 UTC (3,282 KB)"
"7","Inverse Design of Photonic Crystal Surface Emitting Lasers is a Sequence Modeling Problem","Ceyao Zhang, Renjie Li, Cheng Zhang, Zhaoyu Zhang, Feng Yin","Applied Physics (physics.app-ph)","Photonic Crystal Surface Emitting Lasers (PCSEL)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. Advanced AI technologies, especially reinforcement learning (RL), have emerged as a powerful tool to augment and accelerate this inverse design process. By modeling the inverse design of PCSEL as a sequential decision-making problem, RL approaches can construct a satisfactory PCSEL structure from scratch. However, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of RL approaches. Recently, sequential models, especially the Transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models. In this paper, we introduce a novel framework named PCSEL Inverse Design Transformer (PiT) that abstracts the inverse design of PCSEL as a sequence modeling problem. The central part of our PiT is a Transformer-based structure that leverages the past trajectories and current states to predict the current actions. Compared with the traditional RL approaches, PiT can output the optimal actions and achieve target PCSEL designs by leveraging offline data and conditioning on the desired return. Results demonstrate that PiT achieves superior performance and data efficiency compared to baselines.","Fri, 8 Mar 2024 08:38:50 UTC (2,445 KB)"
"8","Improving the open cluster census. III. Using cluster masses, radii, and dynamics to create a cleaned open cluster catalogue","Emily L. Hunt, Sabine Reffert","Astrophysics of Galaxies (astro-ph.GA)","The census of open clusters has exploded in size thanks to data from the Gaia satellite. However, it is likely that many of these reported clusters are not gravitationally bound, making the open cluster census impractical for many scientific applications. We test different physically motivated methods for distinguishing between bound and unbound clusters, using them to create a cleaned cluster catalogue. We derived completeness-corrected photometric masses for 6956 clusters from our earlier work. Then, we used these masses to compute the size of the Roche surface of these clusters (their Jacobi radius) and distinguish between bound and unbound clusters. We find that only 5647 (79%) of the clusters from our previous catalogue are compatible with bound open clusters, dropping to just 11% of clusters within 250 pc. 3530 open clusters are in a strongly cut high quality sample. The moving groups in our sample show different trends in their size as a function of age and mass, suggesting that they are unbound and undergoing different dynamical processes. Our cluster mass measurements constitute the largest catalogue of Milky Way cluster masses to date, which we also use for further science. Firstly, we inferred the mass-dependent completeness limit of the open cluster census, showing that the census is complete within 1.8 kpc only for objects heavier than 230 M$_\odot$. Next, we derived a completeness-corrected age and mass function for our open cluster catalogue, including estimating that the Milky Way contains a total of $1.3 \times 10^5$ open clusters, only ~4% of which are currently known. Finally, we show that most open clusters have mass functions compatible with the Kroupa initial mass function. We demonstrate Jacobi radii for distinguishing between bound and unbound star clusters, and publish an updated star cluster catalogue with masses and improved cluster classifications. (abridged)","Fri, 8 Mar 2024 08:27:23 UTC (4,952 KB)"
"9","Single Family Algebra Operation on ZDDs Leads To Exponential Blow-Up","Kengo Nakamura, Masaaki Nishino, Shuhei Denzumi","Data Structures and Algorithms (cs.DS)","Zero-suppressed binary decision diagram (ZDD) is a data structure to represent a family of (sub)sets compactly, and it can be used as a succinct index for a family of sets. To build ZDD representing a desired family of sets, there are many transformation operations that take ZDDs as inputs and output ZDD representing the resultant family after performing operations such as set union and intersection. However, except for some basic operations, the worst-time complexity of taking such transformation on ZDDs has not been extensively studied, and some contradictory statements about it have arisen in the literature. In this paper, we show that many transformation operations on ZDDs cannot be performed in worst-case polynomial time with respect to the size of input ZDDs. This refutes some of the folklore circulated in past literature and resolves an open problem raised by Knuth. Our results are stronger in that such blow-up of computational time occurs even when the ordering, which has a significant impact on the efficiency of treating ZDDs, is reasonable.","Fri, 8 Mar 2024 05:57:12 UTC (371 KB)"
"10","Data-Dependent LSH for the Earth Mover's Distance","Rajesh Jayaram, Erik Waingarten, Tian Zhang","Data Structures and Algorithms (cs.DS)","We give new data-dependent locality sensitive hashing schemes (LSH) for the Earth Mover's Distance ($\mathsf{EMD}$), and as a result, improve the best approximation for nearest neighbor search under $\mathsf{EMD}$ by a quadratic factor. Here, the metric $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ consists of sets of $s$ vectors in $\mathbb{R}^d$, and for any two sets $x,y$ of $s$ vectors the distance $\mathsf{EMD}(x,y)$ is the minimum cost of a perfect matching between $x,y$, where the cost of matching two vectors is their $\ell_p$ distance. Previously, Andoni, Indyk, and Krauthgamer gave a (data-independent) locality-sensitive hashing scheme for $\mathsf{EMD}_s(\mathbb{R}^d,\ell_p)$ when $p \in [1,2]$ with approximation $O(\log^2 s)$. By being data-dependent, we improve the approximation to $\tilde{O}(\log s)$. Our main technical contribution is to show that for any distribution $\mu$ supported on the metric $\mathsf{EMD}_s(\mathbb{R}^d, \ell_p)$, there exists a data-dependent LSH for dense regions of $\mu$ which achieves approximation $\tilde{O}(\log s)$, and that the data-independent LSH actually achieves a $\tilde{O}(\log s)$-approximation outside of those dense regions. Finally, we show how to ""glue"" together these two hashing schemes without any additional loss in the approximation. Beyond nearest neighbor search, our data-dependent LSH also gives optimal (distributional) sketches for the Earth Mover's Distance. By known sketching lower bounds, this implies that our LSH is optimal (up to $\mathrm{poly}(\log \log s)$ factors) among those that collide close points with constant probability.","Fri, 8 Mar 2024 04:35:55 UTC (136 KB)"
"11","A basic lower bound for property testing","Eldar Fischer","Data Structures and Algorithms (cs.DS)","An $\epsilon$-test for any non-trivial property (one for which there are both satisfying inputs and inputs of large distance from the property) should use a number of queries that is at least inversely proportional in $\epsilon$. However, to the best of our knowledge there is no reference proof for this intuition. Such a proof is provided here. It is written so as to not require any prior knowledge of the related literature, and in particular does not use Yao's method.","Fri, 8 Mar 2024 02:27:59 UTC (6 KB)"
"12","NP-Completeness for the Space-Optimality of Double-Array Tries","Hideo Bannai, Keisuke Goto, Shunsuke Kanda, Dominik Köppl","Data Structures and Algorithms (cs.DS)","Indexing a set of strings for prefix search or membership queries is a fundamental task with many applications such as information retrieval or database systems. A classic abstract data type for modelling such an index is a trie. Due to the fundamental nature of this problem, it has sparked much interest, leading to a variety of trie implementations with different characteristics. A trie implementation that has been well-used in practice is the double-array (trie) consisting of merely two integer arrays. While a traversal takes constant time per node visit, the needed space consumption in computer words can be as large as the product of the number of nodes and the alphabet size. Despite that several heuristics have been proposed on lowering the space requirements, we are unaware of any theoretical guarantees. In this paper, we study the decision problem whether there exists a double-array of a given size. To this end, we first draw a connection to the sparse matrix compression problem, which makes our problem NP-complete for alphabet sizes linear to the number of nodes. We further propose a reduction from the restricted directed Hamiltonian path problem, leading to NP-completeness even for logarithmic-sized alphabets.","Thu, 7 Mar 2024 23:41:01 UTC (787 KB)"
"13","A Mixed-Integer Conic Program for the Moving-Target Traveling Salesman Problem based on a Graph of Convex Sets","Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset","Robotics (cs.RO)","This paper introduces a new formulation that finds the optimum for the Moving-Target Traveling Salesman Problem (MT-TSP), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. The formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. The problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. We compare our formulation with the current state-of-the-art Mixed Integer Conic Program (MICP) solver for the MT-TSP. The experimental results show that our formulation outperforms the MICP for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. We also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower bounds for the MT-TSP than the ones from the MICP.","Thu, 7 Mar 2024 22:03:36 UTC (963 KB)"
"14","Towards Robust Data-Driven Automated Recovery of Symbolic Conservation Laws from Limited Data","Tracey Oellerich, Maria Emelianenko","Numerical Analysis (math.NA)","Conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. If the form of the underlying dynamical system is known, linear algebra and algebraic geometry methods can be used to identify the conservation laws. Our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. Building in part upon the ideas proposed in [arXiv:1811.00961], we develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. We demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. While we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.","Thu, 7 Mar 2024 20:48:18 UTC (1,551 KB)"
"15","Improved Lower Bound for Differentially Private Facility Location","Pasin Manurangsi","Data Structures and Algorithms (cs.DS)","We consider the differentially private (DP) facility location problem in the so called super-set output setting proposed by Gupta et al. [SODA 2010]. The current best known expected approximation ratio for an $\epsilon$-DP algorithm is $O\left(\frac{\log n}{\sqrt{\epsilon}}\right)$ due to Cohen-Addad et al. [AISTATS 2022] where $n$ denote the size of the metric space, meanwhile the best known lower bound is $\Omega(1/\sqrt{\epsilon})$ [NeurIPS 2019]. In this short note, we give a lower bound of $\tilde{\Omega}\left(\min\left\{\log n, \sqrt{\frac{\log n}{\epsilon}}\right\}\right)$ on the expected approximation ratio of any $\epsilon$-DP algorithm, which is the first evidence that the approximation ratio has to grow with the size of the metric space.","Thu, 7 Mar 2024 19:47:20 UTC (10 KB)"
"16","Corrective or Backfire: Characterizing and Predicting User Response to Social Correction","Bing He, Yingchen Ma, Mustaque Ahamad, Srijan Kumar","Social and Information Networks (cs.SI)","Online misinformation poses a global risk with harmful implications for society. Ordinary social media users are known to actively reply to misinformation posts with counter-misinformation messages, which is shown to be effective in containing the spread of misinformation. Such a practice is defined as ""social correction"". Nevertheless, it remains unknown how users respond to social correction in real-world scenarios, especially, will it have a corrective or backfire effect on users. Investigating this research question is pivotal for developing and refining strategies that maximize the efficacy of social correction initiatives. To fill this gap, we conduct an in-depth study to characterize and predict the user response to social correction in a data-driven manner through the lens of X (Formerly Twitter), where the user response is instantiated as the reply that is written toward a counter-misinformation message. Particularly, we first create a novel dataset with 55, 549 triples of misinformation tweets, counter-misinformation replies, and responses to counter-misinformation replies, and then curate a taxonomy to illustrate different kinds of user responses. Next, fine-grained statistical analysis of reply linguistic and engagement features as well as repliers' user attributes is conducted to illustrate the characteristics that are significant in determining whether a reply will have a corrective or backfire effect. Finally, we build a user response prediction model to identify whether a social correction will be corrective, neutral, or have a backfire effect, which achieves a promising F1 score of 0.816. Our work enables stakeholders to monitor and predict user responses effectively, thus guiding the use of social correction to maximize their corrective impact and minimize backfire effects. The code and data is accessible on this https URL.","Thu, 7 Mar 2024 19:06:42 UTC (603 KB)"
"17","Specifying and Verifying the Convergence Stairs of the Collatz Program","Ali Ebnenasir","Discrete Mathematics (cs.DM)","This paper presents an algorithmic method that, given a positive integer $j$, generates the $j$-th convergence stair containing all natural numbers from where the Collatz conjecture holds by exactly $j$ applications of the Collatz function. To this end, we present a novel formulation of the Collatz conjecture as a concurrent program, and provide the general case specification of the $j$-th convergence stair for any $j > 0$. The proposed specifications provide a layered and linearized orientation of Collatz numbers organized in an infinite set of infinite binary trees. To the best of our knowledge, this is the first time that such a general specification is provided, which can have significant applications in analyzing and testing the behaviors of complex non-linear systems. We have implemented this method as a software tool that generates the Collatz numbers of individual stairs. We also show that starting from any value in any convergence stair the conjecture holds. However, to prove the conjecture, one has to show that every natural number will appear in some stair; i.e., the union of all stairs is equal to the set of natural numbers, which remains an open problem.","Thu, 29 Feb 2024 17:36:54 UTC (949 KB)"
"18","SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker Assumptions","Ilias Diakonikolas, Daniel Kane, Lisheng Ren, Yuxin Sun","Machine Learning (cs.LG)","We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for this task that have been applicable to a wide range of contexts. In particular, it was known that for any univariate distribution $A$ satisfying certain conditions, distinguishing between a standard multivariate Gaussian and a distribution that behaves like $A$ in a random hidden direction and like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions were that (1) $A$ matches many low-order moments with the standard univariate Gaussian, and (2) the chi-squared norm of $A$ with respect to the standard Gaussian is finite. While the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. In this work, we establish that the latter condition is indeed not necessary. In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching condition only. Our result naturally generalizes to the setting of a hidden subspace. Leveraging our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.","Thu, 7 Mar 2024 18:49:32 UTC (32 KB)"
"19","A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation","Ankit Pensia","Data Structures and Algorithms (cs.DS)","We study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. Specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean $\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. In particular, the fastest runtime of existing algorithms is quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. Our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly}(k,\log d,1/\epsilon)$ samples. We also provide analogous results for robust sparse PCA. Our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by Valiant.","Thu, 7 Mar 2024 18:23:51 UTC (46 KB)"
"20","mmPlace: Robust Place Recognition with Intermediate Frequency Signal of Low-cost Single-chip Millimeter Wave Radar","Chengzhen Meng, Yifan Duan, Chenming He, Dequan Wang, Xiaoran Fan, Yanyong Zhang","Robotics (cs.RO)","Place recognition is crucial for tasks like loop-closure detection and re-localization. Single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments. However, it encounters two challenges. Firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data. Secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (FOV). We propose mmPlace, a robust place recognition system to address these challenges. Specifically, mmPlace transforms intermediate frequency (IF) signal into range azimuth heatmap and employs a spatial encoder to extract features. Additionally, to improve the performance in scenarios involving rotational and lateral variations, mmPlace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's FOV. We evaluate mmPlace's performance on the milliSonic dataset, which is collected on the University of Science and Technology of China (USTC) campus, the city roads surrounding the campus, and an underground parking garage. The results demonstrate that mmPlace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations.","Thu, 7 Mar 2024 17:53:37 UTC (8,296 KB)"
"21","On $[1,2]$-Domination in Interval and Circle Graphs","Mohsen Alambardar Meybodi, Abolfazl Poureidi","Computational Complexity (cs.CC)","A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.","Thu, 7 Mar 2024 17:43:21 UTC (523 KB)"
"22","Time-Aware Projections: Truly Node-Private Graph Statistics under Continual Observation","Palak Jain, Adam Smith, Connor Wagaman","Data Structures and Algorithms (cs.DS)","We describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). Previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. Our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. Our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms. We provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). To do so, we design new projection algorithms for graph streams, based on the batch-model techniques of Day et al. 2016 and Blocki et al. 2013, which modify the stream to limit its degree. Our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. Our transformation then follows a novel online variant of the Propose-Test-Release framework (Dwork and Lei, 2009), privately testing the safety condition before releasing output at each step.","Thu, 7 Mar 2024 16:14:08 UTC (155 KB)"
"23","Optimizing Inventory Placement for a Downstream Online Matching Problem","Boris Epstein, Will Ma (Columbia University)","Data Structures and Algorithms (cs.DS)","We study the inventory placement problem of splitting $Q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform. We compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: Offline, Myopic, and Fluid placement. On the theory side, we show that optimizing inventory placement for the Offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. We assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. The crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the Offline surrogate. We use statistical learning to show that rounding after optimizing a sample-average Offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss. On the experimental side, we extract real-world sequences of customer orders from publicly-available this http URL data and evaluate different combinations of placement and fulfillment procedures. Optimizing the Offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory.","Thu, 7 Mar 2024 15:46:19 UTC (508 KB)"
"24","Algorithms and complexity for path covers of temporal DAGs: when is Dilworth dynamic?","Dibyayan Chakraborty, Antoine Dailly, Florent Foucaud, Ralf Klasing","Data Structures and Algorithms (cs.DS)","In this paper, we study a dynamic analogue of the Path Cover problem, which can be solved in polynomial-time in directed acyclic graphs. A temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal DAG. A temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. Two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. A temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. In this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as Temporal Path Cover (TPC) and Temporally Disjoint Path Cover (TD-PC). We show that both problems are NP-hard even when the underlying DAG is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. Moreover, TD-PC remains NP-hard even on temporal oriented trees. In contrast, we show that TPC is polynomial-time solvable on temporal oriented trees by a reduction to Clique Cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which Clique Cover admits an efficient algorithm). This highlights an interesting algorithmic difference between the two problems. Although it is NP-hard on temporal oriented trees, TD-PC becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. We also show that TPC (resp. TD-PC) admits an XP (resp. FPT) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.","Thu, 7 Mar 2024 15:35:36 UTC (34 KB)"
"25","A Simple and Near-Optimal Algorithm for Directed Expander Decompositions","Aurelio L. Sulser, Maximilian Probst Gutenberg","Data Structures and Algorithms (cs.DS)","In this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\tilde{O}(m)$. Further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times. Our result improves over previous algorithms of Bernstein-Probst Gutenberg-Saranurak (FOCS 2020), Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) that only obtained algorithms optimal up to subpolynomial factors. At the same time, our algorithm is much simpler and more accessible than previous work. In order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of Goldberg-Tarjan (JACM 1988), which was later dynamized for computing expander decompositions in undirected graphs by Henzinger-Rao-Wang (SIAM J. Comput. 2020), Saranurak-Wang (SODA 2019). We then show that the flow problems formulated in recent work of Hua-Kyng-Probst Gutenberg-Wu (SODA 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework.","Thu, 7 Mar 2024 14:40:07 UTC (47 KB)"
"26","Conflict and Fairness in Resource Allocation","Susobhan Bandopadhyay, Aritra Banik, Sushmita Gupta, Pallavi Jain, Abhishek Sahu, Saket Saurabh, Prafullkumar Tale","Computer Science and Game Theory (cs.GT)","In the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents' welfare is maximized. Motivated by job scheduling, interest in this problem dates back to the work of Deuermeyer et al. [SIAM J. on Algebraic Discrete Methods'82]. Recent works consider the compatibility between resources and assign only mutually compatible resources to an agent. We study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\it conflict graph} on the set of resources (where an edge denotes incompatibility). The goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources. Chiarelli et al. [Algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \NP-hard. In this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters.","Thu, 7 Mar 2024 06:57:20 UTC (352 KB)"
"27","Switching Classes: Characterization and Computation","Dhanyamol Antony, Yixin Cao, Sagartanu Pal, R.B. Sandeep","Data Structures and Algorithms (cs.DS)","In a graph, the switching operation reverses adjacencies between a subset of vertices and the others. For a hereditary graph class $\mathcal{G}$, we are concerned with the maximum subclass and the minimum superclass of $\mathcal{G}$ that are closed under switching. We characterize the maximum subclass for many important classes $\mathcal{G}$, and prove that it is finite when $\mathcal{G}$ is minor-closed and omits at least one graph. For several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. We also show that the recognition of the superclass is NP-complete for $H$-free graphs when $H$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the Exponential Time Hypothesis.","Thu, 7 Mar 2024 06:55:42 UTC (52 KB)"
"28","Equivalence Testing: The Power of Bounded Adaptivity","Diptarka Chakraborty, Sourav Chakraborty, Gunjan Kumar, Kuldeep S. Meel","Data Structures and Algorithms (cs.DS)","Equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance. Conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester). Given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization. Despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\tilde{O}(\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage. Therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity. Our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\tilde{O}(\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing.","Thu, 7 Mar 2024 05:23:06 UTC (30 KB)"
"29","FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of Negative Federated Learning","Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu","Machine Learning (cs.LG)","Federated learning (FL) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. It works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. However, FL may fail to function appropriately when the federation is not ideal, amid an unhealthy state called Negative Federated Learning (NFL), in which most clients gain no benefit from participating in FL. Many studies have tried to address NFL. However, their solutions either (1) predetermine to prevent NFL in the entire learning life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds. Thus, they either (1) indiscriminately incur extra costs even if FL can perform well without such costs or (2) waste numerous learning rounds. Additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed NFL solutions when using those solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a holistic framework that can be employed on any FL system for tackling NFL in a run-time paradigm. That is, to dynamically detect NFL at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. Specifically, we devise a cost-effective NFL detection mechanism, which relies on an estimation of performance gain on clients. Only when NFL is detected, we activate the NFL recovery process, in which each client learns in parallel an adapted model when training the global model. Extensive experiment results confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL to a healthy learning state. We also show that FL-GUARD is compatible with previous NFL solutions and robust against clients unwilling/unable to take any recovery measures.","Thu, 7 Mar 2024 01:52:05 UTC (2,187 KB)"
"30","Optimal Scheduling of Graph States via Path Decompositions","Samuel J. Elman, Jason Gavriel, Ryan L. Mann","Quantum Physics (quant-ph)","We study the optimal scheduling of graph states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of graphs. We define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width. Our analysis shows that approximating the spatial cost of a graph is $\textsf{NP}$-hard, while for graphs with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule.","Thu, 7 Mar 2024 00:50:02 UTC (13 KB)"
"31","Understanding Biology in the Age of Artificial Intelligence","Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro Liò, Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig","Artificial Intelligence (cs.AI)","Modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ML) models. Although ML is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. As such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. Here, we draw from an epistemological toolkit to contextualize recent applications of ML in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ML systems to model biological phenomena and advance scientific knowledge. We propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ML-mediated understanding of biological systems. Through a detailed analysis of two key application areas of ML in modern biological research - protein structure prediction and single cell RNA-sequencing - we explore how these features have thus far enabled ML systems to advance scientific understanding of their target phenomena, how they may guide the development of future ML models, and the key obstacles that remain in preventing ML from achieving its potential as a tool for biological discovery. Consideration of the epistemological features of ML applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.","Wed, 6 Mar 2024 23:20:34 UTC (1,634 KB)"
"32","Semi-Supervised Dialogue Abstractive Summarization via High-Quality Pseudolabel Selection","Jianfeng He, Hang Su, Jason Cai, Igor Shalyminov, Hwanjun Song, Saab Mansour","Computation and Language (cs.CL)","Semi-supervised dialogue summarization (SSDS) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. While addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. However, these methods are not directly applicable to SSDS, as it is a generative task, and each dialogue can be summarized in different ways. In this work, we propose a novel scoring approach, SiCF, which encapsulates three primary dimensions of summarization model quality: Semantic invariance (indicative of model confidence), Coverage (factual recall), and Faithfulness (factual precision). Using the SiCF score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. Comprehensive experiments on three public datasets demonstrate the effectiveness of SiCF scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. Our code is available at \url{this https URL}.","Wed, 6 Mar 2024 22:06:23 UTC (1,350 KB)"
"33","A Sierpinski Triangle Data Structure for Efficient Array Value Update and Prefix Sum Calculation","Brent Harrison, Jason Necaise, Andrew Projansky, James D. Whitfield","Data Structures and Algorithms (cs.DS)","The binary indexed tree, or Fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array. It allows both of these operations to be performed in $O(\log_2 N)$ time. Here we present a novel data structure resembling the Sierpinski triangle, which accomplishes these operations with the same memory usage in $O(\log_3 N)$ time instead. We show this order to be optimal by making use of a connection to quantum computing.","Wed, 6 Mar 2024 19:01:04 UTC (170 KB)"
"34","On Outer Bi-Lipschitz Extensions of Linear Johnson-Lindenstrauss Embeddings of Subsets of $\mathbb{R}^N$","Rafael Chiclana, Mark A. Iwen, Mark Philip Roach","Metric Geometry (math.MG)","The celebrated Johnson-Lindenstrauss lemma states that for all $\varepsilon \in (0,1)$ and finite sets $X \subseteq \mathbb{R}^N$ with $n>1$ elements, there exists a matrix $\Phi \in \mathbb{R}^{m \times N}$ with $m=\mathcal{O}(\varepsilon^{-2}\log n)$ such that \[ (1 - \varepsilon) \|x-y\|_2 \leq \|\Phi x-\Phi y\|_2 \leq (1+\varepsilon)\| x- y\|_2 \quad \forall\, x, y \in X.\] Herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the Johnson-Lindenstrauss lemma for finite sets. After a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $X \subseteq \mathbb{R}^N$, and then specialize our generalized results to the case where $X$ is a low-dimensional compact submanifold of $\mathbb{R}^N$. In particular, we prove the following generalization of the Johnson-Lindenstrauss lemma: For all $\varepsilon \in (0,1)$ and $X\subseteq\mathbb{R}^N$, there exists a terminal embedding $f: \mathbb{R}^N \longrightarrow \mathbb{R}^{m}$ such that $$(1 - \varepsilon) \| x - y \|_2 \leq \left\| f(x) - f(y) \right\|_2 \leq (1 + \varepsilon) \| x - y \|_2 \quad \forall \, x \in X ~{\rm and}~ \forall \, y \in \mathbb{R}^N.$$ Crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=\mathcal{O}(\varepsilon^{-2} \omega^2(S_X))$, where $\omega(S_X)$ is the Gaussian width of the set of unit secants of $X$, $S_X=\overline{\{(x-y)/\|x-y\|_2 \colon x \neq y \in X\}}$. Furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice.","Wed, 6 Mar 2024 18:04:45 UTC (1,133 KB)"
"35","On HTLC-Based Protocols for Multi-Party Cross-Chain Swaps","Emily Clark, Chloe Georgiou, Katelyn Poon, Marek Chrobak","Data Structures and Algorithms (cs.DS)","In his 2018 paper, Herlihy introduced an atomic protocol for multi-party asset swaps across different blockchains. His model represents an asset swap by a directed graph whose nodes are the participating parties and edges represent asset transfers, and rational behavior of the participants is captured by a preference relation between a protocol's outcomes. Asset transfers between parties are achieved using smart contracts. These smart contracts are quite involved and they require storage and processing of a large number of paths in the swap digraph, limiting practical significance of his protocol. His paper also describes a different protocol that uses only standard hash time-lock contracts (HTLC's), but this simpler protocol applies only to some special types of digraphs. He left open the question whether there is a simple and efficient protocol for cross-chain asset swaps in arbitrary digraphs. Motivated by this open problem, we conducted a comprehensive study of \emph{HTLC-based protocols}, in which all asset transfers are implemented with HTLCs. Our main contribution is a full characterization of swap digraphs that have such protocols.","Wed, 6 Mar 2024 18:08:01 UTC (421 KB)"
"36","Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications","Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit Pensia, Kevin Tian","Numerical Analysis (math.NA)","The $k$-principal component analysis ($k$-PCA) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. In statistical settings, the goal of $k$-PCA is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. Motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-PCA algorithms, where we model access to the unknown target matrix via a black-box $1$-PCA oracle which returns an approximate top eigenvector, under two popular notions of approximation. Despite being arguably the most natural reduction-based approach to $k$-PCA algorithm design, such black-box methods, which recursively call a $1$-PCA oracle $k$ times, were previously poorly-understood. Our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-PCA. For a quadratic form notion of approximation we term ePCA (energy PCA), we show deflation methods suffer no parameter loss. For an alternative well-studied approximation notion we term cPCA (correlation PCA), we tightly characterize the parameter regimes where deflation methods are feasible. Moreover, we show that in all feasible regimes, $k$-cPCA deflation algorithms suffer no asymptotic parameter loss for any constant $k$. We apply our framework to obtain state-of-the-art $k$-PCA algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.","Wed, 6 Mar 2024 18:07:20 UTC (91 KB)[v2] Thu, 7 Mar 2024 02:12:55 UTC (91 KB)"
"37","Digitality as a ""longue durèe"" historical phenomenon","Salvatore Spina","Computers and Society (cs.CY)","The digital age introduced the Digital Ecological Niche (DEN), revolutionizing human interactions. The advent of Digital History (DHy) has marked a methodological shift in historical studies, tracing its roots to Babbage and Lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" This evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by Babbage and Lovelace's contributions to mathematical linguistic systems, laying the groundwork for Computer Science. This field, central to 20th-century mainframe interaction through programming languages and formalization, situates Digital History within a broader historical context. Here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. Nonetheless, the extent to which computation and Turing machines can fully understand and interpret history remains a subject of debate.","Wed, 6 Mar 2024 17:18:37 UTC (344 KB)"
"38","Parameterized Algorithms for Balanced Cluster Edge Modification Problems","Jayakrishnan Madathil, Kitty Meeks","Data Structures and Algorithms (cs.DS)","We introduce Cluster Edge Modification problems with constraints on the size of the clusters and study their complexity. A graph $G$ is a cluster graph if every connected component of $G$ is a clique. In a typical Cluster Edge Modification problem such as the widely studied Cluster Editing, we are given a graph $G$ and a non-negative integer $k$ as input, and we have to decide if we can turn $G$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges. In this paper, we study the parameterized complexity of such problems, but with an additional constraint: The size difference between any two connected components of the resulting cluster graph should not exceed a given threshold. Depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems. We show that all three problems, when parameterized by $k$, admit single-exponential time FPT algorithms and polynomial kernels. Our problems may be thought of as the size-constrained or balanced counterparts of the typical Cluster Edge Modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-Means Clustering.","Wed, 6 Mar 2024 16:19:35 UTC (72 KB)"
"39","In the Search of Optimal Tree Networks: Hardness and Heuristics","Maxim Buzdalov, Pavel Martynov, Sergey Pankratov, Vitaly Aksenov, Stefan Schmid","Networking and Internet Architecture (cs.NI)","Demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve. These networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters. In this work, we consider networks with one of the most common topologies~ -- a binary tree. We show that finding an optimal demand-aware binary tree network is NP-hard. Then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads.","Wed, 6 Mar 2024 14:15:01 UTC (140 KB)"
"40","Largest common subgraph of two forests","Dieter Rautenbach, Florian Werner","Data Structures and Algorithms (cs.DS)","A common subgraph of two graphs $G_1$ and $G_2$ is a graph that is isomorphic to subgraphs of $G_1$ and $G_2$. In the largest common subgraph problem the task is to determine a common subgraph for two given graphs $G_1$ and $G_2$ that is of maximum possible size ${\rm lcs}(G_1,G_2)$. This natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains NP-hard even restricted to unions of paths. We present a simple $4$-approximation algorithm for forests, and, for every fixed $\epsilon\in (0,1)$, we show that, for two given forests $F_1$ and $F_2$ of order at most $n$, one can determine in polynomial time a common subgraph $F$ of $F_1$ and $F_2$ with at least ${\rm lcs}(F_1,F_2)-\epsilon n$ edges. Restricted to instances with ${\rm lcs}(F_1,F_2)\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. Our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $O(\log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions.","Wed, 6 Mar 2024 13:25:41 UTC (18 KB)"
"41","Wildest Dreams: Reproducible Research in Privacy-preserving Neural Network Training","Tanveer Khan, Mindaugas Budzys, Khoa Nguyen, Antonis Michalas","Cryptography and Security (cs.CR)","Machine Learning (ML), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ML models require substantial computing power and are only as powerful as the data utilized. Due to high computational cost of ML methods, data scientists frequently use Machine Learning-as-a-Service (MLaaS) to outsource computation to external servers. However, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. Recent advances in Privacy-Preserving Techniques (PPTs) have enabled ML training and inference over protected data through the use of Privacy-Preserving Machine Learning (PPML). However, these techniques are still at a preliminary stage and their application in real-world situations is demanding. In order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of PPML, focusing on Homomorphic Encryption (HE) and Secure Multi-party Computation (SMPC) applied to ML. This work primarily focuses on the ML model's training phase, where maintaining user data privacy is of utmost importance. We provide a solid theoretical background that eases the understanding of current approaches and their limitations. In addition, we present a SoK of the most recent PPML frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. Also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. We believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in PPML, specifically regarding open-source availability, reproducibility, and usability.","Wed, 6 Mar 2024 10:25:36 UTC (1,049 KB)"
"42","Studying ECG signals using nonlinear oscillators and Genetic Algorithm","Sourav Chowdhury, Apratim Ghosal, Suparna Roychowhury, Indranath Chaudhuri","Medical Physics (physics.med-ph)","Cardiovascular diseases are the leading cause of death and disability in the world and thus their detection is extremely important as early as possible so that it can be prognosed and managed appropriately. Hence, electrophysiological models dealing with cardiac conduction are critically important in the field of interdisciplinary sciences. The primary aim of this paper is to reproduce a normal sinus rhythm ECG waveform which will act as the baseline for fitting and then fit any clinical ECG waveform that does not deviate much from normal sinus rhythm. To reproduce the ECG, we modeled the pacemaker complex using three coupled van der Pol (VDP) oscillators with appropriate delays to generate the action potentials. These action potentials are responsible for the excitation of the non-pacemaker cells of the atria and ventricles whose electrical activity gets recorded as the ECG signal. The ECG signal is composed of a periodic set of individual waves corresponding to atrial and ventricular contraction and relaxation. These waves are modeled with the help of four FitzHugh-Nagumo (FHN) equations with impulses corresponding to the action potentials generated by the pacemaker cells. After the successful reproduction of a normal sinus rhythm ECG, we have developed a framework where we have used genetic algorithm (GA) to fit a given clinical ECG data with parameters belonging to the above mentioned system of delay differential equations (DDEs). The GA framework has enabled us to fit ECG data representing different cardiac conditions reasonably well. We aim to use this work to get a better understanding of the cardiac conduction system and cardiovascular diseases which will help humanity in the future.","Wed, 6 Mar 2024 10:11:39 UTC (7,566 KB)"
"43","Graph Visualization for Blockchain Data","Marcell Dietl, Andre Gemünd, Daniel Oeltz, Felix M. Thiele, Christian Werner","Data Structures and Algorithms (cs.DS)","In this report, we introduce a novel approach to visualize extremely large graphs efficiently. Our method combines two force-directed algorithms, Kamada-Kawai and ForceAtlas2, to handle different graph components based on their node count. Additionally, we suggest utilizing the Fast Multipole method to enhance the speed of ForceAtlas2. Although initially designed for analyzing bitcoin transaction graphs, for which we present results here, this algorithm can also be applied to other crypto currency transaction graphs or graphs from diverse domains.","Wed, 6 Mar 2024 07:19:53 UTC (15,983 KB)"
"44","Double Exponential Lower Bound for Telephone Broadcast","Prafullkumar Tale","Data Structures and Algorithms (cs.DS)","Consider the Telephone Broadcast problem in which an input is a connected graph $G$ on $n$ vertices, a source vertex $s \in V(G)$, and a positive integer $t$. The objective is to decide whether there is a broadcast protocol from $s$ that ensures that all the vertices of $G$ get the message in at most $t$ rounds. We consider the broadcast protocol where, in a round, any node aware of the message can forward it to at most one of its neighbors. As the number of nodes aware of the message can at most double at each round, for a non-trivial instance we have $n \le 2^t$. Hence, the brute force algorithm that checks all the permutations of the vertices runs in time $2^{2^{\calO(t)}} \cdot n^{\calO(1)}$. As our first result, we prove this simple algorithm is the best possible in the following sense. Telephone Broadcast does not admit an algorithm running in time $2^{2^{o(t)}} \cdot n^{\calO(1)}$, unless the Ð fails. To the best of our knowledge, this is only the fourth example of \NP-Complete problem that admits a double exponential lower bound when parameterized by the solution size. It also resolves the question by Fomin, Fraigniaud, and Golovach [WG 2023]. In the same article, the authors asked whether the problem is \FPT\ when parameterized by the feedback vertex set number of the graph. We answer this question in the negative. Telephone Broadcast, when restricted to graphs of the feedback vertex number one, and hence treewidth of two, is \NP-\complete. We find this a relatively rare example of problems that admit a polynomial-time algorithm on trees but is \NP-\complete\ on graphs of treewidth two.","Wed, 6 Mar 2024 07:08:38 UTC (2,424 KB)"
"45","Undergraduate data science education: Who has the microphone and what are they saying?","Mine Dogucu, Sinem Demirci, Harry Bendekgey, Federica Zoe Ricci, Catalina M. Medina","Other Statistics (stat.OT)","The presence of data science has been profound in the scientific community in almost every discipline. An important part of the data science education expansion has been at the undergraduate level. We conducted a systematic literature review to (1) specify current evidence and knowledge gaps in undergraduate data science education and (2) inform policymakers and data science educators/practitioners about the present status of data science education research. The majority of the publications in data science education that met our search criteria were available open-access. Our results indicate that data science education research lacks empirical data and reproducibility. Not all disciplines contribute equally to the field of data science education. Computer science and data science as a separate field emerge as the leading contributors to the literature. In contrast, fields such as statistics, mathematics, as well as other fields closely related to data science exhibit a limited presence in studies. We recommend that federal agencies and researchers 1) invest in empirical data science education research; 2) diversify research efforts to enrich the spectrum of types of studies; 3) encourage scholars in key data science fields that are currently underrepresented in the literature to contribute more to research and publications.","Wed, 6 Mar 2024 00:49:08 UTC (49 KB)"
"46","Fine-Grained Privacy Guarantees for Coverage Problems","Laxman Dhulipala, George Z. Li","Data Structures and Algorithms (cs.DS)","We introduce a new notion of neighboring databases for coverage problems such as Max Cover and Set Cover under differential privacy. In contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. We illustrate several scenarios of Set Cover and Max Cover where our privacy notion is desired one for the application. Our main result is an $\epsilon$-edge differentially private algorithm for Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(k/\epsilon))$-approximation with high probability. Furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\Omega(k/\epsilon)$ is necessary under edge-differential privacy. Via group privacy properties, this implies a new algorithm for $\epsilon$-node differentially private Max Cover which obtains an $(1-1/e-\eta,\tilde{O}(fk/\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. When $f\ll k$, this improves over the best known algorithm for Max Cover under pure (node) differential privacy, which obtains an $(1-1/e,\tilde{O}(k^2/\epsilon))$-approximation.","Tue, 5 Mar 2024 21:40:10 UTC (50 KB)"
"47","Deep Configuration Performance Learning: A Systematic Survey and Taxonomy","Jingzhi Gong, Tao Chen","Software Engineering (cs.SE)","Performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. Our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. We also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. To promote open science, all the raw results of this survey can be accessed at our repository: this https URL.","Tue, 5 Mar 2024 21:05:16 UTC (1,311 KB)"
"48","Maintaining Light Spanners via Minimal Updates","Hadi Khodabandeh, David Eppstein","Computational Geometry (cs.CG)","We study the problem of maintaining a lightweight bounded-degree $(1+\varepsilon)$-spanner of a dynamic point set in a $d$-dimensional Euclidean space, where $\varepsilon>0$ and $d$ are arbitrary constants. In our fully-dynamic setting, points are allowed to be inserted as well as deleted, and our objective is to maintain a $(1+\varepsilon)$-spanner that has constant bounds on its maximum degree and its lightness (the ratio of its weight to that of the minimum spanning tree), while minimizing the recourse, which is the number of edges added or removed by each point insertion or deletion. We present a fully-dynamic algorithm that handles point insertion with amortized constant recourse and point deletion with amortized $O(\log\Delta)$ recourse, where $\Delta$ is the aspect ratio of the point set.","Tue, 5 Mar 2024 19:45:11 UTC (27 KB)"
"49","Shuffling Momentum Gradient Algorithm for Convex Optimization","Trang H. Tran, Quoc Tran-Dinh, Lam M. Nguyen","Optimization and Control (math.OC)","The Stochastic Gradient Descent method (SGD) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. In the last decades, researchers have made substantial effort to study the theoretical performance of SGD and its shuffling variants. However, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and Nesterov's momentum for convex settings. In this work, we extend the analysis of the shuffling momentum gradient method developed in [Tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. We provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $O(1/nT^2)$, where $n$ is the number of samples and $T$ is the number of training epochs. Our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.","Tue, 5 Mar 2024 18:19:02 UTC (395 KB)"
"50","Jovian sodium nebula and Io plasma torus S$^+$ and brightnesses 2017 -- 2023: insights into volcanic vs. sublimation supply","Jeffrey P. Morgenthaler (1), Carl A. Schmidt (2), Marissa F. Vogt (1), Nicholas M. Schneider (3), Max Marconi (1) ((1) Planetary Science Institute, (2) Center for Space Physics Boston University, (3) University Of Colorado, Boulder)","Earth and Planetary Astrophysics (astro-ph.EP)","We present first results derived from the largest collection of contemporaneously recorded Jovian sodium nebula and Io plasma torus (IPT) in [S II] 673.1 nm images assembled to date. The data were recorded by the Planetary Science Institute's Io Input/Output observatory (IoIO) and provide important context to Io geologic and atmospheric studies as well as the Juno mission and supporting observations. Enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from Io is determined by the enhancements, not any quiescent state. The enhancements are not seen at periodicities associated with modulation in solar insolation of Io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of Io's atmospheric escape. We suggest that geologic activity, likely involving volcanic plumes, drives escape.","Tue, 5 Mar 2024 17:16:37 UTC (1,423 KB)"
"51","The Exchange Problem","Mohit Garg, Suneel Sarswat","Data Structures and Algorithms (cs.DS)","Auctions are widely used in exchanges to match buy and sell requests. Once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. The two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. In this work, we study the algorithmic complexity of the problems arising from these matching tasks. We present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $O(n\log n)$ time to match $n$ requests. For dynamic price matching, we establish a lower bound of $\Omega(n \log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.","Tue, 5 Mar 2024 15:30:24 UTC (20 KB)"
"52","Cover Edge-Based Novel Triangle Counting","David A. Bader, Fuhuan Li, Zhihui Du, Palina Pauliuchenka, Oliver Alvarado Rodriguez, Anant Gupta, Sai Sri Vastav Minnal, Valmik Nahata, Anya Ganeshan, Ahmet Gundogdu, Jason Lew","Data Structures and Algorithms (cs.DS)","Listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality. In this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. Leveraging the breadth-first search (BFS) method, we can quickly generate a compact cover-edge set. Novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. The novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the Graph500 Benchmark and the MIT/Amazon/IEEE Graph Challenge. We implement 22 sequential algorithms for performance evaluation and comparison. At the same time, we employ OpenMP to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. Furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs. In our estimate from massive-scale Graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x. Comprehensive experiments are conducted on the recently launched Intel Xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.","Tue, 5 Mar 2024 14:24:48 UTC (546 KB)"
"53","Space Complexity of Euclidean Clustering","Xiaoyi Zhu, Yuxiang Tian, Lingxiao Huang, Zengfeng Huang","Computational Geometry (cs.CG)","The $(k, z)$-Clustering problem in Euclidean space $\mathbb{R}^d$ has been extensively studied. Given the scale of data involved, compression methods for the Euclidean $(k, z)$-Clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. However, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\varepsilon$, remains unclear in existing literature. This paper initiates the study of space complexity for Euclidean $(k, z)$-Clustering and offers both upper and lower bounds. Our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. Furthermore, our lower bound result for $(k, z)$-Clustering establishes a tight space bound of $\Theta( n d )$ for terminal embedding, where $n$ represents the dataset size. Our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.","Tue, 5 Mar 2024 13:49:32 UTC (1,557 KB)[v2] Wed, 6 Mar 2024 02:05:36 UTC (797 KB)"
"54","Hamiltonian Property Testing","Andreas Bluhm, Matthias C. Caro, Aadil Oufkir","Quantum Physics (quant-ph)","Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2^{n/2})$ many queries and $\Omega(2^{n/2}/\varepsilon)$ total evolution time. In contrast, when distances are measured according to the normalized Frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent Hamiltonian locality testing algorithm based on randomized measurements. In fact, our procedure can be used to simultaneously test a wide class of Hamiltonian properties beyond locality. Finally, we prove that learning a general Hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between Hamiltonian testing and learning. Our work initiates the study of property testing for quantum Hamiltonians, demonstrating that a broad class of Hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning Hamiltonian testing as an independent area of research alongside Hamiltonian learning.","Tue, 5 Mar 2024 13:44:28 UTC (276 KB)"
"55","Improving the quality of individual-level online information tracking: challenges of existing approaches and introduction of a new content- and long-tail sensitive academic solution","Silke Adam, Mykola Makhortykh, Michaela Maier, Viktor Aigenseer, Aleksandra Urman, Teresa Gil Lopez, Clara Christner, Ernesto de León, Roberto Ulloa","Computers and Society (cs.CY)","This article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues. To overcome some of these problems, the article introduces a new academic tracking solution, WebTrack, an open source tracking tool maintained by a major European research institution. The design logic, the interfaces and the backend requirements for WebTrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed. Finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through WebTrack leads to new innovative shifts in the processing of tracking data. As WebTrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news.","Tue, 5 Mar 2024 12:49:25 UTC (422 KB)"
"56","DynST: Dynamic Sparse Training for Resource-Constrained Spatio-Temporal Forecasting","Hao Wu, Haomin Wen, Guibin Zhang, Yutong Xia, Kai Wang, Yuxuan Liang, Yu Zheng, Kun Wang","Artificial Intelligence (cs.AI)","The ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. Concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. To alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. These methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. Regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. Worse still, the complex technical design may ultimately lead to a model with weak generalizability. In this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. To our knowledge, this is the first proposal (termed DynST) of an industry-level deployment optimization concept at the data level. However, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. To achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. During the training process, DynST utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.","Tue, 5 Mar 2024 12:31:24 UTC (7,643 KB)"
"57","Citizen Science and Machine Learning for Research and Nature Conservation: The Case of Eurasian Lynx, Free-ranging Rodents and Insects","Kinga Skorupska, Rafał Stryjek, Izabela Wierzbowska, Piotr Bebas, Maciej Grzeszczuk, Piotr Gago, Jarosław Kowalski, Maciej Krzywicki, Jagoda Lazarek, Wiesław Kopeć","Human-Computer Interaction (cs.HC)","Technology is increasingly used in Nature Reserves and National Parks around the world to support conservation efforts. Endangered species, such as the Eurasian Lynx (Lynx lynx), are monitored by a network of automatic photo traps. Yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. Therefore, researchers working in this area increasingly need support to process this incoming information. One opportunity is to seek support from volunteer Citizen Scientists who can help label the data, however, it is challenging to retain their interest. Another way is to automate the process with image recognition using convolutional neural networks. During the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of Citizen Science and Machine Learning to expedite the process of data preparation, labelling and analysis.","Tue, 5 Mar 2024 12:13:27 UTC (6,621 KB)"
"58","A Note on High-Probability Analysis of Algorithms with Exponential, Sub-Gaussian, and General Light Tails","Amit Attia, Tomer Koren","Machine Learning (cs.LG)","This short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. We show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. This approach simultaneously applies to any light-tailed randomization, including exponential, sub-Gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. Analyses of a generalized Azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.","Tue, 5 Mar 2024 11:38:20 UTC (11 KB)"
"59","HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents","Sam Yu-Te Lee, Kwan-Liu Ma","Human-Computer Interaction (cs.HC)","Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.","Tue, 5 Mar 2024 08:15:57 UTC (7,662 KB)"
"60","Neural Fractional Differential Equations","C. Coelho, M. Fernanda P. Costa, L.L. Ferrás","Machine Learning (cs.LG)","Fractional Differential Equations (FDEs) are essential tools for modelling complex systems in science and engineering. They extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours. This property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions. Having this in mind, and drawing inspiration from Neural Ordinary Differential Equations (Neural ODEs), we propose the Neural FDE, a novel deep neural network architecture that adjusts a FDE to the dynamics of data. This work provides a comprehensive overview of the numerical method employed in Neural FDEs and the Neural FDE architecture. The numerical outcomes suggest that, despite being more computationally demanding, the Neural FDE may outperform the Neural ODE in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.","Tue, 5 Mar 2024 07:45:29 UTC (629 KB)"
"61","Distributed OpenMP Offloading of OpenMC on Intel GPU MAX Accelerators","Yehonatan Fridman, Guy Tamir, Uri Steinitz, Gal Oren","Distributed, Parallel, and Cluster Computing (cs.DC)","Monte Carlo (MC) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. Harnessing the computational power of high-performance computing (HPC) systems, especially Graphics Processing Units (GPUs), has become essential for accelerating MC simulations. This paper focuses on the adaptation and optimization of the OpenMC neutron and photon transport Monte Carlo code for Intel GPUs, specifically the Intel Data Center Max 1100 GPU (codename Ponte Vecchio, PVC), through distributed OpenMP offloading. Building upon prior work by Tramm J.R., et al. (2022), which laid the groundwork for GPU adaptation, our study meticulously extends the OpenMC code's capabilities to Intel GPUs. We present a comprehensive benchmarking and scaling analysis, comparing performance on Intel MAX GPUs to state-of-the-art CPU execution (Intel Xeon Platinum 8480+ Processor, codename 4th generation Sapphire Rapids). The results demonstrate a remarkable acceleration factor compared to CPU execution, showcasing the GPU-adapted code's superiority over its CPU counterpart as computational load increases.","Tue, 5 Mar 2024 07:43:24 UTC (80 KB)"
"62","DGAP: Efficient Dynamic Graph Analysis on Persistent Memory","Abdullah Al Raqibul Islam, Dong Dai","Data Structures and Algorithms (cs.DS)","Dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. To accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices. Emerging persistent memory technologies, such as Optane DCPMM, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high IOPS together. In light of this, we propose DGAP, a framework for efficient dynamic graph analysis on persistent memory. Unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, DGAP utilizes a single mutable Compressed Sparse Row (CSR) graph structure with new designs for persistent memory to construct the framework. Specifically, DGAP introduces a \textit{per-section edge log} to reduce write amplification on persistent memory; a \textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. Our extensive evaluation results demonstrate that DGAP can achieve up to $3.2\times$ better graph update performance and up to $3.77\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as XPGraph, LLAMA, and GraphOne.","Tue, 5 Mar 2024 05:40:45 UTC (5,442 KB)"
"63","The Influence of Validation Data on Logical and Scientific Interpretations of Forensic Expert Opinions","Steven P. Lund, Hari Iyer","Applications (stat.AP)","Forensic experts use specialized training and knowledge to enable other members of the judicial system to make better informed and more just decisions. Factfinders, in particular, are tasked with judging how much weight to give to experts' reports and opinions. Many references describe assessing evidential weight from the perspective of a forensic expert. Some recognize that stakeholders are each responsible for evaluating their own weight of evidence. Morris (1971, 1974, 1977) provided a general framework for recipients to update their own uncertainties after learning an expert's opinion. Although this framework is normative under Bayesian axioms and several forensic scholars advocate the use of Bayesian reasoning, few resources describe its application in forensic science. This paper addresses this gap by examining how recipients can combine principles of science and Bayesian reasoning to evaluate their own likelihood ratios for expert opinions. This exercise helps clarify how an expert's role depends on whether one envisions recipients to be logical and scientific or deferential. Illustrative examples with an expert's opinion expressed as a categorical conclusion, likelihood ratio, or range of likelihood ratios, or with likelihood ratios from multiple experts, each reveal the importance and influence of validation data for logical recipients' interpretations.","Tue, 5 Mar 2024 05:22:05 UTC (2,352 KB)"
"64","Algorithms for Galois Words: Detection, Factorization, and Rotation","Diptarama Hendrian, Dominik Köppl, Ryo Yoshinaka, Ayumi Shinohara","Data Structures and Algorithms (cs.DS)","Lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [Bannai+, SIAM J. Comput.'17]. We can determine Lyndon words, factorize a word into Lyndon words in lexicographically decreasing order, and find the Lyndon rotation of a word, all in linear time within constant additional working space. A recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of Lyndon words. In particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed. While a Lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a Galois word exhibits the same property by exchanging the lexicographic order with the alternating order. Unfortunately, this exchange has a large impact on the properties Galois words exhibit, which makes it a nontrivial task to translate results from Lyndon words to Galois words. Up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of Duval's algorithm are possible for computing the Galois factorization or the Galois rotation. Here, we affirm this conjecture as follows. Given a word $T$ of length $n$, we can determine whether $T$ is a Galois word, in $O(n)$ time with constant additional working space. Within the same complexities, we can also determine the Galois rotation of $T$, and compute the Galois factorization of $T$ online. The last result settles Open Problem~1 in [Dolce et al., TCS'2019] for Galois words.","Tue, 5 Mar 2024 03:59:20 UTC (202 KB)"
"65","On Approximate Fully-Dynamic Matching and Online Matrix-Vector Multiplication","Yang P. Liu","Data Structures and Algorithms (cs.DS)","We study connections between the problem of fully dynamic $(1-\epsilon)$-approximate maximum bipartite matching, and the dual $(1+\epsilon)$-approximate vertex cover problem, with the online matrix-vector ($\mathsf{OMv}$) conjecture which has recently been used in several fine-grained hardness reductions. We prove that there is an online algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $n^{1-c}\epsilon^{-C}$ time for constants $c, C > 0$ for fully dynamic updates if and only if the $\mathsf{OMv}$ conjecture is false. Similarly, we prove that there is an online algorithm that maintains a $(1-\epsilon)$-approximate maximum matching in amortized $n^{1-c}\epsilon^{-C}$ time if and only if there is a nontrivial algorithm for another dynamic problem, which we call dynamic approximate $\mathsf{OMv}$, that has seemingly no matching structure. This provides some evidence against achieving amortized sublinear update times for approximate fully dynamic matching and vertex cover. Leveraging these connections, we obtain faster algorithms for approximate fully dynamic matching in both the online and offline settings. 1. We give a randomized algorithm that with high probability maintains a $(1-\epsilon)$-approximate bipartite matching and $(1+\epsilon)$-approximate vertex cover in fully dynamic graphs, in amortized $O(\epsilon^{-O(1)} \frac{n}{2^{\Omega(\sqrt{\log n})}})$ update time. Our algorithm leverages fast algorithms for $\mathsf{OMv}$ due to Larsen-Williams [SODA 2017]. 2. We give a randomized offline algorithm for $(1-\epsilon)$-approximate maximum matching with amortized runtime $O(n^{.58}\epsilon^{-O(1)})$ by using fast matrix multiplication, significantly improving over the runtimes achieved via online algorithms. We also give an offline algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $O(n^{.723}\epsilon^{-O(1)})$ time.","Tue, 5 Mar 2024 01:33:09 UTC (39 KB)"
"66","Born Accessible Data Science and Visualization Courses: Challenges of Developing Curriculum to be Taught by Blind Instructors to Blind Students","JooYoung Seo, Sile O'Modhrain, Yilin Xia, Sanchita Kamath, Bongshin Lee, James M. Coughlan","Human-Computer Interaction (cs.HC)","While recent years have seen a growing interest in accessible visualization tools and techniques for blind people, little attention is paid to the learning opportunities and teaching strategies of data science and visualization tailored for blind individuals. Whereas the former focuses on the accessibility issues of data visualization tools, the latter is concerned with the learnability of concepts and skills for data science and visualization. In this paper, we present novel approaches to teaching data science and visualization to blind students in an online setting. Taught by blind instructors, nine blind learners having a wide range of professional backgrounds participated in a two-week summer course. We describe the course design, teaching strategies, and learning outcomes. We also discuss the challenges and opportunities of teaching data science and visualization to blind students. Our work contributes to the growing body of knowledge on accessible data science and visualization education, and provides insights into the design of online courses for blind students.","Tue, 5 Mar 2024 00:49:13 UTC (1,278 KB)"
"67","A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing","Yu Wang","Computation and Language (cs.CL)","The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets. The tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.","Mon, 4 Mar 2024 21:51:11 UTC (1,091 KB)"
"68","End-to-end variational quantum sensing","Benjamin MacLellan, Piotr Roztocki, Stefanie Czischek, Roger G. Melko","Quantum Physics (quant-ph)","Harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering. Real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging. Numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice. Here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively. The framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ansätze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation. End-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors.","Mon, 4 Mar 2024 19:00:04 UTC (428 KB)"
"69","nimCSO: A Nim package for Compositional Space Optimization","Adam M. Krajewski, Arindam Debnath, Wesley F. Reinhart, Allison M. Beese, Zi-Kui Liu","Materials Science (cond-mat.mtrl-sci)","nimCSO is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning. Making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. Such spaces are encountered, for instance, in materials science, where datasets on Compositionally Complex Materials (CCMs) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions. At its core, nimCSO leverages the metaprogramming ability of the Nim language (this http URL) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. As demonstrated in this paper, nimCSO reaches the physical limits of the hardware (L1 cache latency) and can outperform an efficient native Python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming NumPy implementation 35 and 17 times, respectively, when checking a candidate solution. It is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability. All configuration is done with a simple human-readable YAML config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills.","Mon, 4 Mar 2024 18:59:45 UTC (1,691 KB)"
"70","Contract Design for Pandora's Box","Martin Hoefer, Conrad Schecker, Kevin Schewior","Computer Science and Game Theory (cs.GT)","We study a natural application of contract design to search problems with probabilistic prior and exploration costs. These problems have a plethora of applications and are expressed concisely within the Pandora's Box model. Its optimal solution is the ingenious index policy proposed originally by Weitzman in 1979. In our principal-agent setting, the search task is delegated to an agent. The agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. Agent and principal obtain an individual value based on the selected prize. To influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. The goal of the principal to maximize her expected reward, i.e., value minus payment. We show how to compute optimal contracts for the principal in several scenarios. A popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. For general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. Interestingly, a suitable adaptation of the index policy results in an optimal contract here. More generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. These results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.","Mon, 4 Mar 2024 18:53:27 UTC (28 KB)"
"71","Statistical Query Lower Bounds for Learning Truncated Gaussians","Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis","Data Structures and Algorithms (cs.DS)","We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded number of rectangles whose VC dimension and Gaussian surface are small. As a corollary of our construction, it also follows that the complexity of the previously known algorithm for this task is qualitatively best possible.","Mon, 4 Mar 2024 18:30:33 UTC (43 KB)"
"72","Astronomy in Colombia: a bibliometric perspective","Sofía Guevara-Montoya, Felipe Ortiz-Ferreira, María Paula Silva-Arévalo, Paola A. Niño-Muñoz, Jaime E. Forero-Romero","Digital Libraries (cs.DL)","In Colombia, astronomical research is experiencing accelerated growth. In order to better understand its evolution and current state, we conducted a bibliometric study using data from the Astrophysics Data System (ADS) and Web of Science (WoS). In ADS, we identified 422 peer-reviewed publications from 1980, the year of the first publication, until 2023, which was the cutoff date for our study. Among the 25 Colombian institutions identified as participants in at least one publication, the contributions of four universities stand out: Universidad de los Andes, Universidad Nacional de Colombia, Universidad Industrial de Santander, and Universidad de Antioquia, with 104, 78, 68, and 67 publications, respectively. By cross-referencing information from ADS and WoS, we found that the areas with the greatest impact in publications are threefold: high-energy and fundamental physics, stars and stellar physics, and galaxies and cosmology. Globally, according to WoS, Colombia ranks 52nd in the number of peer-reviewed publications between 2019 and 2023, and fifth in Latin America. Additionally, we identified three highly cited publications (top 1% worldwide) belonging to the field of observational cosmology. When analyzing countries with equal or greater bibliographic production, we estimate that Colombian production is approximately four times lower than expected considering its population and GDP.","Mon, 4 Mar 2024 17:41:50 UTC (1,315 KB)"
"73","Constraint Satisfaction Problems with Advice","Suprovat Ghoshal, Konstantin Makarychev, Yury Makarychev","Data Structures and Algorithms (cs.DS)","We initiate the study of algorithms for constraint satisfaction problems with ML oracle advice. We introduce two models of advice and then design an approximation algorithm for Max Cut and Max 2-Lin in these models.","Mon, 4 Mar 2024 17:00:31 UTC (14 KB)"
"74","Matching Algorithms in the Sparse Stochastic Block Model","Anna Brandenberger, Byron Chin, Nathan S. Sheffield, Divya Shyamal","Data Structures and Algorithms (cs.DS)","The stochastic block model (SBM) is a generalization of the Erdős--Rényi model of random graphs that describes the interaction of a finite number of distinct communities. In sparse Erdős--Rényi graphs, it is known that a linear-time algorithm of Karp and Sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such graphs in terms of solutions to an ODE. We provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the Karp--Sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general SBM instances. We also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive. We show that the competitive ratio lower bound of 0.837 found by Mastin and Jaillet for the Erdős--Rényi case is tight whenever the expected degrees in all communities are equal. We propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality.","Mon, 4 Mar 2024 15:50:17 UTC (2,480 KB)"
"75","Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks","Emre Anakok, Pierre Barbillon, Colin Fontaine, Elisa Thebault","Machine Learning (stat.ML)","We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological networks when applied to the Spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias.","Mon, 4 Mar 2024 13:12:02 UTC (1,260 KB)"
"76","Faster MEM-finding in $O (r + \bar{r} + g)$ space","Travis Gagie","Data Structures and Algorithms (cs.DS)","Suppose we are given a text $T [1..n]$, a straight-line program with $g$ rules for $T$ and an assignment of tags to the characters in $T$ such that the Burrows-Wheeler Transform of $T$ has $r$ runs, the Burrows-Wheeler Transform of the reverse of $T$ has $\bar{r}$ runs and the tag array -- the list of tags in the lexicographic order of the suffixes starting at the characters the tags are assigned to -- has $t$ runs. If the alphabet size is at most polylogarithmic in $n$ then there is an $O (r + \bar{r} + g + t)$-space index for $T$ such that when we are given a pattern $P [1..m]$ we can compute the maximal exact matches (MEMs) of $P$ with respect to $T$ in $O (m)$ time plus $O (\log n)$ time per MEM and then list the distinct tags assigned to the first characters of occurrences of that MEM in constant time per tag listed, all correctly with high probability.","Mon, 4 Mar 2024 13:04:32 UTC (5 KB)"
"77","Ab initio path integral Monte Carlo simulations of warm dense two-component systems without fixed nodes: structural properties","Tobias Dornheim, Sebastian Schwalbe, Maximilian Böhme, Zhandos Moldabekov, Jan Vorberger, Panagiotis Tolias","Computational Physics (physics.comp-ph)","We present extensive new \emph{ab initio} path integral Monte Carlo (PIMC) results for a variety of structural properties of warm dense hydrogen and beryllium. To deal with the fermion sign problem -- an exponential computational bottleneck due to the antisymmetry of the electronic thermal density matrix -- we employ the recently proposed [\textit{J.~Chem.~Phys.}~\textbf{157}, 094112 (2022); \textbf{159}, 164113 (2023)] $\xi$-extrapolation method and find excellent agreement with exact direct PIMC reference data where available. This opens up the intriguing possibility to study a gamut of properties of light elements and potentially material mixtures over a substantial part of the warm dense matter regime, with direct relevance for astrophysics, material science, and inertial confinement fusion research.","Mon, 4 Mar 2024 12:23:05 UTC (1,064 KB)"
"78","Towards Deterministic Algorithms for Constant-Depth Factors of Constant-Depth Circuits","Mrinal Kumar, Varun Ramanathan, Ramprasad Saptharishi, Ben Lee Volk","Computational Complexity (cs.CC)","We design a deterministic subexponential time algorithm that takes as input a multivariate polynomial $f$ computed by a constant-depth circuit over rational numbers, and outputs a list $L$ of circuits (of unbounded depth and possibly with division gates) that contains all irreducible factors of $f$ computable by constant-depth circuits. This list $L$ might also include circuits that are spurious: they either do not correspond to factors of $f$ or are not even well-defined, e.g. the input to a division gate is a sub-circuit that computes the identically zero polynomial. The key technical ingredient of our algorithm is a notion of the pseudo-resultant of $f$ and a factor $g$, which serves as a proxy for the resultant of $g$ and $f/g$, with the advantage that the circuit complexity of the pseudo-resultant is comparable to that of the circuit complexity of $f$ and $g$. This notion, which might be of independent interest, together with the recent results of Limaye, Srinivasan and Tavenas, helps us derandomize one key step of multivariate polynomial factorization algorithms - that of deterministically finding a good starting point for Newton Iteration for the case when the input polynomial as well as the irreducible factor of interest have small constant-depth circuits.","Mon, 4 Mar 2024 12:09:16 UTC (43 KB)"
"79","Random Generation of Git Graphs","Julien Courtiel (GREYC), Martin Pépin (GREYC)","Data Structures and Algorithms (cs.DS)","Version Control Systems, such as Git and Mercurial, manage the history of a project as a Directed Acyclic Graph encoding the various divergences and synchronizations happening in its life cycle. A popular workflow in the industry, called the feature branch workflow, constrains these graphs to be of a particular shape: a unique main branch, and non-interfering feature branches. Here we focus on the uniform random generation of those graphs with n vertices, including k on the main branch, for which we provide three algorithms, for three different use-cases. The first, based on rejection, is efficient when aiming for small values of k (more precisely whenever k = O($\sqrt$ n)). The second takes as input any number k of commits in the main branch, but requires costly precalculation. The last one is a Boltzmann generator and enables us to generate very large graphs while targeting a constant k/n ratio. All these algorithms are linear in the size of their outputs.","Mon, 4 Mar 2024 10:01:00 UTC (187 KB)"
"80","The Canadian Traveller Problem on outerplanar graphs","Laurent Beaudou, Pierre Bergé, Vsevolod Chernyshev, Antoine Dailly, Yan Gerard, Aurélie Lagoutte, Vincent Limouzy, Lucas Pastor","Data Structures and Algorithms (cs.DS)","We study the PSPACE-complete $k$-Canadian Traveller Problem, where a weighted graph $G=(V,E,\omega)$ with a source $s\in V$ and a target $t\in V$ are given. This problem also has a hidden input $E_* \subsetneq E$ of cardinality at most $k$ representing blocked edges. The objective is to travel from $s$ to $t$ with the minimum distance. At the beginning of the walk, the blockages $E_*$ are unknown: the traveller discovers that an edge is blocked when visiting one of its endpoints. Online algorithms, also called strategies, have been proposed for this problem and assessed with the competitive ratio, i.e. the ratio between the distance actually traversed by the traveller divided by the distance we would have traversed knowing the blockages in advance. Even though the optimal competitive ratio is $2k+1$ even on unit-weighted planar graphs of treewidth 2, we design a polynomial-time strategy achieving competitive ratio $9$ on unit-weighted outerplanar graphs. This value $9$ also stands as a lower bound for this family of graphs as we prove that, for any $\varepsilon > 0$, no strategy can achieve a competitive ratio $9-\varepsilon$. Finally, we show that it is not possible to achieve a constant competitive ratio (independent of $G$ and $k$) on weighted outerplanar graphs.","Mon, 4 Mar 2024 09:29:29 UTC (51 KB)[v2] Fri, 8 Mar 2024 14:58:14 UTC (51 KB)"
"81","Schema-Based Query Optimisation for Graph Databases","Chandan Sharma (TYREX), Pierre Genevès (TYREX), Nils Gesbert (TYREX), Nabil Layaïda (TYREX)","Databases (cs.DB)","Recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. Graph data often come with schema information that describe how nodes and edges are organized. We propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. We show that this schema information can be useful in order to improve the performance when evaluating acylic recursive graph queries. Furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.","Mon, 4 Mar 2024 09:19:50 UTC (1,437 KB)"
"82","Fully Polynomial-time Algorithms Parameterized by Vertex Integrity Using Fast Matrix Multiplication","Matthias Bentert, Klaus Heeger, Tomohiro Koana","Data Structures and Algorithms (cs.DS)","We study the computational complexity of several polynomial-time-solvable graph problems parameterized by vertex integrity, a measure of a graph's vulnerability to vertex removal in terms of connectivity. Vertex integrity is the smallest number $\iota$ such that there is a set $S$ of $\iota' \le \iota$ vertices such that every connected component of $G-S$ contains at most $\iota-\iota'$ vertices. It is known that the vertex integrity lies between the well-studied parameters vertex cover number and tree-depth. Alon and Yuster [ESA 2007] designed algorithms for graphs with small vertex cover number using fast matrix multiplications. We demonstrate that fast matrix multiplication can also be effectively used when parameterizing by vertex integrity $\iota$ by developing efficient algorithms for problems including an $O(\iota^{\omega-1}n)$-time algorithm for computing the girth of a graph, randomized $O(\iota^{\omega - 1}n)$-time algorithms for Maximum Matching and for finding any induced four-vertex subgraph except for a clique or an independent set, and an $O(\iota^{(\omega-1)/2}n^2) \subseteq O(\iota^{0.687} n^2)$-time algorithm for All-Pairs Shortest Paths. These algorithms can be faster than previous algorithms parameterized by tree-depth, for which fast matrix multiplication is not known to be effective.","Mon, 4 Mar 2024 08:36:15 UTC (33 KB)"
"83","Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search","Lars Gottesbüren, Laxman Dhulipala, Rajesh Jayaram, Jakub Lacki","Data Structures and Algorithms (cs.DS)","We consider the fundamental problem of decomposing a large-scale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving shards, so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a routing algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines. In this paper, we design simple and highly efficient routing methods, and prove strong theoretical guarantees on their performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with any partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated partitioning method. In particular, our new routing methods enable the use of balanced graph partitioning, which is a high-quality partitioning method without a naturally associated routing algorithm. Thus, we provide the first methods for routing using balanced graph partitioning that are extremely fast to train, admit low latency, and achieve high recall. We provide a comprehensive evaluation of our full partitioning and routing pipeline on billion-scale datasets, where it outperforms existing scalable partitioning methods by significant margins, achieving up to 2.14x higher QPS at 90% recall$@10$ than the best competitor.","Mon, 4 Mar 2024 07:37:09 UTC (1,410 KB)"
"84","K-stars LDP: A Novel Framework for (p, q)-clique Enumeration under Local Differential Privacy","Henan Sun, Zhengyu Wu, Rong-Hua Li, Guoren Wang, Zening Li","Cryptography and Security (cs.CR)","(p,q)-clique enumeration on a bipartite graph is critical for calculating clustering coefficient and detecting densest subgraph. It is necessary to carry out subgraph enumeration while protecting users' privacy from any potential attacker as the count of subgraph may contain sensitive information. Most recent studies focus on the privacy protection algorithms based on edge LDP (Local Differential Privacy). However, these algorithms suffer a large estimation error due to the great amount of required noise. In this paper, we propose a novel idea of k-stars LDP and a novel k-stars LDP algorithm for (p, q)-clique enumeration with a small estimation error, where a k-stars is a star-shaped graph with k nodes connecting to one node. The effectiveness of edge LDP relies on its capacity to obfuscate the existence of an edge between the user and his one-hop neighbors. This is based on the premise that a user should be aware of the existence of his one-hop neighbors. Similarly, we can apply this premise to k-stars as well, where an edge is a specific genre of 1-stars. Based on this fact, we first propose the k-stars neighboring list to enable our algorithm to obfuscate the existence of k-stars with Warner' s RR. Then, we propose the absolute value correction technique and the k-stars sampling technique to further reduce the estimation error. Finally, with the two-round user-collector interaction mechanism, we propose our k-stars LDP algorithm to count the number of (p, q)-clique while successfully protecting users' privacy. Both the theoretical analysis and experiments have showed the superiority of our algorithm over the algorithms based on edge LDP.","Mon, 4 Mar 2024 07:30:10 UTC (3,763 KB)"
"85","Structured methods for parameter inference and uncertainty quantification for mechanistic models in the life sciences","Michael J. Plank, Matthew J. Simpson","Quantitative Methods (q-bio.QM)","Parameter inference and uncertainty quantification are important steps when relating mathematical models to real-world observations, and when estimating uncertainty in model predictions. However, methods for doing this can be computationally expensive, particularly when the number of unknown model parameters is large. The aim of this study is to develop and test an efficient profile likelihood-based method, which takes advantage of the structure of the mathematical model being used. We do this by identifying specific parameters that affect model output in a known way, such as a linear scaling. We illustrate the method by applying it to three caricature models from different areas of the life sciences: (i) a predator-prey model from ecology; (ii) a compartment-based epidemic model from health sciences; and, (iii) an advection-diffusion-reaction model describing transport of dissolved solutes from environmental science. We show that the new method produces results of comparable accuracy to existing profile likelihood methods, but with substantially fewer evaluations of the forward model. We conclude that our method could provide a much more efficient approach to parameter inference for models where a structure approach is feasible. Code to apply the new method to user-supplied models and data is provided via a publicly accessible repository.","Mon, 4 Mar 2024 02:13:58 UTC (289 KB)"
"86","Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa","L. Crow, S. J. Watts","Machine Learning (stat.ML)","The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification algorithm gives the confusion matrix and Kappa. Theory and methods are discussed in detail and then applied to Monte Carlo data and real datasets. Four very different real datasets - Breast Cancer, Coronary Heart Disease, Bankruptcy, and Particle Identification - are analysed, with both continuous and discrete values, and their classification performance compared to the expected theoretical limit. In all cases this analysis shows that the algorithms could not have performed any better due to the underlying probability density functions for the two classes. Important lessons are learnt on how to predict the performance of algorithms for imbalanced data using training datasets that are approximately balanced. Machine learning is very powerful but classification performance ultimately depends on the quality of the data and the relevance of the variables to the problem.","Sun, 3 Mar 2024 17:36:42 UTC (4,596 KB)"
"87","Approximations and Hardness of Packing Partially Ordered Items","Ilan Doron-Arad, Guy Kortsarz, Joseph Naor, Baruch Schieber, Hadas Shachnai","Data Structures and Algorithms (cs.DS)","Motivated by applications in production planning and storage allocation in hierarchical databases, we initiate the study of covering partially ordered items (CPO). Given a capacity $k \in \mathbb{Z}^+$, and a directed graph $G=(V,E)$ where each vertex has a size in $\{0,1, \ldots,k\}$, we seek a collection of subsets of vertices $S_1, \ldots, S_m$ that cover all the vertices, such that for any $1 \leq j \leq m$, the total size of vertices in $S_j$ is bounded by $k$, and there are no edges from $V \setminus S_j$ to $S_j$. The objective is to minimize the number of subsets $m$. CPO is closely related to the rule caching problem (RCP) that is of wide interest in the networking area. The input for RCP is a directed graph $G=(V,E)$, a profit function $p:V \rightarrow \mathbb{Z}_{0}^+$, and $k \in \mathbb{Z}^+$. The output is a subset $S \subseteq V$ of maximum profit such that $|S| \leq k$ and there are no edges from $V \setminus S$ to $S$. Our main result is a $2$-approximation algorithm for CPO on out-trees, complemented by an asymptotic $1.5$-hardness of approximation result. We also give a two-way reduction between RCP and the densest $k$-subhypergraph problem, surprisingly showing that the problems are equivalent w.r.t. polynomial-time approximation within any factor $\rho \geq 1$. This implies that RCP cannot be approximated within factor $|V|^{1-\eps}$ for any fixed $\eps>0$, under standard complexity assumptions. Prior to this work, RCP was just known to be strongly NP-hard. We further show that there is no EPTAS for the special case of RCP where the profits are uniform, assuming Gap-ETH. Since this variant admits a PTAS, we essentially resolve the complexity status of this problem.","Sun, 3 Mar 2024 17:17:08 UTC (227 KB)"
"88","Local weak convergence and its applications","Sayan Banerjee, Shankar Bhamidi, Jianan Shen, Seth Parker Young","Probability (math.PR)","Motivated in part by understanding average case analysis of fundamental algorithms in computer science, and in part by the wide array of network data available over the last decade, a variety of random graph models, with corresponding processes on these objects, have been proposed over the last few years. The main goal of this paper is to give an overview of local weak convergence, which has emerged as a major technique for understanding large network asymptotics for a wide array of functionals and models. As opposed to a survey, the main goal is to try to explain some of the major concepts and their use to junior researchers in the field and indicate potential resources for further reading.","Sun, 3 Mar 2024 15:43:19 UTC (9,601 KB)"
"89","Piggybacking astronomical hazard investigations on scientific Big Data missions","Gijs A. Verdoes Kleijn, Teymoor Saifollahi, Rees Williams, Oscar Stolk, Georg Feulner","Instrumentation and Methods for Astrophysics (astro-ph.IM)","Current and upcoming large optical and near-infrared astronomical surveys have fundamental science as their primary drivers. To cater to those, these missions scan large fractions of the entire sky at multiple wavelengths and epochs. These aspects make these data sets also valuable for investigations into astronomical hazards for life on Earth. The Netherlands Research School for Astronomy (NOVA) is a partner in several optical / near-infrared surveys. In this paper we focus on the astronomical hazard value for two sets of those: the surveys with the OmegaCAM wide-field imager at the VST and with the Euclid Mission. For each of them we provide a brief overview of the astronomical survey hardware, the data and the information systems. We present first results related to the astronomical hazard investigations. We evaluate to what extent the existing functionality of the information systems covers the needs for the astronomical hazard investigations","Sun, 3 Mar 2024 15:38:17 UTC (5,236 KB)"
"90","An Authentic Algorithm for Ciphering and Deciphering Called Latin Djokovic","Diogen Babuc","Computational Engineering, Finance, and Science (cs.CE)","The question that is a motivation of writing is how many devote themselves to discovering something in the world of science where much is discerned and revealed, but at the same time, much is unknown. The insightful elements of this algorithm are the ciphering and deciphering algorithms of Playfair, Caesar, and Vigenere. Only a few of their main properties are taken and modified, with the aim of forming a specific functionality of the algorithm called Latin Djokovic. Specifically, a string is entered as input data. A key k is given, with a random value between the values a and b = a + 3. The obtained value is stored in a variable with the aim of being constant during the run of the algorithm. In correlation to the given key, the string is divided into several groups of substrings, and each substring has a length of k characters. The next step involves encoding each substring from the list of existing substrings. Encoding is performed using the basis of the Caesar algorithm, i.e., shifting with k characters. However, that k is incremented by 1 when moving to the next substring in that list. When the value of k becomes greater than b + 1, it will return to its initial value. The algorithm is executed, following the same procedure, until the last substring in the list is traversed. Using this polyalphabetic method, ciphering and deciphering of strings are achieved. The algorithm also works for a 100-character string. The x character is not used when the number of characters in a substring is incompatible with the expected length. The algorithm is simple to implement, but it is questionable if it works better than the other methods from the point of view of execution time and storage space.","Sun, 3 Mar 2024 09:59:21 UTC (601 KB)"
"91","Fusion of Gaussian Processes Predictions with Monte Carlo Sampling","Marzieh Ajirak, Daniel Waxman, Fernando Llorente, Petar M. Djuric","Machine Learning (cs.LG)","In science and engineering, we often work with models designed for accurate prediction of variables of interest. Recognizing that these models are approximations of reality, it becomes desirable to apply multiple models to the same data and integrate their outcomes. In this paper, we operate within the Bayesian paradigm, relying on Gaussian processes as our models. These models generate predictive probability density functions (pdfs), and the objective is to integrate them systematically, employing both linear and log-linear pooling. We introduce novel approaches for log-linear pooling, determining input-dependent weights for the predictive pdfs of the Gaussian processes. The aggregation of the pdfs is realized through Monte Carlo sampling, drawing samples of weights from their posterior. The performance of these methods, as well as those based on linear pooling, is demonstrated using a synthetic dataset.","Sun, 3 Mar 2024 04:21:21 UTC (770 KB)"
"92","Euclidean distance compression via deep random features","Brett Leroux, Luis Rademacher","Computational Geometry (cs.CG)","Motivated by the problem of compressing point sets into as few bits as possible while maintaining information about approximate distances between points, we construct random nonlinear maps $\varphi_\ell$ that compress point sets in the following way. For a point set $S$, the map $\varphi_\ell:\mathbb{R}^d \to N^{-1/2}\{-1,1\}^N$ has the property that storing $\varphi_\ell(S)$ (a \emph{sketch} of $S$) allows one to report pairwise squared distances between points in $S$ up to some multiplicative $(1\pm \epsilon)$ error with high probability as long as the minimum distance is not too small compared to $\epsilon$. The maps $\varphi_\ell$ are the $\ell$-fold composition of a certain type of random feature mapping. Moreover, we determine how large $N$ needs to be as a function of $\epsilon$ and other parameters of the point set. Compared to existing techniques, our maps offer several advantages. The standard method for compressing point sets by random mappings relies on the Johnson-Lindenstrauss lemma which implies that if a set of $n$ points is mapped by a Gaussian random matrix to $\mathbb{R}^k$ with $k =\Theta(\epsilon^{-2}\log n)$, then pairwise distances between points are preserved up to a multiplicative $(1\pm \epsilon)$ error with high probability. The main advantage of our maps $\varphi_\ell$ over random linear maps is that ours map point sets directly into the discrete cube $N^{-1/2}\{-1,1\}^N$ and so there is no additional step needed to convert the sketch to bits. For some range of parameters, our maps $\varphi_\ell$ produce sketches which require fewer bits of storage space.","Sat, 2 Mar 2024 22:24:31 UTC (21 KB)"
"93","The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models","Stephanie Eckman, Barbara Plank, Frauke Kreuter","Human-Computer Interaction (cs.HC)","Whether future AI models make the world safer or less safe for humans rests in part on our ability to efficiently collect accurate data from people about what they want the models to do. However, collecting high quality data is difficult, and most AI/ML researchers are not trained in data collection methods. The growing emphasis on data-centric AI highlights the potential of data to enhance model performance. It also reveals an opportunity to gain insights from survey methodology, the science of collecting high-quality survey data. In this position paper, we summarize lessons from the survey methodology literature and discuss how they can improve the quality of training and feedback data, which in turn improve model performance. Based on the cognitive response process model, we formulate specific hypotheses about the aspects of label collection that may impact training data quality. We also suggest collaborative research ideas into how possible biases in data collection can be mitigated, making models more accurate and human-centric.","Sat, 2 Mar 2024 13:34:43 UTC (795 KB)"
"94","A Strongly Subcubic Combinatorial Algorithm for Triangle Detection with Applications","Adrian Dumitrescu","Data Structures and Algorithms (cs.DS)","We revisit the algorithmic problem of finding a triangle in a graph: We give a randomized combinatorial algorithm for triangle detection in a given $n$-vertex graph with $m$ edges running in $O(n^{7/3})$ time, or alternatively in $O(m^{4/3})$ time. This may come as a surprise since it invalidates several conjectures in the literature. In particular, - the $O(n^{7/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection based on matrix multiplication running in $O(n^\omega) = O(n^{2.372})$ time, due to Itai and Rodeh (1978). - the $O(m^{4/3})$ runtime surpasses the long-standing fastest algorithm for triangle detection in sparse graphs based on matrix multiplication running in $O(m^{2\omega/(\omega+1)})= O(m^{1.407})$ time due to Alon, Yuster, and Zwick (1997). - the $O(n^{7/3})$ time algorithm for triangle detection leads to a $O(n^{25/9} \log{n})$ time combinatorial algorithm for $n \times n$ Boolean matrix multiplication, by a reduction of V. V. Williams and R.~R.~Williams (2018).This invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014). - the $O(m^{4/3})$ runtime invalidates a conjecture of A.~Abboud and V. V. Williams (FOCS 2014) that any combinatorial algorithm for triangle detection requires $m^{3/2 - o(1)}$ time. - as a direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the $k$-clique problem, surpassing an almost $40$ years old algorithm of Ne{š}et{ř}il and Poljak (1985). This result strongly disproves the combinatorial $k$-clique conjecture. - as another direct application of the triangle detection algorithm, we obtain a faster exact algorithm for the \textsc{Max-Cut} problem, surpassing an almost $20$ years old algorithm of R.~R.~Williams (2005).","Sat, 2 Mar 2024 04:01:26 UTC (23 KB)[v2] Tue, 5 Mar 2024 15:14:48 UTC (1 KB) (withdrawn)"
"95","Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice","Karan Shah, Julie Butler, Alexis Knaub, Anıl Zenginoğlu, William Ratcliff, Mohammad Soltanieh-ha","Physics Education (physics.ed-ph)","With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and inspiration to educators who seek to integrate data science into their teaching, helping to prepare the next generation of physicists for a data-driven world.","Fri, 1 Mar 2024 20:21:42 UTC (4,431 KB)"
"96","ClassInSight: Designing Conversation Support Tools to Visualize Classroom Discussion for Personalized Teacher Professional Development","Tricia J. Ngoon, S Sushil, Angela Stewart, Ung-Sang Lee, Saranya Venkatraman, Neil Thawani, Prasenjit Mitra, Sherice Clarke, John Zimmerman, Amy Ogan","Human-Computer Interaction (cs.HC)","Teaching is one of many professions for which personalized feedback and reflection can help improve dialogue and discussion between the professional and those they serve. However, professional development (PD) is often impersonal as human observation is labor-intensive. Data-driven PD tools in teaching are of growing interest, but open questions about how professionals engage with their data in practice remain. In this paper, we present ClassInSight, a tool that visualizes three levels of teachers' discussion data and structures reflection. Through 22 reflection sessions and interviews with 5 high school science teachers, we found themes related to dissonance, contextualization, and sustainability in how teachers engaged with their data in the tool and in how their professional vision, the use of professional expertise to interpret events, shifted over time. We discuss guidelines for these conversational support tools to support personalized PD in professions beyond teaching where conversation and interaction are important.","Fri, 1 Mar 2024 20:09:44 UTC (1,147 KB)"
"97","Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights","Yuan Wang, Lokesh Kumar Sambasivan, Mingang Fu, Prakhar Mehrotra","Artificial Intelligence (cs.AI)","Generative AI applications, such as ChatGPT or DALL-E, have shown the world their impressive capabilities in generating human-like text or image. Diving deeper, the science stakeholder for those AI applications are Deep Generative Models, a.k.a DGMs, which are designed to learn the underlying distribution of the data and generate new data points that are statistically similar to the original dataset. One critical question is raised: how can we leverage DGMs into morden retail supply chain realm? To address this question, this paper expects to provide a comprehensive review of DGMs and discuss their existing and potential usecases in retail supply chain, by (1) providing a taxonomy and overview of state-of-the-art DGMs and their variants, (2) reviewing existing DGM applications in retail supply chain from a end-to-end view of point, and (3) discussing insights and potential directions on how DGMs can be further utilized on solving retail supply chain problems.","Thu, 29 Feb 2024 21:03:46 UTC (58 KB)"
"98","The ArgusSpec Prototype: Autonomous Spectroscopic Follow-up of Flares Detected by Large Array Telescopes","Nathan W. Galliher, Thomas Procter, Nicholas M. Law, Hank Corbett, Ward S. Howard, Alan Vasquez Soto, Ramses Gonzalez, Lawrence Machia, Jonathan Carney, William J. Marshall","Instrumentation and Methods for Astrophysics (astro-ph.IM)","ArgusSpec is a prototype autonomous spectroscopic follow-up instrument designed to characterize flares detected by the Argus Pathfinder telescope array by taking short exposure (30 s) broadband spectra (370 - 750 nm) at low resolutions (R~150 at 500 nm). The instrument is built from consumer off-the-shelf astronomical equipment, assembled inside a shipping container, and deployed alongside the Argus Pathfinder at a dark sky observing site in Western North Carolina. The \$35k prototype ArgusSpec was designed, built, and deployed in under a year, largely from existing parts, and has been operating on-sky since March 2023. With current hardware and software, the system is capable of receiving an observation, slewing, performing autonomous slit acquisition, and beginning data acquisition within an average of 32 s. With Argus Pathfinder's 1-second-cadence survey reporting alerts of rising sources within 2 s of onset, ArgusSpec can reach new targets well within a minute of the start of the event. As built, ArgusSpec can observe targets down to a 20$\sigma$ limiting magnitude of $m_V$~13 at 30 s cadence with an optical resolution of R~150 (at 500 nm). With automated rapid acquisition demonstrated, later hardware upgrades will significantly improve the limiting magnitude, and potentially enable deep spectroscopy by the coaddition of data from an array of ArgusSpec systems. ArgusSpec's primary science driver is the characterization of the blackbody evolution of flares from nearby M-dwarfs. Large flares emitted by these stars could have significant impacts on the potential habitability of any orbiting exoplanets, but our current understanding of these events is in large part built on observations from a handful of active stars. ArgusSpec will characterize large numbers of flares, building a spectroscopic library of the most extreme events from a wide variety of stellar masses and ages.","Thu, 29 Feb 2024 19:00:00 UTC (15,403 KB)"
"99","Virtual Reality for Understanding Artificial-Intelligence-driven Scientific Discovery with an Application in Quantum Optics","Philipp Schmidt, Sören Arlt, Carlos Ruiz-Gonzalez, Xuemei Gu, Carla Rodríguez, Mario Krenn","Human-Computer Interaction (cs.HC)","Generative Artificial Intelligence (AI) models can propose solutions to scientific problems beyond human capability. To truly make conceptual contributions, researchers need to be capable of understanding the AI-generated structures and extracting the underlying concepts and ideas. When algorithms provide little explanatory reasoning alongside the output, scientists have to reverse-engineer the fundamental insights behind proposals based solely on examples. This task can be challenging as the output is often highly complex and thus not immediately accessible to humans. In this work we show how transferring part of the analysis process into an immersive Virtual Reality (VR) environment can assist researchers in developing an understanding of AI-generated solutions. We demonstrate the usefulness of VR in finding interpretable configurations of abstract graphs, representing Quantum Optics experiments. Thereby, we can manually discover new generalizations of AI-discoveries as well as new understanding in experimental quantum optics. Furthermore, it allows us to customize the search space in an informed way - as a human-in-the-loop - to achieve significantly faster subsequent discovery iterations. As concrete examples, with this technology, we discover a new resource-efficient 3-dimensional entanglement swapping scheme, as well as a 3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our results show the potential of VR for increasing a human researcher's ability to derive knowledge from graph-based generative AI that, which is a common abstract data representation used in diverse fields of science.","Tue, 20 Feb 2024 17:48:01 UTC (12,146 KB)"
"100","Cognitive Bias in High-Stakes Decision-Making with LLMs","Jessica Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, Zexue He","Artificial Intelligence (cs.AI)","Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method using LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type.","Sun, 25 Feb 2024 02:35:56 UTC (7,208 KB)"
"101","A framework for understanding data science","Michael L Brodie","Other Statistics (stat.OT)","The objective of this research is to provide a framework with which the data science community can understand, define, and develop data science as a field of inquiry. The framework is based on the classical reference framework (axiology, ontology, epistemology, methodology) used for 200 years to define knowledge discovery paradigms and disciplines in the humanities, sciences, algorithms, and now data science. I augmented it for automated problem-solving with (methods, technology, community). The resulting data science reference framework is used to define the data science knowledge discovery paradigm in terms of the philosophy of data science addressed in previous papers and the data science problem-solving paradigm, i.e., the data science method, and the data science problem-solving workflow, both addressed in this paper. The framework is a much called for unifying framework for data science as it contains the components required to define data science. For insights to better understand data science, this paper uses the framework to define the emerging, often enigmatic, data science problem-solving paradigm and workflow, and to compare them with their well-understood scientific counterparts, scientific problem-solving paradigm and workflow.","Wed, 14 Feb 2024 15:55:40 UTC (4,403 KB)"
"102","An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots","Tobias Haubold, Petra Linke","Robotics (cs.RO)","As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines. In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of virtual worlds and data scientists even work on the same files. We want to contribute to that topic by describing an approach where data scientists don't require knowledge about the simulation software. Our approach uses the standalone simulation software Webots, the Robot Operating System to communicate with simulated robots as well as the simulation software itself and container technology to separate the simulation from the model development environment. We put emphasize on the APIs the data scientists work with and the use of a standalone simulation software in unattended training pipelines. We show the parts that are specific to the Robotino and the robot task to learn.","Tue, 6 Feb 2024 12:08:01 UTC (188 KB)"
"103","The Probability to Hit Every Bin with a Linear Number of Balls","Stefan Walzer","Probability (math.PR)","Assume that $2n$ balls are thrown independently and uniformly at random into $n$ bins. We consider the unlikely event $E$ that every bin receives at least one ball, showing that $\Pr[E] = \Theta(b^n)$ where $b \approx 0.836$. Note that, due to correlations, $b$ is not simply the probability that any single bin receives at least one ball. More generally, we consider the event that throwing $\alpha n$ balls into $n$ bins results in at least $d$ balls in each bin.","Fri, 1 Mar 2024 18:32:34 UTC (59 KB)"
"104","Scalable Learning of Item Response Theory Models","Susanne Frick, Amer Krivošija, Alexander Munteanu","Machine Learning (cs.LG)","Item Response Theory (IRT) models aim to assess latent abilities of $n$ examinees along with latent difficulty characteristics of $m$ test items from categorical data that indicates the quality of their corresponding answers. Classical psychometric assessments are based on a relatively small number of examinees and items, say a class of $200$ students solving an exam comprising $10$ problems. More recent global large scale assessments such as PISA, or internet studies, may lead to significantly increased numbers of participants. Additionally, in the context of Machine Learning where algorithms take the role of examinees and data analysis problems take the role of items, both $n$ and $m$ may become very large, challenging the efficiency and scalability of computations. To learn the latent variables in IRT models from large data, we leverage the similarity of these models to logistic regression, which can be approximated accurately using small weighted subsets called coresets. We develop coresets for their use in alternating IRT training algorithms, facilitating scalable learning from large data.","Fri, 1 Mar 2024 17:12:53 UTC (393 KB)"
"105","Optimal Control of Underdamped Systems: An Analytic Approach","Julia Sanders, Marco Baldovin, Paolo Muratore-Ginanneschi","Statistical Mechanics (cond-mat.stat-mech)","Optimal control theory deals with finding protocols to steer a system between assigned initial and final states, such that a trajectory-dependent cost function is minimized. The application of optimal control to stochastic systems is an open and challenging research frontier, with a spectrum of applications ranging from stochastic thermodynamics, to biophysics and data science. Among these, the design of nanoscale electronic components motivates the study of underdamped dynamics, leading to practical and conceptual difficulties. In this work, we develop analytic techniques to determine protocols steering finite time transitions at minimum dissipation for stochastic underdamped dynamics. For transitions between Gaussian states, we prove that optimal protocols satisfy a Lyapunov equation, a central tool in stability analysis of dynamical systems. For transitions between states described by general Maxwell-Boltzmann distributions, we introduce an infinite-dimensional version of the Poincaré-Linstedt multiscale perturbation theory around the overdamped limit. This technique fundamentally improves the standard multiscale expansion. Indeed, it enables the explicit computation of momentum cumulants, whose variation in time is a distinctive trait of underdamped dynamics and is directly accessible to experimental observation. Our results allow us to numerically study cost asymmetries in expansion and compression processes and make predictions for inertial corrections to optimal protocols in the Landauer erasure problem at the nanoscale.","Fri, 1 Mar 2024 17:10:51 UTC (2,279 KB)"
"106","Undercomplete Decomposition of Symmetric Tensors in Linear Time, and Smoothed Analysis of the Condition Number","Pascal Koiran, Subhayan Saha","Data Structures and Algorithms (cs.DS)","We study symmetric tensor decompositions, i.e., decompositions of the form $T = \sum_{i=1}^r u_i^{\otimes 3}$ where $T$ is a symmetric tensor of order 3 and $u_i \in \mathbb{C}^n$.In order to obtain efficient decomposition algorithms, it is necessary to require additional properties from $u_i$. In this paper we assume that the $u_i$ are linearly independent. This implies $r \leq n$,that is, the decomposition of T is undercomplete. We give a randomized algorithm for the following problem in the exact arithmetic model of computation: Let $T$ be an order-3 symmetric tensor that has an undercomplete decomposition.Then given some $T'$ close to $T$, an accuracy parameter $\varepsilon$, and an upper bound B on the condition number of the tensor, output vectors $u'_i$ such that $||u_i - u'_i|| \leq \varepsilon$ (up to permutation and multiplication by cube roots of unity) with high probability. The main novel features of our algorithm are: 1) We provide the first algorithm for this problem that runs in linear time in the size of the input tensor. More specifically, it requires $O(n^3)$ arithmetic operations for all accuracy parameters $\varepsilon =$ 1/poly(n) and B = poly(n). 2) Our algorithm is robust, that is, it can handle inverse-quasi-polynomial noise (in $n$,B,$\frac{1}{\varepsilon}$) in the input tensor. 3) We present a smoothed analysis of the condition number of the tensor decomposition problem. This guarantees that the condition number is low with high probability and further shows that our algorithm runs in linear time, except for some rare badly conditioned inputs. Our main algorithm is a reduction to the complete case ($r=n$) treated in our previous work [Koiran,Saha,CIAC 2023]. For efficiency reasons we cannot use this algorithm as a blackbox. Instead, we show that it can be run on an implicitly represented tensor obtained from the input tensor by a change of basis.","Fri, 1 Mar 2024 16:22:39 UTC (39 KB)"
"107","Nearest-Neighbours Estimators for Conditional Mutual Information","Jake Witter, Conor Houghton","Information Theory (cs.IT)","The conditional mutual information quantifies the conditional dependence of two random variables. It has numerous applications; it forms, for example, part of the definition of transfer entropy, a common measure of the causal relationship between time series. It does, however, require a lot of data to estimate accurately and suffers the curse of dimensionality, limiting its application in machine learning and data science. However, the Kozachenko-Leonenko approach can address this problem: it is possible, in this approach to define a nearest-neighbour estimator which depends only on the distance between data points and not on the dimension of the data. Furthermore, the bias can be calculated analytically for this estimator. Here this estimator is described and is tested on simulated data.","Fri, 1 Mar 2024 14:28:00 UTC (319 KB)"
"108","Approximating the Geometric Knapsack Problem in Near-Linear Time and Dynamically","Moritz Buchem, Paul Deuker, Andreas Wiese","Data Structures and Algorithms (cs.DS)","An important goal in algorithm design is determining the best running time for solving a problem (approximately). For some problems, we know the optimal running time, assuming certain conditional lower bounds. In this work, we study the $d$-dimensional geometric knapsack problem where we are far from this level of understanding. We are given a set of weighted d-dimensional geometric items like squares, rectangles, or hypercubes and a knapsack which is a square or a (hyper-)cube. We want to select a subset of items that fit non-overlappingly inside the knapsack, maximizing the total profit of the packed items. We make a significant step towards determining the best running time for solving these problems approximately by presenting approximation algorithms with near-linear running times for any constant dimension d and any constant parameter $\epsilon$. For (hyper)-cubes, we present a $(1+\epsilon)$-approximation algorithm whose running time drastically improves upon the known $(1+\epsilon)$-approximation algorithm which has a running time where the exponent of n depends exponentially on $1/\epsilon$ and $d$. Moreover, we present a $(2+\epsilon)$-approximation algorithm for rectangles in the setting without rotations and a $(17/9+\epsilon)$-approximation algorithm if we allow rotations by 90 degrees. The best known polynomial time algorithms for these settings have approximation ratios of $17/9+\epsilon$ and $1.5+\epsilon$, respectively, and running times in which the exponent of n depends exponentially on $1/\epsilon$. We also give dynamic algorithms with polylogarithmic query and update times and the same approximation guarantees as the algorithms above. Key to our results is a new family of structured packings which we call easily guessable packings. They are flexible enough to guarantee profitable solutions and structured enough so that we can compute these solutions quickly.","Fri, 1 Mar 2024 13:49:27 UTC (319 KB)"
"109","Polyamorous Scheduling","Leszek Gąsieniec, Benjamin Smith, Sebastian Wild","Data Structures and Algorithms (cs.DS)","Finding schedules for pairwise meetings between the members of a complex social group without creating interpersonal conflict is challenging, especially when different relationships have different needs. We formally define and study the underlying optimisation problem: Polyamorous Scheduling. In Polyamorous Scheduling, we are given an edge-weighted graph and try to find a periodic schedule of matchings in this graph such that the maximal weighted waiting time between consecutive occurrences of the same edge is minimised. We show that the problem is NP-hard and that there is no efficient approximation algorithm with a better ratio than 13/12 unless P = NP. On the positive side, we obtain an $O(\log n)$-approximation algorithm. We also define a generalisation of density from the Pinwheel Scheduling Problem, ""poly density"", and ask whether there exists a poly density threshold similar to the 5/6-density threshold for Pinwheel Scheduling [Kawamura, STOC 2024]. Polyamorous Scheduling is a natural generalisation of Pinwheel Scheduling with respect to its optimisation variant, Bamboo Garden Trimming. Our work contributes the first nontrivial hardness-of-approximation reduction for any periodic scheduling problem, and opens up numerous avenues for further study of Polyamorous Scheduling.","Fri, 1 Mar 2024 11:39:46 UTC (82 KB)"
"110","Rethinking Tokenization: Crafting Better Tokenizers for Large Language Models","Jinbiao Yang","Computation and Language (cs.CL)","Tokenization significantly influences language models(LMs)' performance. This paper traces the evolution of tokenizers from word-level to subword-level, analyzing how they balance tokens and types to enhance model adaptability while controlling complexity. Despite subword tokenizers like Byte Pair Encoding (BPE) overcoming many word tokenizer limitations, they encounter difficulties in handling non-Latin languages and depend heavily on extensive training data and computational resources to grasp the nuances of multiword expressions (MWEs). This article argues that tokenizers, more than mere technical tools, should drawing inspiration from the cognitive science about human language processing. This study then introduces the ""Principle of Least Effort"" from cognitive science, that humans naturally seek to reduce cognitive effort, and discusses the benefits of this principle for tokenizer development. Based on this principle, the paper proposes that the Less-is-Better (LiB) model could be a new approach for LLM tokenizer. The LiB model can autonomously learn an integrated vocabulary consisting of subwords, words, and MWEs, which effectively reduces both the numbers of tokens and types. Comparative evaluations show that the LiB tokenizer outperforms existing word and BPE tokenizers, presenting an innovative method for tokenizer development, and hinting at the possibility of future cognitive science-based tokenizers being more efficient.","Fri, 1 Mar 2024 10:03:07 UTC (610 KB)"
"111","qPMS Sigma -- An Efficient and Exact Parallel Algorithm for the Planted $(l, d)$ Motif Search Problem","Saurav Dhar, Amlan Saha, Dhiman Goswami, Md. Abul Kashem Mia","Data Structures and Algorithms (cs.DS)","Motif finding is an important step for the detection of rare events occurring in a set of DNA or protein sequences. Extraction of information about these rare events can lead to new biological discoveries. Motifs are some important patterns that have numerous applications including the identification of transcription factors and their binding sites, composite regulatory patterns, similarity between families of proteins, etc. Although several flavors of motif searching algorithms have been studied in the literature, we study the version known as $ (l, d) $-motif search or Planted Motif Search (PMS). In PMS, given two integers $ l $, $ d $ and $ n $ input sequences we try to find all the patterns of length $ l $ that appear in each of the $ n $ input sequences with at most $ d $ mismatches. We also discuss the quorum version of PMS in our work that finds motifs that are not planted in all the input sequences but at least in $ q $ of the sequences. Our algorithm is mainly based on the algorithms qPMSPrune, qPMS7, TraverStringRef and PMS8. We introduce some techniques to compress the input strings and make faster comparison between strings with bitwise operations. Our algorithm performs a little better than the existing exact algorithms to solve the qPMS problem in DNA sequence. We have also proposed an idea for parallel implementation of our algorithm.","Fri, 1 Mar 2024 06:18:39 UTC (2,286 KB)"
"112","Algorithms for Efficient, Compact Online Data Stream Curation","Matthew Andres Moreno, Santiago Rodriguez Papa, Emily Dolson","Data Structures and Algorithms (cs.DS)","Data stream algorithms tackle operations on high-volume sequences of read-once data items. Data stream scenarios include inherently real-time systems like sensor networks and financial markets. They also arise in purely-computational scenarios like ordered traversal of big data or long-running iterative simulations. In this work, we develop methods to maintain running archives of stream data that are temporally representative, a task we call ""stream curation."" Our approach contributes to rich existing literature on data stream binning, which we extend by providing stateless (i.e., non-iterative) curation schemes that enable key optimizations to trim archive storage overhead and streamline processing of incoming observations. We also broaden support to cover new trade-offs between curated archive size and temporal coverage. We present a suite of five stream curation algorithms that span $\mathcal{O}(n)$, $\mathcal{O}(\log n)$, and $\mathcal{O}(1)$ orders of growth for retained data items. Within each order of growth, algorithms are provided to maintain even coverage across history or bias coverage toward more recent time points. More broadly, memory-efficient stream curation can boost the data stream mining capabilities of low-grade hardware in roles such as sensor nodes and data logging devices.","Fri, 1 Mar 2024 04:01:34 UTC (39,072 KB)"
"113","Extracting Polymer Nanocomposite Samples from Full-Length Documents","Ghazal Khalighinejad, Defne Circi, L.C. Brinson, Bhuwan Dhingra","Computation and Language (cs.CL)","This paper investigates the use of large language models (LLMs) for extracting sample lists of polymer nanocomposites (PNCs) from full-length materials science research papers. The challenge lies in the complex nature of PNC samples, which have numerous attributes scattered throughout the text. The complexity of annotating detailed information on PNCs limits the availability of data, making conventional document-level relation extraction techniques impractical due to the challenge in creating comprehensive named entity span annotations. To address this, we introduce a new benchmark and an evaluation technique for this task and explore different prompting strategies in a zero-shot manner. We also incorporate self-consistency to improve the performance. Our findings show that even advanced LLMs struggle to extract all of the samples from an article. Finally, we analyze the errors encountered in this process, categorizing them into three main challenges, and discuss potential strategies for future research to overcome them.","Fri, 1 Mar 2024 03:51:56 UTC (578 KB)"
"114","Analysis of Phylogeny Tracking Algorithms for Serial and Multiprocess Applications","Matthew Andres Moreno, Santiago Rodriguez Papa, Emily Dolson","Data Structures and Algorithms (cs.DS)","Since the advent of modern bioinformatics, the challenging, multifaceted problem of reconstructing phylogenetic history from biological sequences has hatched perennial statistical and algorithmic innovation. Studies of the phylogenetic dynamics of digital, agent-based evolutionary models motivate a peculiar converse question: how to best engineer tracking to facilitate fast, accurate, and memory-efficient lineage reconstructions? Here, we formally describe procedures for phylogenetic analysis in both serial and distributed computing scenarios. With respect to the former, we demonstrate reference-counting-based pruning of extinct lineages. For the latter, we introduce a trie-based phylogenetic reconstruction approach for ""hereditary stratigraphy"" genome annotations. This process allows phylogenetic relationships between genomes to be inferred by comparing their similarities, akin to reconstruction of natural history from biological DNA sequences. Phylogenetic analysis capabilities significantly advance distributed agent-based simulations as a tool for evolutionary research, and also benefit application-oriented evolutionary computing. Such tracing could extend also to other digital artifacts that proliferate through replication, like digital media and computer viruses.","Fri, 1 Mar 2024 03:21:00 UTC (117 KB)[v2] Mon, 4 Mar 2024 05:30:17 UTC (117 KB)"
"115","A citizen science toolkit to collect human perceptions of urban environments using open street view images","Matthew Danish, SM Labib, Britta Ricker, Marco Helbich","Computer Vision and Pattern Recognition (cs.CV)","Street View-level Imagery (SVI) is a valuable data source for studies (e.g., environmental assessments, green space identification or land cover classification). While commercial SVI is available, such providers commonly restrict copying or reuse in ways necessary for research. Open SVI datasets are readily available from less restrictive sources, such as Mapillary, but due to the heterogeneity of the images, these require substantial preprocessing, filtering, and careful quality checks. We present an efficient method for automated downloading, processing, cropping, and filtering open SVI, to be used in a survey of human perceptions of the streets portrayed in these images. We demonstrate our open-source reusable SVI preparation and smartphone-friendly perception-survey software with Amsterdam (Netherlands) as the case study. Using a citizen science approach, we collected from 331 people 22,637 ratings about their perceptions for various criteria. We have published our software in a public repository for future re-use and reproducibility.","Thu, 29 Feb 2024 22:58:13 UTC (4,398 KB)"
"116","Average-Case Local Computation Algorithms","Amartya Shankha Biswas, Ruidi Cao, Edward Pyne, Ronitt Rubinfeld","Data Structures and Algorithms (cs.DS)","We initiate the study of Local Computation Algorithms on average case inputs. In the Local Computation Algorithm (LCA) model, we are given probe access to a huge graph, and asked to answer membership queries about some combinatorial structure on the graph, answering each query with sublinear work. For instance, an LCA for the $k$-spanner problem gives access to a sparse subgraph $H\subseteq G$ that preserves distances up to a factor of $k$. We build simple LCAs for this problem assuming the input graph is drawn from the well-studied Erdos-Reyni and Preferential Attachment graph models. In both cases, our spanners achieve size and stretch tradeoffs that are impossible to achieve for general graphs, while having dramatically lower query complexity than worst-case LCAs. Our second result investigates the intersection of LCAs with Local Access Generators (LAGs). Local Access Generators provide efficient query access to a random object, for instance an Erdos Reyni random graph. We explore the natural problem of generating a random graph together with a combinatorial structure on it. We show that this combination can be easier to solve than focusing on each problem by itself, by building a fast, simple algorithm that provides access to an Erdos Reyni random graph together with a maximal independent set.","Thu, 29 Feb 2024 21:14:01 UTC (38 KB)"
"117","Quantum Readiness in Healthcare and Public Health: Building a Quantum Literate Workforce","Jonathan B VanGeest, Kieran J Fogarty, William G Hervey, Robert A Hanson, Suresh Nair, Timothy A Akers","Physics and Society (physics.soc-ph)","Quantum technologies, including quantum computing, cryptography, and sensing, among others, are set to revolutionize sectors ranging from materials science to drug discovery. Despite their significant potential, the implications for public health have been largely overlooked, highlighting a critical gap in recognition and preparation. This oversight necessitates immediate action, as public health remains largely unaware of quantum technologies as a tool for advancement. The application of quantum principles to epidemiology and health informatics, termed quantum health epidemiology and quantum health informatics, has the potential to radically transform disease surveillance, prediction, modeling, and analysis of health data. However, there is a notable lack of quantum expertise within the public health workforce and educational pipelines. This gap underscores the urgent need for the development of quantum literacy among public health practitioners, leaders, and students to leverage emerging opportunities while addressing risks and ethical considerations. Innovative teaching methods, such as interactive simulations, games, visual models, and other tailored platforms, offer viable solutions for bridging knowledge gaps without the need for advanced physics or mathematics. However, the opportunity to adapt is fleeting as the quantum era in healthcare looms near. It is imperative that public health urgently focuses on updating its educational approaches, workforce strategies, data governance, and organizational culture to proactively meet the challenges of quantum disruption thereby becoming quantum ready.","Thu, 29 Feb 2024 20:55:56 UTC (625 KB)"
"118","Future of Pandemic Prevention and Response CCC Workshop Report","David Danks, Rada Mihalcea, Katie Siek, Mona Singh, Brian Dixon, Haley Griffin","Computers and Society (cs.CY)","This report summarizes the discussions and conclusions of a 2-day multidisciplinary workshop that brought together researchers and practitioners in healthcare, computer science, and social sciences to explore what lessons were learned and what actions, primarily in research, could be taken. One consistent observation was that there is significant merit in thinking not only about pandemic situations, but also about peacetime advances, as many healthcare networks and communities are now in a perpetual state of crisis. Attendees discussed how the COVID-19 pandemic amplified gaps in our health and computing systems, and how current and future computing technologies could fill these gaps and improve the trajectory of the next pandemic. Three major computing themes emerged from the workshop: models, data, and infrastructure. Computational models are extremely important during pandemics, from anticipating supply needs of hospitals, to determining the care capacity of hospital and social service providers, to projecting the spread of the disease. Accurate, reliable models can save lives, and inform community leaders on policy decisions. Health system users require accurate, reliable data to achieve success when applying models. This requires data and measurement standardization across health care organizations, modernizing the data infrastructure, and methods for ensuring data remains private while shared for model development, validation, and application. Finally, many health care systems lack the data, compute, and communication infrastructures required to build models on their data, use those models in ordinary operations, or even to reliably access their data. Robust and timely computing research has the potential to better support healthcare works to save lives in times of crisis (e.g., pandemics) and today during relative peacetime.","Thu, 29 Feb 2024 19:54:22 UTC (1,831 KB)"
"119","Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing","Pranav Shetty, Aishat Adeboye, Sonakshi Gupta, Chao Zhang, Rampi Ramprasad","Materials Science (cond-mat.mtrl-sci)","We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain data-driven insights. Our insights include active learning strategies that can simultaneously optimize the material system and train strong predictive models of material properties. This work provides a valuable framework for research in material science.","Thu, 29 Feb 2024 18:54:46 UTC (5,391 KB)"
"120","Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm","Leo Zhou, Joao Basso, Song Mei","Quantum Physics (quant-ph)","The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization. In this paper, we analyze the performance of the QAOA on a statistical estimation problem, namely, the spiked tensor model, which exhibits a statistical-computational gap classically. We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold $\Theta(n^{(q-2)/4})$ for spiked $q$-tensors. Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, finding an intriguing sine-Gaussian law verified through simulations. For some $p$ and $q$, the QAOA attains an overlap that is larger by a constant factor than the tensor power iteration overlap. Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure.","Thu, 29 Feb 2024 18:50:28 UTC (1,149 KB)"
"121","Higher-Order Networks Representation and Learning: A Survey","Hao Tian, Reza Zafarani","Social and Information Networks (cs.SI)","Network data has become widespread, larger, and more complex over the years. Traditional network data is dyadic, capturing the relations among pairs of entities. With the need to model interactions among more than two entities, significant research has focused on higher-order networks and ways to represent, analyze, and learn from them. There are two main directions to studying higher-order networks. One direction has focused on capturing higher-order patterns in traditional (dyadic) graphs by changing the basic unit of study from nodes to small frequently observed subgraphs, called motifs. As most existing network data comes in the form of pairwise dyadic relationships, studying higher-order structures within such graphs may uncover new insights. The second direction aims to directly model higher-order interactions using new and more complex representations such as simplicial complexes or hypergraphs. Some of these models have long been proposed, but improvements in computational power and the advent of new computational techniques have increased their popularity. Our goal in this paper is to provide a succinct yet comprehensive summary of the advanced higher-order network analysis techniques. We provide a systematic review of its foundations and algorithms, along with use cases and applications of higher-order networks in various scientific domains.","Thu, 29 Feb 2024 18:11:25 UTC (99 KB)"
"122","Total Completion Time Scheduling Under Scenarios","Thomas Bosman, Martijn van Ee, Ekin Ergen, Csanad Imreh, Alberto Marchetti-Spaccamela, Martin Skutella, Leen Stougie","Data Structures and Algorithms (cs.DS)","Scheduling jobs with given processing times on identical parallel machines so as to minimize their total completion time is one of the most basic scheduling problems. We study interesting generalizations of this classical problem involving scenarios. In our model, a scenario is defined as a subset of a predefined and fully specified set of jobs. The aim is to find an assignment of the whole set of jobs to identical parallel machines such that the schedule, obtained for the given scenarios by simply skipping the jobs not in the scenario, optimizes a function of the total completion times over all scenarios. While the underlying scheduling problem without scenarios can be solved efficiently by a simple greedy procedure (SPT rule), scenarios, in general, make the problem NP-hard. We paint an almost complete picture of the evolving complexity landscape, drawing the line between easy and hard. One of our main algorithmic contributions relies on a deep structural result on the maximum imbalance of an optimal schedule, based on a subtle connection to Hilbert bases of a related convex cone.","Thu, 29 Feb 2024 15:31:15 UTC (312 KB)"
"123","Edit and Alphabet-Ordering Sensitivity of Lex-parse","Yuto Nakashima, Dominik Köppl, Mitsuru Funakoshi, Shunsuke Inenaga, Hideo Bannai","Data Structures and Algorithms (cs.DS)","We investigate the compression sensitivity [Akagi et al., 2023] of lex-parse [Navarro et al., 2021] for two operations: (1) single character edit and (2) modification of the alphabet ordering, and give tight upper and lower bounds for both operations. For both lower bounds, we use the family of Fibonacci words. For the bounds on edit operations, our analysis makes heavy use of properties of the Lyndon factorization of Fibonacci words to characterize the structure of lex-parse.","Thu, 29 Feb 2024 14:55:04 UTC (182 KB)"
"124","Computing Longest Common Subsequence under Cartesian-Tree Matching Model","Taketo Tsujimoto, Koki Shibata, Takuya Mieno, Yuto Nakashima, Shunsuke Inenaga","Data Structures and Algorithms (cs.DS)","Two strings of the same length are said to Cartesian-tree match (CT-match) if their Cartesian-trees are isomorphic [Park et al., TCS 2020]. Cartesian-tree matching is a natural model that allows for capturing similarities of numerical sequences. Oizumi et al. [CPM 2022] showed that subsequence pattern matching under CT-matching model can be solved in polynomial time. This current article follows and extends this line of research: We present the first polynomial-time algorithm that finds the longest common subsequence under CT-matching of two given strings $S$ and $T$ of length $n$, in $O(n^6)$ time and $O(n^4)$ space for general ordered alphabets. We then show that the problem has a faster solution in the binary case, by presenting an $O(n^2 / \log n)$-time and space algorithm.","Thu, 29 Feb 2024 13:29:53 UTC (110 KB)"
"125","Unraveling electronic correlations in warm dense quantum plasmas","Tobias Dornheim, Tilo Döppner, Panagiotis Tolias, Maximilian Böhme, Luke Fletcher, Thomas Gawne, Frank Graziani, Dominik Kraus, Michael MacDonald, Zhandos Moldabekov, Sebastian Schwalbe, Dirk Gericke, Jan Vorberger","Plasma Physics (physics.plasm-ph)","The study of matter at extreme densities and temperatures has emerged as a highly active frontier at the interface of plasma physics, material science and quantum chemistry with direct relevance for planetary modeling and inertial confinement fusion. A particular feature of such warm dense matter is the complex interplay of strong Coulomb interactions, quantum effects, and thermal excitations, rendering its rigorous theoretical description a formidable challenge. Here, we report a breakthrough in path integral Monte Carlo simulations that allows us to unravel this intricate interplay for light elements without nodal restrictions. This new capability gives us access to electronic correlations previously unattainable. As an example, we apply our method to strongly compressed beryllium to describe x-ray Thomson scattering (XRTS) data obtained at the National Ignition Facility. We find excellent agreement between simulation and experiment. Our analysis shows an unprecedented level of consistency for independent observations without the need for any empirical input parameters.","Thu, 29 Feb 2024 12:45:24 UTC (2,789 KB)"
"126","Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity","Mohammad Khalid Imam Rahmani","Cryptography and Security (cs.CR)","Various decision support systems are available that implement Data Mining and Data Warehousing techniques for diving into the sea of data for getting useful patterns of knowledge (pearls). Classification, regression, clustering, and many other algorithms are used to enhance the precision and accuracy of the decision process. So, there is scope for increasing the response time of the decision process, especially in mission-critical operations. If data are ordered with suitable and efficient sorting operation, the response time of the decision process can be minimized. Insertion sort is much more suitable for such applications due to its simple and straight logic along with its dynamic nature suitable for list implementation. But it is slower than merge sort and quick sort. The main reasons this is slow: firstly, a sequential search is used to find the actual position of the next key element into the sorted left subarray and secondly, shifting of elements is required by one position towards the right for accommodating the newly inserted element. Therefore, I propose a new algorithm by using a novel technique of binary search mechanism for finding the sorted location of the next key item into the previously sorted left subarray much quicker than the conventional insertion sort algorithm. Performance measurement in terms of the actual running time of the new algorithm has been compared with those of other conventional sorting algorithms apart from the insertion sort. The results obtained on various sample data show that the new algorithm is better in performance than the conventional insertion sort and merge sort algorithms.","Thu, 29 Feb 2024 12:38:57 UTC (343 KB)"
"127","On the Improvement of Predictive Modeling Using Bayesian Stacking and Posterior Predictive Checking","Mariana Nold, Florian Meinfelder, David Kaplan","Methodology (stat.ME)","Model uncertainty is pervasive in real world analysis situations and is an often-neglected issue in applied statistics. However, standard approaches to the research process do not address the inherent uncertainty in model building and, thus, can lead to overconfident and misleading analysis interpretations. One strategy to incorporate more flexible models is to base inferences on predictive modeling. This approach provides an alternative to existing explanatory models, as inference is focused on the posterior predictive distribution of the response variable. Predictive modeling can advance explanatory ambitions in the social sciences and in addition enrich the understanding of social phenomena under investigation. Bayesian stacking is a methodological approach rooted in Bayesian predictive modeling. In this paper, we outline the method of Bayesian stacking but add to it the approach of posterior predictive checking (PPC) as a means of assessing the predictive quality of those elements of the stacking ensemble that are important to the research question. Thus, we introduce a viable workflow for incorporating PPC into predictive modeling using Bayesian stacking without presuming the existence of a true model. We apply these tools to the PISA 2018 data to investigate potential inequalities in reading competency with respect to gender and socio-economic background. Our empirical example serves as rough guideline for practitioners who want to implement the concepts of predictive modeling and model uncertainty in their work to similar research questions.","Thu, 29 Feb 2024 11:19:48 UTC (81 KB)"
"128","Machine learning-enabled exploration of mesoscale architectures in amphiphilic-molecule self-assembly","Takeo Sudo, Satoki Ishiai, Yuuki Ishiwatari, Takahiro Yokoyama, Kenji Yasuoka, Noriyoshi Arai","Soft Condensed Matter (cond-mat.soft)","Amphiphilic molecules spontaneously form self-assembled structures of various shapes depending on their molecular structures, the temperature, and other physical conditions. The functionalities of these structures are dictated by their formations and their properties must be evaluated for reproduction using molecular simulations. However, the assessment of such intricate structures involves many procedural steps. This study investigates the potential of machine-learning models to extract structural features from mesoscale non-ordered self-assembled structures, and suggests a methodology in which machine-learning models for the structural analysis of self-assembled structures are trained on particle types and coordinate data. In the proposed approach, graph neural networks are utilised to extract local structural data for analysis. In simulations using several hundred self-assembled structures of up to 4050 coarse-grained particles, local structures are successfully extracted and classified with up to 78.35 % accuracy. As the machine-learning models learn structural characteristics without the need for human-made feature engineering, the proposed method has important potential applications in the field of materials science.","Thu, 29 Feb 2024 10:29:05 UTC (1,252 KB)"
"129","Efficient Processing of Subsequent Densest Subgraph Query","Chia-Yang Hung, Chih-Ya Shen","Data Structures and Algorithms (cs.DS)","Dense subgraph extraction is a fundamental problem in graph analysis and data mining, aimed at identifying cohesive and densely connected substructures within a given graph. It plays a crucial role in various domains, including social network analysis, biological network analysis, recommendation systems, and community detection. However, extracting a subgraph with the highest node similarity is a lack of exploration. To address this problem, we studied the Member Selection Problem and extended it with a dynamic constraint variant. By incorporating dynamic constraints, our algorithm can adapt to changing conditions or requirements, allowing for more flexible and personalized subgraph extraction. This approach enables the algorithm to provide tailored solutions that meet specific needs, even in scenarios where constraints may vary over time. We also provide the theoretical analysis to show that our algorithm is 1/3-approximation. Eventually, the experiments show that our algorithm is effective and efficient in tackling the member selection problem with dynamic constraints.","Thu, 29 Feb 2024 06:01:57 UTC (921 KB)"
"130","Data Interpreter: An LLM Agent For Data Science","Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu Zheng","Artificial Intelligence (cs.AI)","Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at this https URL.","Wed, 28 Feb 2024 19:49:55 UTC (34,982 KB)[v2] Mon, 4 Mar 2024 18:58:37 UTC (36,808 KB)"
"131","QRIS: A Quantitative Reflectance Imaging System for the Pristine Sample of Asteroid Bennu","Ruby E. Fulford, Dathon R. Golish, Dante S. Lauretta, Daniella N. DellaGiustina, Steve Meyer, Nicole Lunning, Christopher Snead, Jason P. Dworkin, Carina A. Bennett, Harold C. Connolly Jr., Taylor Johnson, Anjani T. Polit, Pierre Haennecour, Andrew J. Ryan","Earth and Planetary Astrophysics (astro-ph.EP)","The Quantitative Reflectance Imaging System (QRIS) is a laboratory-based spectral imaging system constructed to image the sample of asteroid Bennu delivered to Earth by the Origins, Spectral Interpretation, Resource Identification, and Security-Regolith Explorer (OSIRIS-REx) spacecraft. The system was installed in the OSIRIS-REx cleanroom at NASA's Johnson Space Center to collect data during preliminary examination of the Bennu sample. QRIS uses a 12-bit machine vision camera to measure reflectance over wavelength bands spanning the near ultraviolet to the near infrared. Raw data are processed by a calibration pipeline that generates a series of monochromatic, high-dynamic-range reflectance images, as well as band ratio maps, band depth maps, and 3-channel color images. The purpose of these spectral reflectance data is to help characterize lithologies in the sample and compare them to lithologies observed on Bennu by the OSIRIS-REx spacecraft. This initial assessment of lithological diversity was intended to help select the subsamples that will be used to address mission science questions about the early solar system and the origins of life and to provide important context for the selection of representative subsamples for preservation and distribution to international partners. When QRIS imaged the Bennu sample, unexpected calibration issues arose that had not been evident at imaging rehearsals and negatively impacted the quality of QRIS data. These issues were caused by stray light within the lens and reflections off the glovebox window and interior, and were exacerbated by the sample's extremely low reflectance. QRIS data were useful for confirming conclusions drawn from other data, but reflectance and spectral data from QRIS alone unfortunately have limited utility.","Wed, 28 Feb 2024 19:38:15 UTC (36,980 KB)"
"132","The DESI Early Data Release White Dwarf Catalogue","Christopher J. Manser, Paula Izquierdo, Boris T. Gänsicke, Andrew Swan, Detlev Koester, Akshay Robert, Siyi Xu, Keith Inight, Ben Amroota, N. P. Gentile Fusillo, Sergey E. Koposov, Bokyoung Kim, Arjun Dey, Carlos Allende Prieto, J. Aguilar, S. Ahlen, R. Blum, D. Brooks, T. Claybaugh, A. P. Cooper, K. Dawson, A. de la Macorra, P. Doel, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, K. Honscheid, T. Kisner, A. Kremin, A. Lambert, M. Landriau, L. Le Guillou, Michael E. Levi, T. S. Li, A. Meisner, R. Miquel, J. Moustakas, J. Nie, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, M. Rezaie, G. Rossi, E. Sanchez, E. F. Schlafly, D. Schlegel, M. Schubnell, H. Seo, J. Silber, G. Tarlé, B. A. Weaver, Z. Zhou, H. Zou","Solar and Stellar Astrophysics (astro-ph.SR)","The Early Data Release (EDR) of the Dark Energy Spectroscopic Instrument (DESI) comprises spectroscopy obtained from 2020 December 14 to 2021 June 10. White dwarfs were targeted by DESI both as calibration sources and as science targets and were selected based on Gaia photometry and astrometry. Here we present the DESI EDR white dwarf catalogue, which includes 2706 spectroscopically confirmed white dwarfs of which approximately 1630 (roughly 60 per cent) have been spectroscopically observed for the first time, as well as 66 white dwarf binary systems. We provide spectral classifications for all white dwarfs, and discuss their distribution within the Gaia Hertzsprung-Russell diagram. We provide atmospheric parameters derived from spectroscopic and photometric fits for white dwarfs with pure hydrogen or helium photospheres, a mixture of those two, and white dwarfs displaying carbon features in their spectra. We also discuss the less abundant systems in the sample, such as those with magnetic fields, and cataclysmic variables. The DESI EDR white dwarf sample is significantly less biased than the sample observed by the Sloan Digital Sky Survey, which is skewed to bluer and therefore hotter white dwarfs, making DESI more complete and suitable for performing statistical studies of white dwarfs.","Wed, 28 Feb 2024 19:00:05 UTC (21,762 KB)"
"133","Dynamic Deterministic Constant-Approximate Distance Oracles with $n^ε$ Worst-Case Update Time","Bernhard Haeupler, Yaowei Long, Thatchaphol Saranurak","Data Structures and Algorithms (cs.DS)","We present a new distance oracle in the fully dynamic setting: given a weighted undirected graph $G=(V,E)$ with $n$ vertices undergoing both edge insertions and deletions, and an arbitrary parameter $\epsilon$ where $1/\log^{c} n<\epsilon<1$ and $c>0$ is a small constant, we can deterministically maintain a data structure with $n^{\epsilon}$ worst-case update time that, given any pair of vertices $(u,v)$, returns a $2^{{\rm poly}(1/\epsilon)}$-approximate distance between $u$ and $v$ in ${\rm poly}(1/\epsilon)\log\log n$ query time. Our algorithm significantly advances the state-of-the-art in two aspects, both for fully dynamic algorithms and even decremental algorithms. First, no existing algorithm with worst-case update time guarantees a $o(n)$-approximation while also achieving an $n^{2-\Omega(1)}$ update and $n^{o(1)}$ query time, while our algorithm offers a constant $O_{\epsilon}(1)$-approximation with $n^{\epsilon}$ update time and $O_{\epsilon}(\log \log n)$ query time. Second, even if amortized update time is allowed, it is the first deterministic constant-approximation algorithm with $n^{1-\Omega(1)}$ update and query time. The best result in this direction is the recent deterministic distance oracle by Chuzhoy and Zhang [STOC 2023] which achieves an approximation of $(\log\log n)^{2^{O(1/\epsilon^{3})}}$ with amortized update time of $n^{\epsilon}$ and query time of $2^{{\rm poly}(1/\epsilon)}\log n\log\log n$. We obtain the result by dynamizing tools related to length-constrained expanders [Haeupler-Räcke-Ghaffari, STOC 2022; Haeupler-Hershkowitz-Tan, 2023; Haeupler-Huebotter-Ghaffari, 2022]. Our technique completely bypasses the 40-year-old Even-Shiloach tree, which has remained the most pervasive tool in the area but is inherently amortized.","Wed, 28 Feb 2024 18:24:05 UTC (111 KB)"
"134","On the enumeration of signatures of XOR-CNF's","Nadia Creignou, Oscar Defrain, Frédéric Olive, Simon Vilmin","Data Structures and Algorithms (cs.DS)","Given a CNF formula $\varphi$ with clauses $C_1, \dots, C_m$ over a set of variables $V$, a truth assignment $\mathbf{a} : V \to \{0, 1\}$ generates a binary sequence $\sigma_\varphi(\mathbf{a})=(C_1(\mathbf{a}), \ldots, C_m(\mathbf{a}))$, called a signature of $\varphi$, where $C_i(\mathbf{a})=1$ if clause $C_i$ evaluates to 1 under assignment $\mathbf{a}$, and $C_i(\mathbf{a})=0$ otherwise. Signatures and their associated generation problems have given rise to new yet promising research questions in algorithmic enumeration. In a recent paper, Bérczi et al. interestingly proved that generating signatures of a CNF is tractable despite the fact that verifying a solution is hard. They also showed the hardness of finding maximal signatures of an arbitrary CNF due to the intractability of satisfiability in general. Their contribution leaves open the problem of efficiently generating maximal signatures for tractable classes of CNFs, i.e., those for which satisfiability can be solved in polynomial time. Stepping into that direction, we completely characterize the complexity of generating all, minimal, and maximal signatures for XOR-CNFs.","Wed, 28 Feb 2024 18:22:03 UTC (101 KB)"
"135","Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes","Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus","Machine Learning (cs.LG)","Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via ""which variables enter the differential of which other variables"". In this paper, we develop a kernel-based test of conditional independence (CI) on ""path-space"" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirically verify that our developed CI test in conjunction with the causal discovery algorithm reliably outperforms baselines across a range of settings.","Wed, 28 Feb 2024 16:58:31 UTC (1,332 KB)"
"136","Interval-Constrained Bipartite Matching over Time","Andreas Abels, Mariia Anapolska","Data Structures and Algorithms (cs.DS)","Interval-constrained online bipartite matching problem frequently occurs in medical appointment scheduling: unit-time jobs representing patients arrive online and are assigned to a time slot within their given time interval. We consider a variant of this problem where reassignments are allowed and extend it by a notion of current time, which is decoupled from the job arrival events. As jobs appear, the current point in time gradually advances. Jobs that are assigned to the current time unit become processed, which fixes part of the matching and disables these jobs or slots for reassignments in future steps. We refer to these time-dependent restrictions on reassignments as the over-time property. We show that FirstFit with reassignments according to the shortest augmenting path rule is $\frac{2}{3}$-competitive with respect to the matching cardinality, and that the bound is tight. Interestingly, this bound holds even if the number of reassignments per job is bound by a constant. For the number of reassignments performed by the algorithm, we show that it is in $\Omega(n \log n)$ in the worst case, where $n$ is the number of patients or jobs on the online side. This result is in line with lower bounds for the number of reassignments in online bipartite matching with reassignments, and, similarly to this previous work, we also conjecture that this bound should be tight. Known upper bounds like the $O(n \log^2 n)$ for online bipartite matching with reassignments by Bernstein, Holm, and Rotenberg do not transfer directly: while our interval constraints simplify the problem, the over-time property restricts the set of possible reassignments.","Wed, 28 Feb 2024 16:44:10 UTC (26 KB)[v2] Fri, 1 Mar 2024 17:41:52 UTC (26 KB)"
"137","Out-of-Domain Generalization in Dynamical Systems Reconstruction","Niclas Göring, Florian Hess, Manuel Brenner, Zahra Monfared, Daniel Durstewitz","Machine Learning (cs.LG)","In science we are interested in finding the governing equations, the dynamical rules, underlying empirical phenomena. While traditionally scientific models are derived through cycles of human insight and experimentation, recently deep learning (DL) techniques have been advanced to reconstruct dynamical systems (DS) directly from time series data. State-of-the-art dynamical systems reconstruction (DSR) methods show promise in capturing invariant and long-term properties of observed DS, but their ability to generalize to unobserved domains remains an open challenge. Yet, this is a crucial property we would expect from any viable scientific theory. In this work, we provide a formal framework that addresses generalization in DSR. We explain why and how out-of-domain (OOD) generalization (OODG) in DSR profoundly differs from OODG considered elsewhere in machine learning. We introduce mathematical notions based on topological concepts and ergodic theory to formalize the idea of learnability of a DSR model. We formally prove that black-box DL techniques, without adequate structural priors, generally will not be able to learn a generalizing DSR model. We also show this empirically, considering major classes of DSR algorithms proposed so far, and illustrate where and why they fail to generalize across the whole phase space. Our study provides the first comprehensive mathematical treatment of OODG in DSR, and gives a deeper conceptual understanding of where the fundamental problems in OODG lie and how they could possibly be addressed in practice.","Wed, 28 Feb 2024 14:52:58 UTC (9,514 KB)"
"138","DynaWarp -- Efficient, large-scale log storage and retrieval","Julian Reichinger, Thomas Krismayer, Jan Rellermeyer","Information Retrieval (cs.IR)","Modern, large scale monitoring systems have to process and store vast amounts of log data in near real-time. At query time the systems have to find relevant logs based on the content of the log message using support structures that can scale to these amounts of data while still being efficient to use. We present our novel DynaWarp membership sketch, capable of answering Multi-Set Multi-Membership-Queries, that can be used as an alternative to existing indexing structures for streamed log data. In our experiments, DynaWarp required up to 93% less storage space than the tested state-of-the-art inverted index and had up to four orders of magnitude less false-positives than the tested state-of-the-art membership sketch. Additionally, DynaWarp achieved up to 250 times higher query throughput than the tested inverted index and up to 240 times higher query throughput than the tested membership sketch.","Wed, 28 Feb 2024 14:26:52 UTC (992 KB)"
"139","Polynomial-time approximation schemes for induced subgraph problems on fractionally tree-independence-number-fragile graphs","Esther Galby, Andrea Munaro, Shizhou Yang","Data Structures and Algorithms (cs.DS)","We investigate a relaxation of the notion of fractional treewidth-fragility, namely fractional tree-independence-number-fragility. In particular, we obtain polynomial-time approximation schemes for meta-problems such as finding a maximum-weight sparse induced subgraph satisfying a given $\mathsf{CMSO}_2$ formula on fractionally tree-independence-number-fragile graph classes. Our approach unifies and extends several known polynomial-time approximation schemes on seemingly unrelated graph classes, such as classes of intersection graphs of fat objects in a fixed dimension or proper minor-closed classes. We also study the related notion of layered tree-independence number, a relaxation of layered treewidth, and its applications to exact subexponential-time algorithms.","Wed, 28 Feb 2024 14:20:29 UTC (143 KB)"
"140","Online Edge Coloring is (Nearly) as Easy as Offline","Joakim Blikstad, Ola Svensson, Radu Vintan, David Wajc","Data Structures and Algorithms (cs.DS)","The classic theorem of Vizing (Diskret. Analiz.'64) asserts that any graph of maximum degree $\Delta$ can be edge colored (offline) using no more than $\Delta+1$ colors (with $\Delta$ being a trivial lower bound). In the online setting, Bar-Noy, Motwani and Naor (IPL'92) conjectured that a $(1+o(1))\Delta$-edge-coloring can be computed online in $n$-vertex graphs of maximum degree $\Delta=\omega(\log n)$. Numerous algorithms made progress on this question, using a higher number of colors or assuming restricted arrival models, such as random-order edge arrivals or vertex arrivals (e.g., AGKM FOCS'03, BMM SODA'10, CPW FOCS'19, BGW SODA'21, KLSST STOC'22). In this work, we resolve this longstanding conjecture in the affirmative in the most general setting of adversarial edge arrivals. We further generalize this result to obtain online counterparts of the list edge coloring result of Kahn (J. Comb. Theory. A'96) and of the recent ""local"" edge coloring result of Christiansen (STOC'23).","Wed, 28 Feb 2024 14:03:59 UTC (183 KB)"
"141","Fractional Linear Matroid Matching is in quasi-NC","Rohit Gurjar, Taihei Oki, Roshan Raj","Computational Complexity (cs.CC)","The matching and linear matroid intersection problems are solvable in quasi-NC, meaning that there exist deterministic algorithms that run in polylogarithmic time and use quasi-polynomially many parallel processors. However, such a parallel algorithm is unknown for linear matroid matching, which generalizes both of these problems. In this work, we propose a quasi-NC algorithm for fractional linear matroid matching, which is a relaxation of linear matroid matching and commonly generalizes fractional matching and linear matroid intersection. Our algorithm builds upon the connection of fractional matroid matching to non-commutative Edmonds' problem recently revealed by Oki and Soma~(2023). As a corollary, we also solve black-box non-commutative Edmonds' problem with rank-two skew-symmetric coefficients.","Wed, 28 Feb 2024 12:12:37 UTC (28 KB)"
"142","Output-Sensitive Enumeration of Potential Maximal Cliques in Polynomial Space","Caroline Brosse, Alessio Conte, Vincent Limouzy, Giulia Punzi, Davide Rucci","Data Structures and Algorithms (cs.DS)","A set of vertices in a graph forms a potential maximal clique if there exists a minimal chordal completion in which it is a maximal clique. Potential maximal cliques were first introduced as a key tool to obtain an efficient, though exponential-time algorithm to compute the treewidth of a graph. As a byproduct, this allowed to compute the treewidth of various graph classes in polynomial time. In recent years, the concept of potential maximal cliques regained interest as it proved to be useful for a handful of graph algorithmic problems. In particular, it turned out to be a key tool to obtain a polynomial time algorithm for computing maximum weight independent sets in $P_5$-free and $P_6$-free graphs (Lokshtanov et al., SODA `14 and Grzeskik et al., SODA `19. In most of their applications, obtaining all the potential maximal cliques constitutes an algorithmic bottleneck, thus motivating the question of how to efficiently enumerate all the potential maximal cliques in a graph $G$. The state-of-the-art algorithm by Bouchitté \& Todinca can enumerate potential maximal cliques in output-polynomial time by using exponential space, a significant limitation for the size of feasible instances. In this paper, we revisit this algorithm and design an enumeration algorithm that preserves an output-polynomial time complexity while only requiring polynomial space.","Wed, 28 Feb 2024 11:53:26 UTC (41 KB)"
"143","Max-Cut with $ε$-Accurate Predictions","Vincent Cohen-Addad, Tommaso d'Orsi, Anupam Gupta, Euiwoong Lee, Debmalya Panigrahi","Data Structures and Algorithms (cs.DS)","We study the approximability of the MaxCut problem in the presence of predictions. Specifically, we consider two models: in the noisy predictions model, for each vertex we are given its correct label in $\{-1,+1\}$ with some unknown probability $1/2 + \epsilon$, and the other (incorrect) label otherwise. In the more-informative partial predictions model, for each vertex we are given its correct label with probability $\epsilon$ and no label otherwise. We assume only pairwise independence between vertices in both models. We show how these predictions can be used to improve on the worst-case approximation ratios for this problem. Specifically, we give an algorithm that achieves an $\alpha + \widetilde{\Omega}(\epsilon^4)$-approximation for the noisy predictions model, where $\alpha \approx 0.878$ is the MaxCut threshold. While this result also holds for the partial predictions model, we can also give a $\beta + \Omega(\epsilon)$-approximation, where $\beta \approx 0.858$ is the approximation ratio for MaxBisection given by Raghavendra and Tan. This answers a question posed by Ola Svensson in his plenary session talk at SODA'23.","Wed, 28 Feb 2024 11:51:28 UTC (34 KB)"
"144","Lower Bounds for Leaf Rank of Leaf Powers","Svein Høgemo","Discrete Mathematics (cs.DM)","Leaf powers and $k$-leaf powers have been studied for over 20 years, but there are still several aspects of this graph class that are poorly understood. One such aspect is the leaf rank of leaf powers, i.e. the smallest number $k$ such that a graph $G$ is a $k$-leaf power. Computing the leaf rank of leaf powers has proved a hard task, and furthermore, results about the asymptotic growth of the leaf rank as a function of the number of vertices in the graph have been few and far between. We present an infinite family of rooted directed path graphs that are leaf powers, and prove that they have leaf rank exponential in the number of vertices (utilizing a type of subtree model first presented by Rautenbach [Some remarks about leaf roots. Discrete mathematics, 2006]). This answers an open question by Brandstädt et al. [Rooted directed path graphs are leaf powers. Discrete mathematics, 2010].","Wed, 28 Feb 2024 11:18:16 UTC (224 KB)"
"145","Computing Minimal Absent Words and Extended Bispecial Factors with CDAWG Space","Shunsuke Inenaga, Takuya Mieno, Hiroki Arimura, Mitsuru Funakoshi, Yuta Fujishige","Data Structures and Algorithms (cs.DS)","A string $w$ is said to be a minimal absent word (MAW) for a string $S$ if $w$ does not occur in $S$ and any proper substring of $w$ occurs in $S$. We focus on non-trivial MAWs which are of length at least 2. Finding such non-trivial MAWs for a given string is motivated for applications in bioinformatics and data compression. Fujishige et al. [TCS 2023] proposed a data structure of size $\Theta(n)$ that can output the set $\mathsf{MAW}(S)$ of all MAWs for a given string $S$ of length $n$ in $O(n + |\mathsf{MAW}(S)|)$ time, based on the directed acyclic word graph (DAWG). In this paper, we present a more space efficient data structure based on the compact DAWG (CDAWG), which can output $\mathsf{MAW}(S)$ in $O(|\mathsf{MAW}(S)|)$ time with $O(e)$ space, where $e$ denotes the minimum of the sizes of the CDAWGs for $S$ and for its reversal $S^R$. For any strings of length $n$, it holds that $e < 2n$, and for highly repetitive strings $e$ can be sublinear (up to logarithmic) in $n$. We also show that MAWs and their generalization minimal rare words have close relationships with extended bispecial factors, via the CDAWG.","Wed, 28 Feb 2024 06:22:24 UTC (1,727 KB)"
"146","Tighter Bounds for Local Differentially Private Core Decomposition and Densest Subgraph","Monika Henzinger, A. R. Sricharan, Leqi Zhu","Data Structures and Algorithms (cs.DS)","Computing the core decomposition of a graph is a fundamental problem that has recently been studied in the differentially private setting, motivated by practical applications in data mining. In particular, Dhulipala et al. [FOCS 2022] gave the first mechanism for approximate core decomposition in the challenging and practically relevant setting of local differential privacy. One of the main open problems left by their work is whether the accuracy, i.e., the approximation ratio and additive error, of their mechanism can be improved. We show the first lower bounds on the additive error of approximate and exact core decomposition mechanisms in the centralized and local model of differential privacy, respectively. We also give mechanisms for exact and approximate core decomposition in the local model, with almost matching additive error bounds. Our mechanisms are based on a black-box application of continual counting. They also yield improved mechanisms for the approximate densest subgraph problem in the local model.","Wed, 28 Feb 2024 03:30:31 UTC (113 KB)"
"147","A Compact Anomaly Detection Solution for Science Instruments","Alfonso Lagares de Toledo, Christopher E. Carr","Instrumentation and Detectors (physics.ins-det)","Small low-cost instruments enable new and exciting mission opportunities yet their constrained volume and limited budgets make them especially susceptible to suffering anomalies during flight. Radiation effects as well as sensor or actuator failure can all pose a serious threat to the continued collection of scientific data as well as cause the partial or complete loss of a mission science payload. Onboard anomaly detection could allow instruments to recover from such events but its ad hoc development typically falls outside the mission timeline or monetary constraints. Here we describe a compact solution for the implementation of onboard anomaly detection meant for space science missions. The device is designed to be interoperable with a broad range of instruments utilizing easily accessible power and logic signals to monitor the state of peripherals and actuators without disrupting their functionality. By leveraging a commercially available microcontroller with a radiation hardened alternative package the device can be inexpensively sourced and assembled with minimal work enabling instrument characterization on an expedited timeline. The system can then be exchanged for a radiation hardened version ensuring the replicability of observed anomalies in a laboratory environment during instrument operations. We also present currently implemented anomaly detection algorithms which enable the system to detect anomalies in instruments with varying failure modes and allow mission designers to choose which detection approach best fits the specific needs of their instrument. Finally, we showcase an example application of this system in the detection of anomalies during the operation of a lysis motor designed for use in biological space instruments.","Wed, 28 Feb 2024 01:00:49 UTC (4,537 KB)"
"148","Decremental $(1+ε)$-Approximate Maximum Eigenvector: Dynamic Power Method","Deeksha Adil, Thatchaphol Saranurak","Data Structures and Algorithms (cs.DS)","We present a dynamic algorithm for maintaining $(1+\epsilon)$-approximate maximum eigenvector and eigenvalue of a positive semi-definite matrix $A$ undergoing \emph{decreasing} updates, i.e., updates which may only decrease eigenvalues. Given a vector $v$ updating $A\gets A-vv^{\top}$, our algorithm takes $\tilde{O}(\mathrm{nnz}(v))$ amortized update time, i.e., polylogarithmic per non-zeros in the update vector. Our technique is based on a novel analysis of the influential power method in the dynamic setting. The two previous sets of techniques have the following drawbacks (1) algebraic techniques can maintain exact solutions but their update time is at least polynomial per non-zeros, and (2) sketching techniques admit polylogarithmic update time but suffer from a crude additive approximation. Our algorithm exploits an oblivious adversary. Interestingly, we show that any algorithm with polylogarithmic update time per non-zeros that works against an adaptive adversary and satisfies an additional natural property would imply a breakthrough for checking psd-ness of matrices in $\tilde{O}(n^{2})$ time, instead of $O(n^{\omega})$ time.","Tue, 27 Feb 2024 23:04:28 UTC (69 KB)"
"149","Robustly Learning Single-Index Models via Alignment Sharpness","Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas","Machine Learning (cs.LG)","We study the problem of learning Single-Index Models under the $L_2^2$ loss in the agnostic model. We give an efficient learning algorithm, achieving a constant factor approximation to the optimal loss, that succeeds under a range of distributions (including log-concave distributions) and a broad class of monotone and Lipschitz link functions. This is the first efficient constant factor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link functions. Prior work for the case of unknown link function either works in the realizable setting or does not attain constant factor approximation. The main technical ingredient enabling our algorithm and analysis is a novel notion of a local error bound in optimization that we term alignment sharpness and that may be of broader interest.","Tue, 27 Feb 2024 18:48:07 UTC (290 KB)"
"150","Learning-Based Algorithms for Graph Searching Problems","Adela Frances DePavia, Erasmo Tani, Ali Vakilian","Data Structures and Algorithms (cs.DS)","We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et al. (2022) for the case of searching on a known graph, and establish new lower bounds for this setting.","Tue, 27 Feb 2024 18:12:58 UTC (978 KB)"
