"","title","author","subject","abstract","date"
"1","self-consistency training for hamiltonian prediction","he zhang, chang liu, zun wang, xinran wei, siyuan liu, nanning zheng, bin shao, tie-yan liu","machine learning","hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. yet, its applicability is limited by insufficient labeled data for training. in this work, we highlight that hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. this merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with dft for supervised training, since it is an amortization of dft calculation over a set of molecular structures. we empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios, and the better efficiency from the amortization. these benefits push forward the applicability of hamiltonian prediction to an ever larger scale.",2024-03-14
"2","outlier robust multivariate polynomial regression","vipul arora, arnab bhattacharyya, mathews boban, venkatesan guruswami, esty kelman","data structures and algorithms","we study the problem of robust multivariate polynomial regression: let $p\colon\mathbb{r}^n\to\mathbb{r}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable. we are given as input a set of random samples $(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{r}$ that are noisy versions of $(\mathbf{x}_i,p(\mathbf{x}_i))$. more precisely, each $\mathbf{x}_i$ is sampled independently from some distribution $\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\rho < 1/2$, and otherwise satisfies $|y_i-p(\mathbf{x}_i)|\leq\sigma$. the goal is to output a polynomial $\hat{p}$, of degree at most $d$ in each variable, within an $\ell_\infty$-distance of at most $o(\sigma)$ from $p$. kane, karmalkar, and price [focs'17] solved this problem for $n=1$. we generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity of $o_n(d^n\log d)$, where the hidden constant depends on $n$, if $\chi$ is the $n$-dimensional chebyshev distribution. the sample complexity is $o_n(d^{2n}\log d)$, if the samples are drawn from the uniform distribution instead. the approximation error is guaranteed to be at most $o(\sigma)$, and the run-time depends on $\log(1/\sigma)$. in the setting where each $\mathbf{x}_i$ and $y_i$ are known up to $n$ bits of precision, the run-time's dependence on $n$ is linear. we also show that our sample complexities are optimal in terms of $d^n$. furthermore, we show that it is possible to have the run-time be independent of $1/\sigma$, at the cost of a higher sample complexity.",2024-03-14
"3","deep-learning-assisted optical communication with discretized state space of structural light","minyang zhang, dong-xu chen, pengxiang ruan, jun liu, jun-long zhao, chui-ping yang","optics","the rich structure of the transverse spatial mode of structural light has facilitated its applications in quantum information and optical communication. the laguerre-gaussian (lg) modes, with azimuthal and radial indexes, consist of a complete orthogonal basis to describe the transverse spatial mode of light. the azimuthal index is often endowed with the orbital angular momentum (oam), a high dimensional degree of freedom. the advent of oam in optical science marks a pivotal advancement, surpassing traditional optical techniques in light manipulation for advanced data encoding and signal transmission. here, we present a scheme that utilizes the advanced deep learning technique for lg modes recognition. by discretizing the state space of the lg modes, a neural network model is trained to classify the given samples. a proof-of-principle experiment is performed to show that our scheme requires less samples for model training, while increasing the channel capacity within limited oam number. we further apply our scheme to an image transmission task, demonstrating the ability to encode large data with low oam number. our work opens a new avenue for high capacity optical communication based on structural light.",2024-03-14
"4","a simple reconstruction method to infer nonreciprocal interactions and local driving in complex systems","tim hempel, sarah a. m. loos","statistical mechanics","data-based inference of directed interactions in complex dynamical systems is a problem common to many disciplines of science. in this work, we study networks of spatially separate dynamical entities, which could represent physical systems that interact with each other by reciprocal or nonreciprocal, instantaneous or time-delayed interactions. we present a simple approach that combines markov state models with directed information-theoretical measures for causal inference that can accurately infer the underlying interactions from noisy time series of the dynamical system states alone. remarkably, this is possible despite the built-in simplification of a markov assumption and the choice of a very coarse discretization at the level of probability estimation. our test systems are an ising chain with nonreciprocal coupling imposed by local driving of a single spin, and a system of delay-coupled linear stochastic processes. stepping away from physical systems, the approach infers cause-effect relationships, or more generally, the direction of mutual or one-way influence. the presented method is agnostic to the number of interacting entities and details of the dynamics, so that it is widely applicable to problems in various fields.",2024-03-14
"5","efficient size-prescribed $k$-core search","yiping liu, bo yan, bo zhao, hongyi su, yang chen, michael witbrock","data structures and algorithms","$k$-core is a subgraph where every node has at least $k$ neighbors within the subgraph. the $k$-core subgraphs has been employed in large platforms like network repository to comprehend the underlying structures and dynamics of the network. existing studies have primarily focused on finding $k$-core groups without considering their size, despite the relevance of solution sizes in many real-world scenarios. this paper addresses this gap by introducing the size-prescribed $k$-core search (spcs) problem, where the goal is to find a subgraph of a specified size that has the highest possible core number. we propose two algorithms, namely the {\it tsizekcore-bu} and the {\it tsizekcore-td}, to identify cohesive subgraphs that satisfy both the $k$-core requirement and the size constraint. our experimental results demonstrate the superiority of our approach in terms of solution quality and efficiency. the {\it tsizekcore-bu} algorithm proves to be highly efficient in finding size-prescribed $k$-core subgraphs on large datasets, making it a favorable choice for such scenarios. on the other hand, the {\it tsizekcore-td} algorithm is better suited for small datasets where running time is less critical.",2024-03-14
"6","uncovering the invisible: a study of gaia18ajz, a candidate black hole revealed by microlensing","k. howil, ł. wyrzykowski, k. kruszyńska, p. zieliński, e. bechelet, m. gromadzki, m. jabłońska, z. kaczmarek, p. mróz, m. ratajczak, k. rybicki, p. j. mikołajczyk, s. t. hodgkin, j.m. carrasco, u. burgaz, v. godunova, a. simon, f. cusano, m. jelinek, j. štrobl, o. erece, f. olivares, m. morell","astrophysics of galaxies","identifying black holes is essential for comprehending the development of stars and uncovering novel principles of physics. gravitational microlensing provides an exceptional opportunity to examine an undetectable population of black holes in the milky way. in particular, long-lasting events are likely to be associated with massive lenses, including black holes. we present an analysis of the gaia18ajz microlensing event, reported by the gaia science alerts system, which has exhibited a long timescale and features indicative of the annual microlensing parallax effect. our objective is to estimate the parameters of the lens based on the best-fitting model. we utilized photometric data obtained from the gaia satellite and terrestrial observatories to investigate a variety of microlensing models and calculate the most probable mass and distance to the lens, taking into consideration a galactic model as a prior. subsequently, we applied a mass-brightness relation to evaluate the likelihood that the lens is a main sequence star. we also describe darklenscode, an open-source routine which computes the distribution of probable lens mass, distance and luminosity employing the galaxy priors on stellar density and velocity for microlensing events with detected microlensing parallax. we modelled gaia18ajz event and found its two possible models with most likely einstein timescale of $t_\mathrm{e}=316^{+36}_{-30}$ days and $t_\mathrm{e}=299^{+25}_{-22}$ days. applying galaxy priors for stellar density and motion, we calculated the most probable lens mass of $m_l = 5.6^{+7.5}_{-2.5} m_\odot$ located at $d_s = 1.05^{+0.78}_{-0.60}\,\text{kpc}$ or $m_l = 12.0^{+14.9}_{-5.4} m_\odot$ located at $d_s = 1.18^{+0.82}_{-0.63}\,\text{kpc}$. our analysis of the blended light suggests that the lens is likely a dark remnant of stellar evolution, rather than a main sequence star.",2024-03-13
"7","approximating small sparse cuts","aditya anand, euiwoong lee, jason li, thatchaphol saranurak","data structures and algorithms","we study polynomial-time approximation algorithms for (edge/vertex) sparsest cut and small set expansion in terms of $k$, the number of edges or vertices cut in the optimal solution. our main results are $\mathcal{o}(\text{polylog}\, k)$-approximation algorithms for various versions in this setting. our techniques involve an extension of the notion of sample sets (feige and mahdian stoc'06), originally developed for small balanced cuts, to sparse cuts in general. we then show how to combine this notion of sample sets with two algorithms, one based on an existing framework of lp rounding and another new algorithm based on the cut-matching game, to get such approximation algorithms. our cut-matching game algorithm can be viewed as a local version of the cut-matching game by khandekar, khot, orecchia and vishnoi and certifies an expansion of every vertex set of size $s$ in $\mathcal{o}(\log s)$ rounds. these techniques may be of independent interest. as corollaries of our results, we also obtain an $\mathcal{o}(\log opt)$-approximation for min-max graph partitioning, where $opt$ is the min-max value of the optimal cut, and improve the bound on the size of multicut mimicking networks computable in polynomial time.",2024-03-13
"8","designing a data science simulation with merits: a primer","corrine f elliott, james duncan, tiffany m tang, merle behr, karl kumbier, bin yu","computation","simulations play a crucial role in the modern scientific process. yet despite (or due to) their ubiquity, the data science community shares neither a comprehensive definition for a ""high-quality"" study nor a consolidated guide to designing one. inspired by the predictability-computability-stability (pcs) framework for 'veridical' data science, we propose six merits that a data science simulation should satisfy. modularity and efficiency support the computability of a study, encouraging clean and flexible implementation. realism and stability address the conceptualization of the research problem: how well does a study predict reality, such that its conclusions generalize to new data/contexts? finally, intuitiveness and transparency encourage good communication and trustworthiness of study design and results. drawing an analogy between simulation and cooking, we moreover offer (a) a conceptual framework for thinking about the anatomy of a simulation 'recipe'; (b) a baker's dozen in guidelines to aid the data science practitioner in designing one; and (c) a case study deconstructing a simulation through the lens of our framework to demonstrate its practical utility. by contributing this ""pcs primer"" for high-quality data science simulation, we seek to distill and enrich the best practices of simulation across disciplines into a cohesive recipe for trustworthy, veridical data science.",2024-03-13
"9","deep learning based dynamics identification and linearization of orbital problems using koopman theory","george nehma, madhur tiwari, manasvi lingam","mathematical physics","the study of the two-body and circular restricted three-body problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. with the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. we propose a data-driven framework for simultaneous system identification and global linearization of both the two-body problem and circular restricted three-body problem via deep learning-based koopman theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (lti) system. the linear koopman operator is discovered through purely data-driven training of a deep neural network with a custom architecture. this paper displays the ability of the koopman operator to generalize to various other two-body systems without the need for retraining. we also demonstrate the capability of the same architecture to be utilized to accurately learn a koopman operator that approximates the circular restricted three-body problem.",2024-03-13
"10","two-sided assortment optimization: adaptivity gaps and approximation algorithms","omar el housni, alfredo torrico, ulysse hennebelle","optimization and control","to address the challenge of choice congestion in matching markets, in this work, we introduce a two-sided assortment optimization framework under general choice preferences. the goal in this problem is to maximize the expected number of matches by deciding which assortments are displayed to the agents and the order in which they are shown. in this context, we identify several classes of policies that platforms can use in their design. our goals are: (1) to measure the value that one class of policies has over another one, and (2) to approximately solve the optimization problem itself for a given class. for (1), we define the adaptivity gap as the worst-case ratio between the optimal values of two different policy classes. first, we show that the gap between the class of policies that statically show assortments to one-side first and the class of policies that adaptively show assortments to one-side first is exactly $1-1/e$. second, we show that the gap between the latter class of policies and the fully adaptive class of policies that show assortments to agents one by one is exactly $1/2$. we also note that the worst policies are those who simultaneously show assortments to all the agents, in fact, we show that their adaptivity gap even with respect to one-sided static policies can be arbitrarily small. for (2), we first show that there exists a polynomial time policy that achieves a $1/4$ approximation factor within the class of policies that adaptively show assortments to agents one by one. finally, when agents' preferences are governed by multinomial-logit models, we show that a 0.066 approximation factor can be obtained within the class of policies that show assortments to all agents at once.",2024-03-13
"11","efficiently computing similarities to private datasets","arturs backurs, zinan lin, sepideh mahabadi, sandeep silwal, jakub tarnawski","cryptography and security","many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. we abstract out this common subroutine and study the following fundamental algorithmic problem: given a similarity function $f$ and a large high-dimensional private dataset $x \subset \mathbb{r}^d$, output a differentially private (dp) data structure which approximates $\sum_{x \in x} f(x,y)$ for any query $y$. we consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$ (also known as dp kernel density estimation), or a distance function such as $f(x,y) = \|x-y\|_2$, among others. our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. the unifying approach behind our results is leveraging `low-dimensional structures' present in the specific functions $f$ that we study, using tools such as provable dimensionality reduction, approximation theory, and one-dimensional decomposition of the functions. our algorithms empirically exhibit improved query times and accuracy over prior state of the art. we also present an application to dp classification. our experiments demonstrate that the simple methodology of classifying based on average similarity is orders of magnitude faster than prior dp-sgd based approaches for comparable accuracy.",2024-03-13
"12","people attribute purpose to autonomous vehicles when explaining their behavior","balint gyevnar, stephanie droop, tadeg quillien","human-computer interaction","a hallmark of a good xai system is explanations that users can understand and act on. in many cases, this requires a system to offer causal or counterfactual explanations that are intelligible. cognitive science can help us understand what kinds of explanations users might expect, and in which format to frame these explanations. we briefly review relevant literature from the cognitive science of explanation, particularly as it concerns teleology, the tendency to explain a decision in terms of the purpose it was meant to achieve. we then report empirical data on how people generate explanations for the behavior of autonomous vehicles, and how they evaluate these explanations. in a first survey, participants (n=54) were shown videos of a road scene and asked to generate either mechanistic, counterfactual, or teleological verbal explanations for a vehicle's actions. in the second survey, a different set of participants (n=356) rated these explanations along various metrics including quality, trustworthiness, and how much each explanatory mode was emphasized in the explanation. participants deemed mechanistic and teleological explanations as significantly higher quality than counterfactual explanations. in addition, perceived teleology was the best predictor of perceived quality and trustworthiness. neither perceived teleology nor quality ratings were affected by whether the car whose actions were being explained was an autonomous vehicle or was being driven by a person. the results show people use and value teleological concepts to evaluate information about both other people and autonomous vehicles, indicating they find the 'intentional stance' a convenient abstraction. we make our dataset of annotated video situations with explanations, called human explanations for autonomous driving decisions (headd), publicly available, which we hope will prompt further research.",2024-03-11
"13","improved randomized approximation of hard universality and emptiness problems","pantelis andreou, stavros konstantinidis, taylor j. smith","data structures and algorithms","we build on recent research on polynomial randomized approximation (prax) algorithms for the hard problems of nfa universality and nfa equivalence. loosely speaking, prax algorithms use sampling of infinite domains within any desired accuracy $\delta$. in the spirit of experimental mathematics, we extend the concept of prax algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper. a technical result here is that a linear (w.r.t. $1/\delta$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers. we show how the improved and generalized prax algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2d automata, and to solution sets of certain diophantine equations.",2024-03-13
"14","zero-shot and few-shot generation strategies for artificial clinical records","erlend frayling, jake lever, graham mcdonald","computation and language","the challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. an innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. the creation of these synthetic datasets, particularly without using actual patient data to train large language models (llms), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. this study assesses the capability of the llama 2 llm to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. we focus on generating synthetic narratives for the history of present illness section, utilising data from the mimic-iv dataset for comparison. in this work introduce a novel prompting technique that leverages a chain-of-thought approach, enhancing the model's ability to generate more accurate and contextually relevant medical narratives without prior fine-tuning. our findings suggest that this chain-of-thought prompted approach allows the zero-shot model to achieve results on par with those of fine-tuned models, based on rouge metrics evaluation.",2024-03-13
"15","opportunities and open questions in modern $β$ decay","leendert hayen","nuclear theory","for well over half a century, precision studies of neutron and nuclear $\beta$ decays have been at the forefront of searches for exotic electroweak physics. recent advances in nuclear ab initio theory and the widespread use of effective field theories means that its modern understanding is going through a transitional phase. this has been propelled by current tensions in the global data set leading to renewed scrutiny of its theoretical ingredients. in parallel, a host of novel techniques and methods are being investigated that are able to sidestep many traditional systematic uncertainties and require a diverse palette of skills and collaboration with material science and condensed matter physics. we highlight the current opportunities and open questions with the aim of facilitating the transition to a more modern understanding of $\beta$ decay.",2024-03-13
"16","worst-case to expander-case reductions: derandomized and generalized","amir abboud, nathan wallheimer","data structures and algorithms","a recent paper by abboud and wallheimer [itcs 2023] presents self-reductions for various fundamental graph problems, that transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander. an interesting corollary of their self-reductions is that, if some problem admit such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it. in this paper, we improve their core gadget, which augments a graph to make it an expander while retaining its important structure. our new core construction has the benefit of being simple to analyze and generalize, while obtaining the following results: 1. a derandomization of the self-reductions, showing that the equivalence between worst-case and expander-case holds even for deterministic algorithms, and ruling out the use of expander-decompositions as a derandomization tool. 2. an extension of the results to other models of computation, such as the fully dynamic model and the congested clique model. in the former, we either improve or provide an alternative approach to some recent hardness results for dynamic expander graphs, by henzinger, paz, and sricharan [esa 2022]. in addition, we continue this line of research by designing new self-reductions for more problems, such as max-cut and dynamic densest subgraph, and demonstrating that the core gadget can be utilized to lift lower bounds based on the omv conjecture to expanders.",2024-03-13
"17","cleanagent: automating data standardization with llm-based agents","danrui qi, jiannan wang","machine learning","data standardization is a crucial part in data science life cycle. while tools like pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. although large language models (llms) like chatgpt have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. to solve these challenges, our key idea is to propose a python library with declarative, unified apis for standardizing column types, simplifying the code generation of llm with concise api calls. we first propose dataprep.clean which is written as a component of the dataprep library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. then we introduce the cleanagent framework integrating dataprep.clean and llm-based agents to automate the data standardization process. with cleanagent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.",2024-03-13
"18","height-bounded lempel-ziv encodings","hideo bannai, mitsuru funakoshi, diptarama hendrian, myuji matsuda, simon j. puglisi","data structures and algorithms","we introduce height-bounded lz encodings (lzhb), a new family of compressed representations that is a variant of lempel-ziv parsings with a focus on allowing fast access to arbitrary positions of the text directly via the compressed representation. any lzhb encoding whose referencing height is bounded by $h$ allows access to an arbitrary position of the underlying text using $o(h)$ predecessor queries. we show that there exists a constant $c$ such that the size $\hat{z}_{\mathit{hb}(c\log n)}$ of the optimal (smallest) lzhb encoding whose height is bounded by $c\log n$ for any string of length $n$ is $o(\hat{g}_{\mathrm{rl}})$, where $\hat{g}_{\mathrm{rl}}$ is the size of the smallest run-length grammar. furthermore, we show that there exists a family of strings such that $\hat{z}_{\mathit{hb}(c\log n)} = o(\hat{g}_{\mathrm{rl}})$, thus making $\hat{z}_{\mathit{hb}(c\log n)}$ one of the smallest known repetitiveness measures for which $o(\mathit{polylog} n)$ time access is possible using $o(\hat{z}_{\mathit{hb}(c\log n)})$ space. while computing the optimal lzhb representation for any given height seems difficult, we propose linear and near linear time greedy algorithms which we show experimentally can efficiently find small lzhb representations in practice.",2024-03-13
"19","a bargain for mergesorts (functional pearl) -- how to prove your mergesort correct and stable, almost for free","cyril cohen, kazuhiko sakaguchi","logic in computer science","we present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the correctness of mergesort. as a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characterization property for each variation. to further motivate this work, we show a performance trade-off between tail-recursive and non-tail-recursive mergesorts that (1) the former in call-by-value evaluation avoids using up stack space and is efficient and (2) the latter in call-by-need evaluation is an optimal incremental sort, meaning that it performs only $\mathcal{o}(n + k \log k)$ comparisons to compute the least (or greatest) $k$ items of a list of length $n$. thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the coq proof assistant.",2024-03-13
"20","the runtime of random local search on the generalized needle problem","benjamin doerr, andrew james kelley","neural and evolutionary computing","in their recent work, c. doerr and krejca (transactions on evolutionary computation, 2023) proved upper bounds on the expected runtime of the randomized local search heuristic on generalized needle functions. based on these upper bounds, they deduce in a not fully rigorous manner a drastic influence of the needle radius $k$ on the runtime. in this short article, we add the missing lower bound necessary to determine the influence of parameter $k$ on the runtime. to this aim, we derive an exact description of the expected runtime, which also significantly improves the upper bound given by c. doerr and krejca. we also describe asymptotic estimates of the expected runtime.",2024-03-13
"21","highway preferential attachment models for geographic routing","ofek gila (1), evrim ozel (1), michael t. goodrich (1), ((1) university of california, irvine)","data structures and algorithms","in the 1960s, the world-renowned social psychologist stanley milgram conducted experiments that showed that not only do there exist ``short chains'' of acquaintances between any two arbitrary people, but that these arbitrary strangers are able to find these short chains. this phenomenon, known as the \emph{small-world phenomenon}, is explained in part by any model that has a low diameter, such as the barabási and albert's \emph{preferential attachment} model, but these models do not display the same efficient routing that milgram's experiments showed. in the year 2000, kleinberg proposed a model with an efficient $\mathcal{o}(\log^2{n})$ greedy routing algorithm. in 2004, martel and nguyen showed that kleinberg's analysis was tight, while also showing that kleinberg's model had an expected diameter of only $\theta(\log{n})$ -- a much smaller value than the greedy routing algorithm's path lengths. in 2022, goodrich and ozel proposed the \emph{neighborhood preferential attachment} model (npa), combining elements from barabási and albert's model with kleinberg's model, and experimentally showed that the resulting model outperformed kleinberg's greedy routing performance on u.s. road networks. while they displayed impressive empirical results, they did not provide any theoretical analysis of their model. in this paper, we first provide a theoretical analysis of a generalization of kleinberg's original model and show that it can achieve expected $\mathcal{o}(\log{n})$ routing, a much better result than kleinberg's model. we then propose a new model, \emph{windowed npa}, that is similar to the neighborhood preferential attachment model but has provable theoretical guarantees w.h.p. we show that this model is able to achieve $\mathcal{o}(\log^{1 + \epsilon}{n})$ greedy routing for any $\epsilon > 0$.",2024-03-12
"22","preserving automotive heritage: a blockchain-based solution for secure documentation of classic cars restoration","josé murta, vasco amaral, fernando brito e abreu","computers and society","classic automobiles are an important part of the automotive industry and represent the historical and technological achievements of certain eras. however, to be considered masterpieces, they must be maintained in pristine condition or restored according to strict guidelines applied by expert services. therefore, all data about restoration processes and other relevant information about these vehicles must be rigorously documented to ensure their verifiability and immutability. here, we report on our ongoing research to adequately provide such capabilities to the classic car ecosystem. using a design science research approach, we have developed a blockchain-based solution using hyperledger fabric that facilitates the proper recording of classic car information, restoration procedures applied, and all related documentation by ensuring that this data is immutable and trustworthy while promoting collaboration between interested parties. this solution was validated and received positive feedback from various entities in the classic car sector. the enhanced and secured documentation is expected to contribute to the digital transformation of the classic car sector, promote authenticity and trustworthiness, and ultimately increase the market value of classic cars.",2024-03-12
"23","feasibility of machine learning-based rice yield prediction in india at the district level using climate reanalysis data","djavan de clercq, adam mahdi","machine learning","yield forecasting, the science of predicting agricultural productivity before the crop harvest occurs, helps a wide range of stakeholders make better decisions around agricultural planning. this study aims to investigate whether machine learning-based yield prediction models can capably predict kharif season rice yields at the district level in india several months before the rice harvest takes place. the methodology involved training 19 machine learning models such as catboost, lightgbm, orthogonal matching pursuit, and extremely randomized trees on 20 years of climate, satellite, and rice yield data across 247 of indian rice-producing districts. in addition to model-building, a dynamic dashboard was built understand how the reliability of rice yield predictions varies across districts. the results of the proof-of-concept machine learning pipeline demonstrated that rice yields can be predicted with a reasonable degree of accuracy, with out-of-sample r2, mae, and mape performance of up to 0.82, 0.29, and 0.16 respectively. these results outperformed test set performance reported in related literature on rice yield modeling in other contexts and countries. in addition, shap value analysis was conducted to infer both the importance and directional impact of the climate and remote sensing variables included in the model. important features driving rice yields included temperature, soil water volume, and leaf area index. in particular, higher temperatures in august correlate with increased rice yields, particularly when the leaf area index in august is also high. building on the results, a proof-of-concept dashboard was developed to allow users to easily explore which districts may experience a rise or fall in yield relative to the previous year.",2024-03-12
"24","applying ranking techniques for estimating influence of earth variables on temperature forecast error","m. julia flores, melissa ruiz-vásquez, ana bastos, rené orth","machine learning","this paper describes how to analyze the influence of earth system variables on the errors when providing temperature forecasts. the initial framework to get the data has been based on previous research work, which resulted in a very interesting discovery. however, the aforementioned study only worked on individual correlations of the variables with respect to the error. this research work is going to re-use the main ideas but introduce three main novelties: (1) applying a data science approach by a few representative locations; (2) taking advantage of the rankings created by spearman correlation but enriching them with other metrics looking for a more robust ranking of the variables; (3) evaluation of the methodology by learning random forest models for regression with the distinct experimental variations. the main contribution is the framework that shows how to convert correlations into rankings and combine them into an aggregate ranking. we have carried out experiments on five chosen locations to analyze the behavior of this ranking-based methodology. the results show that the specific performance is dependent on the location and season, which is expected, and that this selection technique works properly with random forest models but can also improve simpler regression models such as bayesian ridge. this work also contributes with an extensive analysis of the results. we can conclude that this selection based on the top-k ranked variables seems promising for this real problem, and it could also be applied in other domains.",2024-03-12
"25","standing on furm ground -- a framework for evaluating fair, useful, and reliable ai models in healthcare systems","alison callahan, duncan mcelfresh, juan m. banda, gabrielle bunney, danton char, jonathan chen, conor k. corbin, debadutta dash, norman l. downing, srikar nallan, sneha s. jain, nikesh kotecha, jonathan masterson, michelle m. mello, keith morse, abby pandya, anurang revri, aditya sharma, christopher sharp, rahul thapa, michael wornow, alaa youssef, michael a. pfeffer, nigam h. shah","computers and society","the impact of using artificial intelligence (ai) to guide patient care or operational processes is an interplay of the ai model's output, the decision-making protocol based on that output, and the capacity of the stakeholders involved to take the necessary subsequent action. estimating the effects of this interplay before deployment, and studying it in real time afterwards, are essential to bridge the chasm between ai model development and achievable benefit. to accomplish this, the data science team at stanford health care has developed a mechanism to identify fair, useful and reliable ai models (furm) by conducting an ethical review to identify potential value mismatches, simulations to estimate usefulness, financial projections to assess sustainability, as well as analyses to determine it feasibility, design a deployment strategy, and recommend a prospective monitoring and evaluation plan. we report on furm assessments done to evaluate six ai guided solutions for potential adoption, spanning clinical and operational settings, each with the potential to impact from several dozen to tens of thousands of patients each year. we describe the assessment process, summarize the six assessments, and share our framework to enable others to conduct similar assessments. of the six solutions we assessed, two have moved into a planning and implementation phase. our novel contributions - usefulness estimates by simulation, financial projections to quantify sustainability, and a process to do ethical assessments - as well as their underlying methods and open source tools, are available for other healthcare systems to conduct actionable evaluations of candidate ai solutions.",2024-02-27
"26","low coordinate degree algorithms i: universality of computational thresholds for hypothesis testing","dmitriy kunisky","statistics theory","we study when low coordinate degree functions (lcdf) -- linear combinations of functions depending on small subsets of entries of a vector -- can hypothesis test between high-dimensional probability measures. these functions are a generalization, proposed in hopkins' 2018 thesis but seldom studied since, of low degree polynomials (ldp), a class widely used in recent literature as a proxy for all efficient algorithms for tasks in statistics and optimization. instead of the orthogonal polynomial decompositions used in ldp calculations, our analysis of lcdf is based on the efron-stein or anova decomposition, making it much more broadly applicable. by way of illustration, we prove channel universality for the success of lcdf in testing for the presence of sufficiently ""dilute"" random signals through noisy channels: the efficacy of lcdf depends on the channel only through the scalar fisher information for a class of channels including nearly arbitrary additive i.i.d. noise and nearly arbitrary exponential families. as applications, we extend lower bounds against ldp for spiked matrix and tensor models under additive gaussian noise to lower bounds against lcdf under general noisy channels. we also give a simple and unified treatment of the effect of censoring models by erasing observations at random and of quantizing models by taking the sign of the observations. these results are the first computational lower bounds against any large class of algorithms for all of these models when the channel is not one of a few special cases, and thereby give the first substantial evidence for the universality of several statistical-to-computational gaps.",2024-03-12
"27","simplified tight bounds for monotone minimal perfect hashing","dmitry kosolobov","data structures and algorithms","given an increasing sequence of integers $x_1,\ldots,x_n$ from a universe $\{0,\ldots,u-1\}$, the monotone minimal perfect hash function (mmphf) for this sequence is a data structure that answers the following rank queries: $rank(x) = i$ if $x = x_i$, for $i\in \{1,\ldots,n\}$, and $rank(x)$ is arbitrary otherwise. assadi, farach-colton, and kuszmaul recently presented at soda'23 a proof of the lower bound $\omega(n \min\{\log\log\log u, \log n\})$ for the bits of space required by mmphf, provided $u \ge n 2^{2^{\sqrt{\log\log n}}}$, which is tight since there is a data structure for mmphf that attains this space bound (and answers the queries in $o(\log u)$ time). in this paper, we close the remaining gap by proving that, for $u \ge (1+\epsilon)n$, where $\epsilon > 0$ is any constant, the tight lower bound is $\omega(n \min\{\log\log\log \frac{u}{n}, \log n\})$, which is also attainable; we observe that, for all reasonable cases when $n < u < (1+\epsilon)n$, known facts imply tight bounds, which virtually settles the problem. along the way we substantially simplify the proof of assadi et al. replacing a part of their heavy combinatorial machinery by trivial observations. however, an important part of the proof still remains complicated. this part of our paper repeats arguments of assadi et al. and is not novel. nevertheless, we include it, for completeness, offering a somewhat different perspective on these arguments.",2024-03-12
"28","the minimax rate of hsic estimation for translation-invariant kernels","florian kalinke, zoltan szabo","statistics theory","kernel techniques are among the most influential approaches in data science and statistics. under mild conditions, the reproducing kernel hilbert space associated to a kernel is capable of encoding the independence of $m\ge 2$ random variables. probably the most widespread independence measure relying on kernels is the so-called hilbert-schmidt independence criterion (hsic; also referred to as distance covariance in the statistics literature). despite various existing hsic estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which hsic can be estimated is still open. in this work, we prove that the minimax optimal rate of hsic estimation on $\mathbb r^d$ for borel measures containing the gaussians with continuous bounded translation-invariant characteristic kernels is $\mathcal o\!\left(n^{-1/2}\right)$. specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the u-statistic, the v-statistic, and the nyström-based one) on $\mathbb r^d$.",2024-03-12
"29","propml: probability partial multi-label learning","łukasz struski, adam pardyl, jacek tabor, bartosz zieliński","machine learning","partial multi-label learning (pml) is a type of weakly supervised learning where each training instance corresponds to a set of candidate labels, among which only some are true. in this paper, we introduce \our{}, a novel probabilistic approach to this problem that extends the binary cross entropy to the pml setup. in contrast to existing methods, it does not require suboptimal disambiguation and, as such, can be applied to any deep architecture. furthermore, experiments conducted on artificial and real-world datasets indicate that \our{} outperforms existing approaches, especially for high noise in a candidate set.",2024-03-12
"30","maximum defective clique computation: improved time complexities and practical performance","lijun chang","data structures and algorithms","the concept of $k$-defective clique, a relaxation of clique by allowing up-to $k$ missing edges, has been receiving increasing interests recently. although the problem of finding the maximum $k$-defective clique is np-hard, several practical algorithms have been recently proposed in the literature, with kdc being the state of the art. kdc not only runs the fastest in practice, but also achieves the best time complexity. specifically, it runs in $o^*(\gamma_k^n)$ time when ignoring polynomial factors; here, $\gamma_k$ is a constant that is smaller than two and only depends on $k$, and $n$ is the number of vertices in the input graph $g$. in this paper, we propose the kdc-two algorithm to improve the time complexity as well as practical performance. kdc-two runs in $o^*( (\alpha\delta)^{k+2} \gamma_{k-1}^\alpha)$ time when the maximum $k$-defective clique size $\omega_k(g)$ is at least $k+2$, and in $o^*(\gamma_{k-1}^n)$ time otherwise, where $\alpha$ and $\delta$ are the degeneracy and maximum degree of $g$, respectively. in addition, with slight modification, kdc-two also runs in $o^*( (\alpha\delta)^{k+2} (k+1)^{\alpha+k+1-\omega_k(g)})$ time by using the degeneracy gap $\alpha+k+1-\omega_k(g)$ parameterization; this is better than $o^*( (\alpha\delta)^{k+2}\gamma_{k-1}^\alpha)$ when $\omega_k(g)$ is close to the degeneracy-based upper bound $\alpha+k+1$. finally, to further improve the practical performance, we propose a new degree-sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with those proposed in the literature. extensive empirical studies on three benchmark graph collections show that our algorithm outperforms the existing fastest algorithm by several orders of magnitude.",2024-03-12
"31","controlling delegations in liquid democracy","shiri alouf-heffetz, tanmay inamdar, pallavi jain, yash more, nimrod talmon","computer science and game theory","in liquid democracy, agents can either vote directly or delegate their vote to a different agent of their choice. this results in a power structure in which certain agents possess more voting weight than others. as a result, it opens up certain possibilities of vote manipulation, including control and bribery, that do not exist in standard voting scenarios of direct democracy. here we formalize a certain kind of election control -- in which an external agent may change certain delegation arcs -- and study the computational complexity of the corresponding combinatorial problem.",2024-03-12
"32","shining light on periodic dominating sets in bounded-treewidth graphs","jakob greilhuber, philipp schepper, philip wellnitz","data structures and algorithms","for the vertex selection problem $(\sigma,\rho)$-domset one is given two fixed sets $\sigma$ and $\rho$ of integers and the task is to decide whether we can select vertices of the input graph, such that, for every selected vertex, the number of selected neighbors is in $\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\rho$. this framework covers independent set and dominating set for example. we investigate the case when $\sigma$ and $\rho$ are periodic sets with the same period $m\ge 2$, that is, the sets are two (potentially different) residue classes modulo $m$. we study the problem parameterized by treewidth and present an algorithm that solves in time $m^{tw} \cdot n^{o(1)}$ the decision, minimization and maximization version of the problem. this significantly improves upon the known algorithms where for the case $m \ge 3$ not even an explicit running time is known. we complement our algorithm by providing matching lower bounds which state that there is no $(m-\epsilon)^{pw} \cdot n^{o(1)}$ unless seth fails. for $m = 2$, we extend these bound to the minimization version as the decision version is efficiently solvable.",2024-03-12
"33","signed graphs in data sciences via communicability geometry","fernando diaz-diaz, ernesto estrada","metric geometry","signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. these include data from biological, ecological, and social systems. here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are euclidean and spherical. we then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. they include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.",2024-03-12
"34","automated discovery of anomalous features in ultra-large planetary remote sensing datasets using variational autoencoders","adam lesnikowski, valentin t. bickel, daniel angerhausen","earth and planetary astrophysics","the nasa lunar reconnaissance orbiter (lro) has returned petabytes of lunar high spatial resolution surface imagery over the past decade, impractical for humans to fully review manually. here we develop an automated method using a deep generative visual model that rapidly retrieves scientifically interesting examples of lro surface imagery representing the first planetary image anomaly detector. we give quantitative experimental evidence that our method preferentially retrieves anomalous samples such as notable geological features and known human landing and spacecraft crash sites. our method addresses a major capability gap in planetary science and presents a novel way to unlock insights hidden in ever-increasing remote sensing data archives, with numerous applications to other science domains. we publish our code and data along with this paper.",2024-03-12
"35","learning-augmented algorithms with explicit predictors","marek elias, haim kaplan, yishay mansour, shay moran","machine learning","recent advances in algorithmic design show how to utilize predictions obtained by machine learning models from past and present data. these approaches have demonstrated an enhancement in performance when the predictions are accurate, while also ensuring robustness by providing worst-case guarantees when predictions fail. in this paper we focus on online problems; prior research in this context was focused on a paradigm where the predictor is pre-trained on past data and then used as a black box (to get the predictions it was trained for). in contrast, in this work, we unpack the predictor and integrate the learning problem it gives rise for within the algorithmic challenge. in particular we allow the predictor to learn as it receives larger parts of the input, with the ultimate goal of designing online learning algorithms specifically tailored for the algorithmic task at hand. adopting this perspective, we focus on a number of fundamental problems, including caching and scheduling, which have been well-studied in the black-box setting. for each of the problems we consider, we introduce new algorithms that take advantage of explicit learning algorithms which we carefully design towards optimizing the overall performance. we demonstrate the potential of our approach by deriving performance bounds which improve over those established in previous work.",2024-03-12
"36","satisfiability to coverage in presence of fairness, matroid, and global constraints","tanmay inamdar, pallavi jain, daniel lokshtanov, abhishek sahu, saket saurabh, anannya upasana","data structures and algorithms","in maxsat with cardinality constraint problem (cc-maxsat), we are given a cnf-formula $\phi$, and $k \ge 0$, and the goal is to find an assignment $\beta$ with at most $k$ variables set to true (also called a weight $k$-assignment) such that the number of clauses satisfied by $\beta$ is maximized. maxcov can be seen as a special case of cc-maxsat, where the formula $\phi$ is monotone, i.e., does not contain any negative literals. cc-maxsat and maxcov are extremely well-studied problems in the approximation algorithms as well as parameterized complexity literature. our first contribution is that the two problems are equivalent to each other in the context of fpt-approximation parameterized by $k$ (approximation is in terms of number of clauses satisfied/elements covered). we give a randomized reduction from cc-maxsat to maxcov in time $o(1/\epsilon)^{k} \cdot (m+n)^{o(1)}$ that preserves the approximation guarantee up to a factor of $1-\epsilon$. furthermore, this reduction also works in the presence of fairness and matroid constraints. armed with this reduction, we focus on designing fpt-approximation schemes (fpt-ases) for maxcov and its generalizations. our algorithms are based on a novel combination of a variety of ideas, including a carefully designed probability distribution that exploits sparse coverage functions. these algorithms substantially generalize the results in jain et al. [soda 2023] for cc-maxsat and maxcov for $k_{d,d}$-free set systems (i.e., no $d$ sets share $d$ elements), as well as a recent fpt-as for matroid-constrained maxcov by sellier [esa 2023] for frequency-$d$ set systems.",2024-03-12
"37","the primal pathwidth seth","michael lampis","computational complexity","motivated by the importance of dynamic programming (dp) in parameterized complexity, we consider several fine-grained questions, such as the following examples: (i) can dominating set be solved in time $(3-\epsilon)^{pw}n^{o(1)}$? (where $pw$ is the pathwidth) (ii) can coloring be solved in time $pw^{(1-\epsilon)pw}n^{o(1)}$? (iii) can a short reconfiguration between two size-$k$ independent sets be found in time $n^{(1-\epsilon)k}$? such questions are well-studied: in some cases the answer is no under the seth, while in others coarse-grained lower bounds are known under the eth. even though questions such as the above seem ""morally equivalent"" as they all ask if a simple dp can be improved, the problems concerned have wildly varying time complexities, ranging from single-exponential fpt to xnlp-complete. this paper's main contribution is to show that, despite their varying complexities, these questions are not just morally equivalent, but in fact they are the same question in disguise. we achieve this by putting forth a natural complexity assumption which we call the primal pathwidth-strong exponential time hypothesis (pp-seth) and which states that 3-sat cannot be solved in time $(2-\epsilon)^{pw}n^{o(1)}$, for any $\epsilon>0$, where $pw$ is the pathwidth of the primal graph of the input. we then show that numerous fine-grained questions in parameterized complexity, including the ones above, are equivalent to the pp-seth, and hence to each other. this allows us to obtain sharp fine-grained lower bounds for problems for which previous lower bounds left a constant in the exponent undetermined, but also to increase our confidence in bounds which were previously known under the seth, because we show that breaking any one such bound requires breaking all (old and new) bounds; and because we show that the pp-seth is more plausible than the seth.",2024-03-12
"38","noisy computing of the threshold function","ziao wang, nadim ghaddar, banghua zhu, lele wang","data structures and algorithms","let $\mathsf{th}_k$ denote the $k$-out-of-$n$ threshold function: given $n$ input boolean variables, the output is $1$ if and only if at least $k$ of the inputs are $1$. we consider the problem of computing the $\mathsf{th}_k$ function using noisy readings of the boolean variables, where each reading is incorrect with some fixed and known probability $p \in (0,1/2)$. as our main result, we show that, when $k = o(n)$, it is both sufficient and necessary to use $$(1 \pm o(1)) \frac{n\log \frac{k}{\delta}}{d_{\mathsf{kl}}(p || 1-p)}$$ queries in expectation to compute the $\mathsf{th}_k$ function with a vanishing error probability $\delta = o(1)$, where $d_{\mathsf{kl}}(p || 1-p)$ denotes the kullback-leibler divergence between $\mathsf{bern}(p)$ and $\mathsf{bern}(1-p)$ distributions. in particular, this says that $(1 \pm o(1)) \frac{n\log \frac{1}{\delta}}{d_{\mathsf{kl}}(p || 1-p)}$ queries in expectation are both sufficient and necessary to compute the $\mathsf{or}$ and $\mathsf{and}$ functions of $n$ boolean variables. compared to previous work, our result tightens the dependence on $p$ in both the upper and lower bounds.",2024-03-12
"39","improving deep learning with prior knowledge and cognitive models: a survey on enhancing explainability, adversarial robustness and zero-shot learning","fuseinin mumuni, alhassan mumuni","machine learning","we review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, explainable artificial intelligence (xai), and zero-shot or few-short learning. data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. in particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. furthermore, models are usually trained on standard datasets with a closed-world assumption. hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.",2024-03-11
"40","imaging of i zw 18 by jwst: i. strategy and first results of dusty stellar populations","alec s. hirschauer, nicolas crouzet, nolan habel, laura lenkić, conor nally, olivia c. jones, giacomo bortolini, martha l. boyer, kay justtanont margaret meixner, göran östlin, gillian s. wright, ruyman azzollini, joris a. d. l. blommaert, bernhard brandl, leen decin, omnarayani nayak, pierre royer, b. a. sargent, paul van der werf","astrophysics of galaxies","we present a james webb space telescope (jwst) imaging survey of i zw 18, the archetypal extremely metal-poor, star-forming, blue compact dwarf galaxy. with an oxygen abundance of only $\sim$3% $z_{\odot}$, it is among the lowest-metallicity systems known in the local universe, and is, therefore, an excellent accessible analog for the galactic building blocks which existed at early epochs of ionization and star formation. these jwst data provide a comprehensive infrared (ir) view of i zw 18 with eight filters utilizing both nircam (f115w, f200w, f356w, and f444w) and miri (f770w, f1000w, f1500w, and f1800w) photometry, which we have used to identify key stellar populations that are bright in the near- and mid-ir. these data allow for a better understanding of the origins of dust and dust-production mechanisms in metal-poor environments by characterizing the population of massive, evolved stars in the red supergiant (rsg) and asymptotic giant branch (agb) phases. in addition, it enables the identification of the brightest dust-enshrouded young stellar objects (ysos), which provide insight into the formation of massive stars at extremely low metallicities typical of the very early universe. this paper provides an overview of the observational strategy and data processing, and presents first science results, including identifications of dusty agb star, rsg, and bright yso candidates. these first results assess the scientific quality of jwst data and provide a guide for obtaining and interpreting future observations of the dusty and evolved stars inhabiting compact dwarf star-forming galaxies in the local universe.",2024-03-11
"41","can llms separate instructions from data? and what do we even mean by that?","egor zverev, sahar abdelnabi, mario fritz, christoph h. lampert","machine learning","instruction-tuned large language models (llms) have achieved breakthrough results, opening countless new possibilities for many practical applications. however, llms lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. in this work, we aim to close this gap. we introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. we also introduce a new dataset, sep (should it be executed or processed?), which allows estimating the measure, and we report results on several state-of-the-art open-source and closed llms. finally, we quantitatively demonstrate that all evaluated llms fail to achieve a high amount of separation, according to our measure. the source code and sep dataset are openly accessible at this https url.",2024-03-11
"42","smc-last extracted photometry","t. a. kuchar, g. c. sloan, d. r. mizuno, kathleen e. kraemer, m. l. boyer, martin a. t. groenewegen, o. c. jones, f. kemper, iain mcdonald, joana m. oliveira, marta sewiło, sundar srinivasan, jacco th. van loon, albert zijlstra","astrophysics of galaxies","we present point-source photometry from the spitzer space telescope's final survey of the small magellanic cloud (smc). we mapped 30 square degrees in two epochs in 2017, with the second extending to early 2018 at 3.6 and 4.5 microns using the infrared array camera. this survey duplicates the footprint from the sage-smc program in 2008. together, these surveys cover a nearly 10 yr temporal baseline in the smc. we performed aperture photometry on the mosaicked maps produced from the new data. we did not use any prior catalogs as inputs for the extractor in order to be sensitive to any moving objects (e.g., foreground brown dwarfs) and other transient phenomena (e.g., cataclysmic variables or fu ori-type eruptions). we produced a point-source catalog with high-confidence sources for each epoch as well as combined-epoch catalog. for each epoch and the combined-epoch data, we also produced a more complete archive with lower-confidence sources. all of these data products will be available to the community at the infrared science archive.",2024-03-11
"43","societal and scientific impact of policy research: a large-scale empirical study of some explanatory factors using altmetric and overton","pablo dorta-gonzález, alejandro rodríguez-caro, maría isabel dorta-gonzález","digital libraries","this study investigates how scientific research influences policymaking by analyzing citations of research articles in policy documents (policy impact) for nearly 125,000 articles across 434 public policy journals. we reveal distinct citation patterns between policymakers and other stakeholders like researchers, journalists, and the public. news and blog mentions, social media engagement, and open access publications (excluding fully open access) significantly increase the likelihood of a research article being cited in policy documents. conversely, articles locked behind paywalls and those published under the full open access model (based on altmetric data) have a lower chance of being policy-cited. publication year and policy type show no significant influence. our findings emphasize the crucial role of science communication channels like news media and social media in bridging the gap between research and policy. interestingly, academic citations hold a weaker influence on policy citations compared to news mentions, suggesting a potential disconnect between how researchers reference research and how policymakers utilize it. this highlights the need for improved communication strategies to ensure research informs policy decisions more effectively. this study provides valuable insights for researchers, policymakers, and science communicators. researchers can tailor their dissemination efforts to reach policymakers through media channels. policymakers can leverage these findings to identify research with higher policy relevance. science communicators can play a critical role in translating research for policymakers and fostering dialogue between the scientific and policymaking communities.",2024-03-11
"44","approximating maximum edge 2-coloring by normalizing graphs","tobias mömke, alexandru popa, aida roshany-tabrizi, michael ruderer, roland vincze","discrete mathematics","in a simple, undirected graph g, an edge 2-coloring is a coloring of the edges such that no vertex is incident to edges with more than 2 distinct colors. the problem maximum edge 2-coloring (me2c) is to find an edge 2-coloring in a graph g with the goal to maximize the number of colors. for a relevant graph class, me2c models anti-ramsey numbers and it was considered in network applications. for the problem a 2-approximation algorithm is known, and if the input graph has a perfect matching, the same algorithm has been shown to have a performance guarantee of 5/3. it is known that me2c is apx-hard and that it is ug-hard to obtain an approximation ratio better than 1.5. we show that if the input graph has a perfect matching, there is a polynomial time 1.625-approximation and if the graph is claw-free or if the maximum degree of the input graph is at most three (i.e., the graph is subcubic), there is a polynomial time 1.5-approximation algorithm for me2c",2024-03-11
"45","untangling gaussian mixtures","eva fluck, sandra kiefer, christoph standke","statistics theory","tangles were originally introduced as a concept to formalize regions of high connectivity in graphs. in recent years, they have also been discovered as a link between structural graph theory and data science: when interpreting similarity in data sets as connectivity between points, finding clusters in the data essentially amounts to finding tangles in the underlying graphs. this paper further explores the potential of tangles in data sets as a means for a formal study of clusters. real-world data often follow a normal distribution. accounting for this, we develop a quantitative theory of tangles in data sets drawn from gaussian mixtures. to this end, we equip the data with a graph structure that models similarity between the points and allows us to apply tangle theory to the data. we provide explicit conditions under which tangles associated with the marginal gaussian distributions exist asymptotically almost surely. this can be considered as a sufficient formal criterion for the separabability of clusters in the data.",2024-03-11
"46","optimal bounds for distinct quartics","panagiotis charalampopoulos, paweł gawrychowski, samah ghazawi","data structures and algorithms","a fundamental concept related to strings is that of repetitions. it has been extensively studied in many versions, from both purely combinatorial and algorithmic angles. one of the most basic questions is how many distinct squares, i.e., distinct strings of the form $uu$, a string of length $n$ can contain as fragments. it turns out that this is always $\mathcal{o}(n)$, and the bound cannot be improved to sublinear in $n$ [fraenkel and simpson, jcta 1998]. several similar questions about repetitions in strings have been considered, and by now we seem to have a good understanding of their repetitive structure. for higher-dimensional strings, the basic concept of periodicity has been successfully extended and applied to design efficient algorithms -- it is inherently more complex than for regular strings. extending the notion of repetitions and understanding the repetitive structure of higher-dimensional strings is however far from complete. quartics were introduced by apostolico and brimkov [tcs 2000] as analogues of squares in two dimensions. charalampopoulos, radoszewski, rytter, waleń, and zuba [esa 2020] proved that the number of distinct quartics in an $n\times n$ 2d string is $\mathcal{o}(n^2 \log^2 n)$ and that they can be computed in $\mathcal{o}(n^2 \log^2 n)$ time. gawrychowski, ghazawi, and landau [spire 2021] constructed an infinite family of $n \times n$ 2d strings with $\omega(n^2 \log n)$ distinct quartics. this brings the challenge of determining asymptotically tight bounds. here, we settle both the combinatorial and the algorithmic aspects of this question: the number of distinct quartics in an $n\times n$ 2d string is $\mathcal{o}(n^2 \log n)$ and they can be computed in the worst-case optimal $\mathcal{o}(n^2 \log n)$ time.",2024-03-11
"47","balanced substructures in bicolored graphs","p. s. ardra, r. krithika, saket saurabh, roohani sharma","data structures and algorithms","an edge-colored graph is said to be balanced if it has an equal number of edges of each color. given a graph $g$ whose edges are colored using two colors and a positive integer $k$, the objective in the edge balanced connected subgraph problem is to determine if $g$ has a balanced connected subgraph containing at least $k$ edges. we first show that this problem is np-complete and remains so even if the solution is required to be a tree or a path. then, we focus on the parameterized complexity of edge balanced connected subgraph and its variants (where the balanced subgraph is required to be a path/tree) with respect to $k$ as the parameter. towards this, we show that if a graph has a balanced connected subgraph/tree/path of size at least $k$, then it has one of size at least $k$ and at most $f(k)$ where $f$ is a linear function. we use this result combined with dynamic programming algorithms based on color coding and representative sets to show that edge balanced connected subgraph and its variants are fpt. further, using polynomial-time reductions to the multilinear monomial detection problem, we give faster randomized fpt algorithms for the problems. in order to describe these reductions, we define a combinatorial object called relaxed-subgraph. we define this object in such a way that balanced connected subgraphs, trees and paths are relaxed-subgraphs with certain properties. this object is defined in the spirit of branching walks known for the steiner tree problem and may be of independent interest.",2024-03-11
"48","arborescences and shortest path trees when colors matter","p. s. ardra, jasine babu, kritika kashyap, r. krithika, sreejith k. pallathumadam, deepak rajendraprasad","data structures and algorithms","color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) graph and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color. some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions. in this work, we study color-constrained arborescences and shortest path trees. computing a color-constrained shortest path tree on weighted digraphs turns out to be np-hard in general but polynomial-time solvable when all cycles have positive weight. this polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original graph. while finding color-constrained arborescence of digraphs is np-hard in general, we give efficient algorithms when the input graph is acyclic. consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed. our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight. en route, we sight nice connections to colored matroids and color-constrained bases.",2024-03-11
"49","an algorithm for correct computation of reeb spaces for pl bivariate fields","amit chattopadhyay, yashwanth ramamurthi, osamu saeki","computational geometry","the reeb space is a topological structure which is a generalization of the notion of the reeb graph to multi-fields. its effectiveness has been established in revealing topological features in data across diverse computational domains which cannot be identified using the reeb graph or other scalar-topology-based methods. approximations of reeb spaces such as the mapper and the joint contour net have been developed based on quantization of the range. however, computing the topologically correct reeb space dispensing the range-quantization is a challenging problem. in the current paper, we develop an algorithm for computing a correct net-like approximation corresponding to the reeb space of a generic piecewise-linear (pl) bivariate field based on a multi-dimensional reeb graph (mdrg). first, we prove that the reeb space is homeomorphic to its mdrg. subsequently, we introduce an algorithm for computing the mdrg of a generic pl bivariate field through the computation of its jacobi set and jacobi structure, a projection of the jacobi set into the reeb space. this marks the first algorithm for mdrg computation without requiring the quantization of bivariate fields. following this, we compute a net-like structure embedded in the corresponding reeb space using the mdrg and the jacobi structure. we provide the proof of correctness and complexity analysis of our algorithm.",2024-03-11
"50","fun maximizing search, (non) instance optimality, and video games for parrots","jérémy barbay","data structures and algorithms","computerized adaptive testing (cat) measures an examinee's ability while adapting to their level. both too many questions and too many hard questions can make a test frustrating. are there some cat algorithms which can be proven to be theoretically better than others, and in which framework? we show that slightly extending the traditional framework yields a partial order on cat algorithms. for uni-dimensional knowledge domains, we analyze the theoretical performance of some old and new algorithms, and we prove that none of the algorithms presented are instance optimal, conjecturing that no instance optimal can exist for the cat problem.",2024-03-11
"51","feataug: automatic feature augmentation from one-to-many relationship tables","danrui qi, weiling zheng, jiannan wang","machine learning","feature augmentation from one-to-many relationship tables is a critical but challenging problem in ml model development. to augment good features, data scientists need to come up with sql queries manually, which is time-consuming. featuretools [1] is a widely used tool by the data science community to automatically augment the training data by extracting new features from relevant tables. it represents each feature as a group-by aggregation sql query on relevant tables and can automatically generate these sql queries. however, it does not include predicates in these queries, which significantly limits its application in many real-world scenarios. to overcome this limitation, we propose feataug, a new feature augmentation framework that automatically extracts predicate-aware sql queries from one-to-many relationship tables. this extension is not trivial because considering predicates will exponentially increase the number of candidate queries. as a result, the original featuretools framework, which materializes all candidate queries, will not work and needs to be redesigned. we formally define the problem and model it as a hyperparameter optimization problem. we discuss how the bayesian optimization can be applied here and propose a novel warm-up strategy to optimize it. to make our algorithm more practical, we also study how to identify promising attribute combinations for predicates. we show that how the beam search idea can partially solve the problem and propose several techniques to further optimize it. our experiments on four real-world datasets demonstrate that feataug extracts more effective features compared to featuretools and other baselines. the code is open-sourced at this https url",2024-03-11
"52","accelerating sparse tensor decomposition using adaptive linearized representation","jan laukemann, ahmed e. helal, s. isaac geronimo anderson, fabio checconi, yongseok soh, jesmin jahan tithi, teresa ranadive, brian j gravelle, fabrizio petrini, jee choi","distributed, parallel, and cluster computing","high-dimensional sparse data emerge in many critical application domains such as cybersecurity, healthcare, anomaly detection, and trend analysis. to quickly extract meaningful insights from massive volumes of these multi-dimensional data, scientists employ unsupervised analysis tools based on tensor decomposition (td) methods. however, real-world sparse tensors exhibit highly irregular shapes, data distributions, and sparsity, which pose significant challenges for making efficient use of modern parallel architectures. this study breaks the prevailing assumption that compressing sparse tensors into coarse-grained structures (i.e., tensor slices or blocks) or along a particular dimension/mode (i.e., mode-specific) is more efficient than keeping them in a fine-grained, mode-agnostic form. our novel sparse tensor representation, adaptive linearized tensor order (alto), encodes tensors in a compact format that can be easily streamed from memory and is amenable to both caching and parallel execution. to demonstrate the efficacy of alto, we accelerate popular td methods that compute the canonical polyadic decomposition (cpd) model across a range of real-world sparse tensors. additionally, we characterize the major execution bottlenecks of td methods on multiple generations of the latest intel xeon scalable processors, including sapphire rapids cpus, and introduce dynamic adaptation heuristics to automatically select the best algorithm based on the sparse tensor characteristics. across a diverse set of real-world data sets, alto outperforms the state-of-the-art approaches, achieving more than an order-of-magnitude speedup over the best mode-agnostic formats. compared to the best mode-specific formats, which require multiple tensor copies, alto achieves more than 5.1x geometric mean speedup at a fraction (25%) of their storage.",2024-03-11
"53","improved fpt approximation scheme and approximate kernel for biclique-free max k-weight sat: greedy strikes back","pasin manurangsi","data structures and algorithms","in the max $k$-weight sat (aka max sat with cardinality constraint) problem, we are given a cnf formula with $n$ variables and $m$ clauses together with a positive integer $k$. the goal is to find an assignment where at most $k$ variables are set to one that satisfies as many constraints as possible. recently, jain et al. [soda'23] gave an fpt approximation scheme (fpt-as) with running time $2^{o\left(\left(dk/\epsilon\right)^d\right)} \cdot (n + m)^{o(1)}$ for max $k$-weight sat when the incidence graph is $k_{d,d}$-free. they asked whether a polynomial-size approximate kernel exists. in this work, we answer this question positively by giving an $(1 - \epsilon)$-approximate kernel with $\left(\frac{d k}{\epsilon}\right)^{o(d)}$ variables. this also implies an improved fpt-as with running time $(dk/\epsilon)^{o(dk)} \cdot (n + m)^{o(1)}$. our approximate kernel is based mainly on a couple of greedy strategies together with a sunflower lemma-style reduction rule.",2024-03-10
"54","revisiting path contraction and cycle contraction","r. krithika, v. k. kutty malu, prafullkumar tale","data structures and algorithms","the path contraction and cycle contraction problems take as input an undirected graph $g$ with $n$ vertices, $m$ edges and an integer $k$ and determine whether one can obtain a path or a cycle, respectively, by performing at most $k$ edge contractions in $g$. we revisit these np-complete problems and prove the following results. path contraction admits an algorithm running in $\mathcal{o}^*(2^{k})$ time. this improves over the current algorithm known for the problem [algorithmica 2014]. cycle contraction admits an algorithm running in $\mathcal{o}^*((2 + \epsilon_{\ell})^k)$ time where $0 < \epsilon_{\ell} \leq 0.5509$ is inversely proportional to $\ell = n - k$. central to these results is an algorithm for a general variant of path contraction, namely, path contraction with constrained ends. we also give an $\mathcal{o}^*(2.5191^n)$-time algorithm to solve the optimization version of cycle contraction. next, we turn our attention to restricted graph classes and show the following results. path contraction on planar graphs admits a polynomial-time algorithm. path contraction on chordal graphs does not admit an algorithm running in time $\mathcal{o}(n^{2-\epsilon} \cdot 2^{o(tw)})$ for any $\epsilon > 0$, unless the orthogonal vectors conjecture fails. here, $tw$ is the treewidth of the input graph. the second result complements the $\mathcal{o}(nm)$-time, i.e., $\mathcal{o}(n^2 \cdot tw)$-time, algorithm known for the problem [discret. appl. math. 2014].",2024-03-10
"55","aqueous solution chemistry in silico and the role of data driven approaches","debarshi banerjee, khatereh azizi, colin k. egan, edward danquah donkor, cesare malosso, solana di pino, gonzalo diaz miron, martina stella, giulia sormani, germaine neza hozana, marta monti, uriel n. morzan, alex rodriguez, giuseppe cassone, asja jelic, damian scherlis, ali hassanali","chemical physics","the use of computer simulations to study the properties of aqueous systems is, today more than ever, an active area of research. in this context, during the last decade there has been a tremendous growth in the use of data-driven approaches to develop more accurate potentials for water as well as to characterize its complexity in chemical and biological contexts. we highlight the progress, giving a historical context, on the path to the development of many-body and reactive potentials to model aqueous chemistry, including the role of machine learning strategies. we focus specifically on conceptual and methodological challenges along the way in performing simulations that seek to tackle problems in modeling the chemistry of aqueous solutions. in conclusion, we summarize our perspectives on the use and integration of advanced data-science techniques to provide chemical insights in physical chemistry and how this will influence computer simulations of aqueous systems in the future.",2024-03-10
"56","steering a fleet: adaptation for large-scale, workflow-based experiments","jim pruyne, valerie hayot-sasson, weijian zheng, ryan chard, justin m. wozniak, tekin bicer, kyle chard, ian t. foster","distributed, parallel, and cluster computing","experimental science is increasingly driven by instruments that produce vast volumes of data and thus a need to manage, compute, describe, and index this data. high performance and distributed computing provide the means of addressing the computing needs; however, in practice, the variety of actions required and the distributed set of resources involved, requires sophisticated ""flows"" defining the steps to be performed on data. as each scan or measurement is performed by an instrument, a new instance of the flow is initiated resulting in a ""fleet"" of concurrently running flows, with the overall goal to process all the data collected during a potentially long-running experiment. during the course of the experiment, each flow may need to adapt its execution due to changes in the environment, such as computational or storage resource availability, or based on the progress of the fleet as a whole such as completion or discovery of an intermediate result leading to a change in subsequent flow's behavior. we introduce a cloud-based decision engine, braid, which flows consult during execution to query their run-time environment and coordinate with other flows within their fleet. braid accepts streams of measurements taken from the run-time environment or from within flow runs which can then be statistically aggregated and compared to other streams to determine a strategy to guide flow execution. for example, queue lengths in execution environments can be used to direct a flow to run computations in one environment or another, or experiment progress as measured by individual flows can be aggregated to determine the progress and subsequent direction of the flows within a fleet. we describe braid, its interface, implementation and performance characteristics. we further show through examples and experience modifying an existing scientific flow how braid is used to make adaptable flows.",2024-03-10
"57","hierarchical query classification in e-commerce search","bing he, sreyashi nag, limeng cui, suhang wang, zheng li, rahul goutam, zhen li, haiyang zhang","information retrieval","e-commerce platforms typically store and structure product information and search data in a hierarchy. efficiently categorizing user search queries into a similar hierarchical structure is paramount in enhancing user experience on e-commerce platforms as well as news curation and academic research. the significance of this task is amplified when dealing with sensitive query categorization or critical information dissemination, where inaccuracies can lead to considerable negative impacts. the inherent complexity of hierarchical query classification is compounded by two primary challenges: (1) the pronounced class imbalance that skews towards dominant categories, and (2) the inherent brevity and ambiguity of search queries that hinder accurate classification. to address these challenges, we introduce a novel framework that leverages hierarchical information through (i) enhanced representation learning that utilizes the contrastive loss to discern fine-grained instance relationships within the hierarchy, called ''instance hierarchy'', and (ii) a nuanced hierarchical classification loss that attends to the intrinsic label taxonomy, named ''label hierarchy''. additionally, based on our observation that certain unlabeled queries share typographical similarities with labeled queries, we propose a neighborhood-aware sampling technique to intelligently select these unlabeled queries to boost the classification performance. extensive experiments demonstrate that our proposed method is better than state-of-the-art (sota) on the proprietary amazon dataset, and comparable to sota on the public datasets of web of science and rcv1-v2. these results underscore the efficacy of our proposed solution, and pave the path toward the next generation of hierarchy-aware query classification systems.",2024-03-09
"58","rome/rea: three-year, tri-color timeseries photometry of the galactic bulge","r.a. street, e. bachelet, y. tsapras, m.p.g. hundertmark, v. bozza, d.m. bramich, a. cassan, m. dominik, r. figuera jaimes, k. horne, s. mao, a. saha, j. wambsganss, weicheng zang","astrophysics of galaxies","the rome/rea (robotic observations of microlensing events/reactive event assessment) survey was a key project at las cumbres observatory (hereafter lco) which continuously monitored 20 selected fields (3.76 sq.deg.) in the galactic bulge throughout their seasonal visibility window over a three-year period, between march 2017 and march 2020. observations were made in three optical passbands (sdss-g', -r', -i'), and lco's multi-site telescope network enabled the survey to achieve a typical cadence of $\sim$10\,hrs in i' and ~15 hrs in g' and r'. in addition, intervals of higher cadence (<1 hr) data were obtained during monitoring of key microlensing events within the fields. this paper describes the difference image analysis data reduction pipeline developed to process these data, and the process for combining the photometry from lco's three observing sites in the southern hemisphere. the full timeseries photometry for all 8 million stars, down to a limiting magnitude of i~18 mag is provided in the data release accompanying this paper, and samples of the data are presented for exemplar microlensing events, illustrating how the tri-band data are used to derive constraints on the microlensing source star parameters, a necessary step in determining the physical properties of the lensing object. the timeseries data also enables a wealth of additional science, for example in characterizing long-timescale stellar variability, and a few examples of the data for known variables are presented.",2024-03-09
"59","hamiltonicity, path cover, and independence number: an fpt perspective","fedor v. fomin, petr a. golovach, danil sagunov, kirill simonov","data structures and algorithms","the connection between hamiltonicity and the independence numbers of graphs has been a fundamental aspect of graph theory since the seminal works of the 1960s. this paper presents a novel algorithmic perspective on these classical problems. our contributions are twofold. first, we establish that a wide array of problems in undirected graphs, encompassing problems such as hamiltonian path and cycle, path cover, largest linkage, and topological minor containment are fixed-parameter tractable (fpt) parameterized by the independence number of a graph. to the best of our knowledge, these results mark the first instances of fpt problems for such parameterization. second, we extend the algorithmic scope of the gallai-milgram theorem. the original theorem by gallai and milgram, asserts that for a graph g with the independence number \alpha(g), the vertex set of g can be covered by at most \alpha(g) vertex-disjoint paths. we show that determining whether a graph can be covered by fewer than \alpha(g) - k vertex-disjoint paths is fpt parameterized by k. notably, the independence number parameterization, which describes graph's density, departs from the typical flow of research in parameterized complexity, which focuses on parameters describing graph's sparsity, like treewidth or vertex cover.",2024-03-09
"60","approximate bipartite $b$-matching using multiplicative auction","bhargav samineni, s m ferdous, mahantesh halappanavar, bala krishnamoorthy","data structures and algorithms","given a bipartite graph $g(v= (a \cup b),e)$ with $n$ vertices and $m$ edges and a function $b \colon v \to \mathbb{z}_+$, a $b$-matching is a subset of edges such that every vertex $v \in v$ is incident to at most $b(v)$ edges in the subset. when we are also given edge weights, the max weight $b$-matching problem is to find a $b$-matching of maximum weight, which is a fundamental combinatorial optimization problem with many applications. extending on the recent work of zheng and henzinger (ipco, 2023) on standard bipartite matching problems, we develop a simple auction algorithm to approximately solve max weight $b$-matching. specifically, we present a multiplicative auction algorithm that gives a $(1 - \varepsilon)$-approximation in $o(m \varepsilon^{-1} \log \varepsilon^{-1} \log \beta)$ worst case time, where $\beta$ the maximum $b$-value. although this is a $\log \beta$ factor greater than the current best approximation algorithm by huang and pettie (algorithmica, 2022), it is considerably simpler to present, analyze, and implement.",2024-03-09
"61","scalable $k$-clique densest subgraph search","xiaowei ye, miao qiao, rong-hua li, qi zhang, guoren wang","data structures and algorithms","in this paper, we present a collection of novel and scalable algorithms designed to tackle the challenges inherent in the $k$-clique densest subgraph problem (\kcdsp) within network analysis. we propose \psctl, a novel algorithm based on the frank-wolfe approach for addressing \kcdsp, effectively solving a distinct convex programming problem. \textcolor{black}{\psctl is able to approximate \kcdsp with near optimal guarantees.} the notable advantage of \psctl lies in its time complexity, which is independent of the count of $k$-cliques, resulting in remarkable efficiency in practical applications. additionally, we present \spath, a sampling-based algorithm with the capability to handle networks on an unprecedented scale, reaching up to $1.8\times 10^9$ edges. by leveraging the \ccpath algorithm as a uniform $k$-clique sampler, \spath ensures the efficient processing of large-scale network data, accompanied by a detailed analysis of accuracy guarantees. together, these contributions represent a significant advancement in the field of $k$-clique densest subgraph discovery. in experimental evaluations, our algorithms demonstrate orders of magnitude faster performance compared to the current state-of-the-art solutions.",2024-03-09
"62","systematic assessment of various universal machine-learning interatomic potentials","haochen yu, matteo giantomassi, giuliana materzanini, gian-marco rignanese","materials science","machine-learning interatomic potentials have revolutionized materials modeling at the atomic scale. thanks to these, it is now indeed possible to perform simulations of \abinitio quality over very large time and length scales. more recently, various universal machine-learning models have been proposed as an out-of-box approach avoiding the need to train and validate specific potentials for each particular material of interest. in this paper, we review and evaluate five different universal machine-learning interatomic potentials (umlips), all based on graph neural network architectures which have demonstrated transferability from one chemical system to another. the evaluation procedure relies on data both from a recent verification study of density-functional-theory implementations and from the materials project. through this comprehensive evaluation, we aim to provide guidance to materials scientists in selecting suitable models for their specific research problems, offer recommendations for model selection and optimization, and stimulate discussion on potential areas for improvement in current machine-learning methodologies in materials science.",2024-03-08
"63","$\mathtt{tsgt}$: stochastic time series modeling with transformer","łukasz kuciński, witold drzewakowski, mateusz olko, piotr kozakowski, łukasz maziarka, marta emilia nowakowska, łukasz kaiser, piotr miłoś","machine learning","time series methods are of fundamental importance in virtually any field of science that deals with temporally structured data. recently, there has been a surge of deterministic transformer models with time series-specific architectural biases. in this paper, we go in a different direction by introducing $\mathtt{tsgt}$, a stochastic time series model built on a general-purpose transformer architecture. we focus on using a well-known and theoretically justified rolling window backtesting and evaluation protocol. we show that $\mathtt{tsgt}$ outperforms the state-of-the-art models on mad and rmse, and surpasses its stochastic peers on ql and crps, on four commonly used datasets. we complement these results with a detailed analysis of $\mathtt{tsgt}$'s ability to model the data distribution and predict marginal quantile values.",2024-03-08
"64","dynamic convex hulls for simple paths","bruce brewer, gerth stølting brodal, haitao wang","computational geometry","we consider the planar dynamic convex hull problem. in the literature, solutions exist supporting the insertion and deletion of points in poly-logarithmic time and various queries on the convex hull of the current set of points in logarithmic time. if arbitrary insertion and deletion of points are allowed, constant time updates and fast queries are known to be impossible. this paper considers two restricted cases where worst-case constant time updates and logarithmic time queries are possible. we assume all updates are performed on a deque (double-ended queue) of points. the first case considers the monotonic path case, where all points are sorted in a given direction, say horizontally left-to-right, and only the leftmost and rightmost points can be inserted and deleted. the second case assumes that the points in the deque constitute a simple path. note that the monotone case is a special case of the simple path case. for both cases, we present solutions supporting deque insertions and deletions in worst-case constant time and standard queries on the convex hull of the points in $o(\log n)$ time, where $n$ is the number of points in the current point set. the convex hull of the current point set can be reported in $o(h+\log n)$ time, where $h$ is the number of edges of the convex hull. for the 1-sided monotone path case, where updates are only allowed on one side, the reporting time can be reduced to $o(h)$, and queries on the convex hull are supported in $o(\log h)$ time. all our time bounds are worst case. in addition, we prove lower bounds that match these time bounds, and thus our results are optimal. for a quick comparison, the previous best update bounds for the simple path problem were amortized $o(\log n)$ time by friedman, hershberger, and snoeyink [socg 1989].",2024-03-08
"65","a vlbi software correlator for fast radio transients","calvin leung, shion andrew, kiyoshi w. masui, charanjot brar, tomas cassanelli, shami chatterjee, victoria kaspi, kholoud khairy, adam e. lanman, mattias lazda, juan mena-parra, gavin noble, aaron b. pearlman, mubdi rahman, pranav sanghavi","instrumentation and methods for astrophysics","one major goal in fast radio burst science is to detect fast radio bursts (frbs) over a wide field of view without sacrificing the angular resolution required to pinpoint them to their host galaxies. wide-field detection and localization capabilities have already been demonstrated using connected-element interferometry; the chime/frb outriggers project will push this further using widefield cylindrical telescopes as widefield outriggers for very long baseline interferometry (vlbi). this paper describes an offline vlbi software correlator written in python for the chime/frb outriggers project. it includes features well-suited to modern widefield instruments like multibeaming/multiple phase center correlation, pulse gating including coherent dedispersion, and a novel correlation algorithm based on the quadratic estimator formalism. this algorithm mitigates sensitivity loss which arises in instruments where the windowing and channelization is done outside the vlbi correlator at each station, which accounts for a 30 percent sensitivity drop away from the phase center. our correlation algorithm recovers this sensitivity on both simulated and real data. as an end to end check of our software, we have written a preliminary pipeline for vlbi calibration and single-pulse localization, which we use in lanman et al. (2024) to verify the astrometric accuracy of the chime/frb outriggers array.",2024-03-08
"66","an intelligent assistive system based on augmented reality and internet of things for patients with alzheimer's disease","fatemeh ghorbani","human-computer interaction","independent life of the individuals suffering from alzheimer's disease (ad) is compromised due to their memory loss. as a result, they depend on others to help them lead their daily life. in this situation, either the family members or the caregivers offer their help; they attach notes on every single object or take out the contents of a drawer to make those visible when they leave the patient alone. the aim of this thesis is to provide multi-level support and some helping means for ad patients and their family members through the integration of existing science and methods. this study reports results on an intelligent assistive (ia) system, achieved through the integration of internet of things (iot), augmented reality (ar), and adaptive fuzzy decision-making methods. the proposed system has four main components; (1) a location and heading data stored in the local fog layer, (2) an ar device to make interactions with the ad patient, (3) a supervisory decision-maker to handle the direct and environmental interactions with the patient, (4) and a user interface for family or caregivers to monitor the patient's real-time situation and send reminders once required. the system operates in different modes, including automated and semi-automated. the first one helps the user complete the activities in their daily life by showing ar messages or making automatic changes. the second one allows manual changes after the real-time assessment of the user's cognitive state based on the ar game score. we provide further evidence that the accuracy, reliability and response time of the ia system are appropriate to be implemented in ad patients' homes. moreover, the system response in the semi-automated mode causes less data loss than the automated mode, as the number of active devices decreases.",2024-02-21
"67","understanding the progression of educational topics via semantic matching","tamador alkhidir (1), edmond awad (2), aamena alshamsi (3) ((1) curriculum department, ministry of education, united arab emirates, (2) department of economics and institute for data science and ai, university of exeter, united kingdom,(3) heuristic world, dubai, united arab emirates)","computers and society","education systems are dynamically changing to accommodate technological advances, industrial and societal needs, and to enhance students' learning journeys. curriculum specialists and educators constantly revise taught subjects across educational grades to identify gaps, introduce new learning topics, and enhance the learning outcomes. this process is usually done within the same subjects (e.g. math) or across related subjects (e.g. math and physics) considering the same and different educational levels, leading to massive multi-layer comparisons. having nuanced data about subjects, topics, and learning outcomes structured within a dataset, empowers us to leverage data science to better understand the progression of various learning topics. in this paper, bidirectional encoder representations from transformers (bert) topic modeling was used to extract topics from the curriculum, which were then used to identify relationships between subjects, track their progression, and identify conceptual gaps. we found that grouping learning outcomes by common topics helped specialists reduce redundancy and introduce new concepts in the curriculum. we built a dashboard to avail the methodology to curriculum specials. finally, we tested the validity of the approach with subject matter experts.",2024-02-10
"68","nurses as agents for achieving environmentally sustainable health systems: a bibliometric analysis","olga maria luque alcaraz, pilar aparicio-martínez, antonio gomera, manuel vaquero-abellán","computers and society","objective: to analyze the current scientific knowledge and research lines focused on environmentally sustainable health systems, including the role of nurses. background: there seem to be differences between creating interventions focused on environmentally sustainable health systems, including nurses, and the scarcity of research on this topic, framed on the sustainable development goals. methods: a bibliometric analysis was carried out, via three databases (web of science, scopus, and pubmed), and the guideline recommendations were followed to select bibliometric data. results: the search resulted in 159 publications, significantly increasing the trends from 2017 to 2021 (p=0.028). the most relevant countries in this area were the united states of america, the united kingdom, and sweden. also, the top articles were from relevant journals, indexed in journal citation report, and the first and the second quartile linked to the nursing field and citations (p<0.001). conclusion: education is key to achieving environmentally sustainable health systems via institutions and policies. implications for nursing management: there is a lack of experimental data and policies on achieving or maintaining environmentally sustainable health care systems, indicating that nurses have an important role and should be consulted and included in decision-making policies regarding sustainability in the healthcare systems.",2024-02-05
"69","the nirspec wide gto survey","michael v. maseda, anna de graaff, marijn franx, hans-walter rix, stefano carniani, isaac laseter, ugne dudzeviciute, tim rawle, eleonora parlanti, santiago arribas, andrew j. bunker, alex j. cameron, stephane charlot, mirko curti, francesco d'eugenio, gareth c. jones, nimisha kumari, roberto maiolino, hannah uebler, aayush saxena, renske smit, chris willott, joris witstok","astrophysics of galaxies","the near-infrared spectrograph (nirspec) on the james webb space telescope is uniquely suited to studying galaxies in the distant universe with its combination of multi-object capabilities and sensitivity over a large range in wavelength (0.6-5.3 microns). here we present the nirspec wide survey, part of the nirspec instrument science team's guaranteed time observations, using nirspec's microshutter array to obtain spectra of more than 3200 galaxies at $z>1$ at both low- and high-resolution ($r\approx100$ and 2700) for a total of 105 hours. with 31 pointings covering $\approx$320 arcmin$^2$ across the five candels fields with exquisite ancillary photometry from the hubble space telescope, the nirspec wide survey represents a fast and efficient way of using jwst to probe galaxies in the early universe. pointing centers are determined to maximize the observability of the rarest, high-value sources. subsequently, the microshutter configurations are optimized to observe the maximum number of ""census"" galaxies with a selection function based primarily on hst/f160w magnitude, photometric/slitless grism redshift, and predicted \ha\ flux tracing the bulk of the galaxy population at cosmic noon ($z_{\rm med}=2.0$). we present details on the survey strategy, the target selection, an outline of the motivating science cases, and discuss upcoming public data releases to the community.",2024-03-08
"70","sparse dynamic network reconstruction through l1-regularization of a lyapunov equation","ian xul belaustegui, marcela ordorica arango, román rossi-pool, naomi ehrich leonard, alessio franci","systems and control","an important problem in many areas of science is that of recovering interaction networks from simultaneous time-series of many interacting dynamical processes. a common approach is to use the elements of the correlation matrix or its inverse as proxies of the interaction strengths, but the reconstructed networks are necessarily undirected. transfer entropy methods have been proposed to reconstruct directed networks but the reconstructed network lacks information about interaction strengths. we propose a network reconstruction method that inherits the best of the two approaches by reconstructing a directed weighted network from noisy data under the assumption that the network is sparse and the dynamics are governed by a linear (or weakly-nonlinear) stochastic dynamical system. the two steps of our method are i) constructing an (infinite) family of candidate networks by solving the covariance matrix lyapunov equation for the state matrix and ii) using l1-regularization to select a sparse solution. we further show how to use prior information on the (non)existence of a few directed edges to drastically improve the quality of the reconstruction.",2024-03-08
"71","advances of deep learning in protein science: a comprehensive survey","bozhen hu, cheng tan, lirong wu, jiangbin zheng, jun xia, zhangyang gao, zicheng liu, fandi wu, guijun zhang, stan z. li","biomolecules","protein representation learning plays a crucial role in understanding the structure and function of proteins, which are essential biomolecules involved in various biological processes. in recent years, deep learning has emerged as a powerful tool for protein modeling due to its ability to learn complex patterns and representations from large-scale protein data. this comprehensive survey aims to provide an overview of the recent advances in deep learning techniques applied to protein science. the survey begins by introducing the developments of deep learning based protein models and emphasizes the importance of protein representation learning in drug discovery, protein engineering, and function annotation. it then delves into the fundamentals of deep learning, including convolutional neural networks, recurrent neural networks, attention models, and graph neural networks in modeling protein sequences, structures, and functions, and explores how these techniques can be used to extract meaningful features and capture intricate relationships within protein data. next, the survey presents various applications of deep learning in the field of proteins, including protein structure prediction, protein-protein interaction prediction, protein function prediction, etc. furthermore, it highlights the challenges and limitations of these deep learning techniques and also discusses potential solutions and future directions for overcoming these challenges. this comprehensive survey provides a valuable resource for researchers and practitioners in the field of proteins who are interested in harnessing the power of deep learning techniques. by consolidating the latest advancements and discussing potential avenues for improvement, this review contributes to the ongoing progress in protein research and paves the way for future breakthroughs in the field.",2024-03-08
"72","interoperability of the metaverse: a digital ecosystem perspective review","liang yang, shi-ting ni, yuyang wang, ao yu, jyh-an lee, pan hui","computers and society","the metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. however, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. interoperability, recognized as a major barrier to the metaverse's full potential, is central to this debate. coinmarketcap's report in february 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. despite consensus on its critical role, there is a research gap in exploring the impact on the metaverse, significance, and developmental extent. our study bridges this gap via a systematic literature review and content analysis of the web of science (wos) and scopus databases, yielding 74 publications after a rigorous selection process. interoperability, difficult to define due to varied contexts and lack of standardization, is central to the metaverse, often seen as a digital ecosystem. urs gasser's framework from harvard law school, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. incorporating this framework, we dissect literature for a comprehensive metaverse interoperability overview. our study seeks to establish benchmarks for future inquiries, navigating the complex field of metaverse interoperability studies and contributing to academic advancement.",2024-03-08
"73","efficient algorithms for personalized pagerank computation: a survey","mingji yang, hanzhi wang, zhewei wei, sibo wang, ji-rong wen","data structures and algorithms","personalized pagerank (ppr) is a traditional measure for node proximity on large graphs. for a pair of nodes $s$ and $t$, the ppr value $\pi_s(t)$ equals the probability that an $\alpha$-discounted random walk from $s$ terminates at $t$ and reflects the importance between $s$ and $t$ in a bidirectional way. as a generalization of google's celebrated pagerank centrality, ppr has been extensively studied and has found multifaceted applications in many fields, such as network analysis, graph mining, and graph machine learning. despite numerous studies devoted to ppr over the decades, efficient computation of ppr remains a challenging problem, and there is a dearth of systematic summaries and comparisons of existing algorithms. in this paper, we recap several frequently used techniques for ppr computation and conduct a comprehensive survey of various recent ppr algorithms from an algorithmic perspective. we classify these approaches based on the types of queries they address and review their methodologies and contributions. we also discuss some representative algorithms for computing ppr on dynamic graphs and in parallel or distributed environments.",2024-03-08
"74","putting language into context using smartphone-based keyboard logging","florian bemmann, timo koch, maximilian bergmann, clemens stachl, daniel buschek, ramona schoedel, sven mayer","human-computer interaction","while the study of language as typed on smartphones offers valuable insights, existing data collection methods often fall short in providing contextual information and ensuring user privacy. we present a privacy-respectful approach - context-enriched keyboard logging - that allows for the extraction of contextual information on the user's input motive, which is meaningful for linguistics, psychology, and behavioral sciences. in particular, with our approach, we enable distinguishing language contents by their channel (i.e., comments, messaging, search inputs). filtering by channel allows for better pre-selection of data, which is in the interest of researchers and improves users' privacy. we demonstrate our approach on a large-scale six-month user study (n=624) of language use in smartphone interactions in the wild. finally, we highlight the implications for research on language use in human-computer interaction and interdisciplinary contexts.",2024-03-08
"75","inverse design of photonic crystal surface emitting lasers is a sequence modeling problem","ceyao zhang, renjie li, cheng zhang, zhaoyu zhang, feng yin","applied physics","photonic crystal surface emitting lasers (pcsel)'s inverse design demands expert knowledge in physics, materials science, and quantum mechanics which is prohibitively labor-intensive. advanced ai technologies, especially reinforcement learning (rl), have emerged as a powerful tool to augment and accelerate this inverse design process. by modeling the inverse design of pcsel as a sequential decision-making problem, rl approaches can construct a satisfactory pcsel structure from scratch. however, the data inefficiency resulting from online interactions with precise and expensive simulation environments impedes the broader applicability of rl approaches. recently, sequential models, especially the transformer architecture, have exhibited compelling performance in sequential decision-making problems due to their simplicity and scalability to large language models. in this paper, we introduce a novel framework named pcsel inverse design transformer (pit) that abstracts the inverse design of pcsel as a sequence modeling problem. the central part of our pit is a transformer-based structure that leverages the past trajectories and current states to predict the current actions. compared with the traditional rl approaches, pit can output the optimal actions and achieve target pcsel designs by leveraging offline data and conditioning on the desired return. results demonstrate that pit achieves superior performance and data efficiency compared to baselines.",2024-03-08
"76","improving the open cluster census. iii. using cluster masses, radii, and dynamics to create a cleaned open cluster catalogue","emily l. hunt, sabine reffert","astrophysics of galaxies","the census of open clusters has exploded in size thanks to data from the gaia satellite. however, it is likely that many of these reported clusters are not gravitationally bound, making the open cluster census impractical for many scientific applications. we test different physically motivated methods for distinguishing between bound and unbound clusters, using them to create a cleaned cluster catalogue. we derived completeness-corrected photometric masses for 6956 clusters from our earlier work. then, we used these masses to compute the size of the roche surface of these clusters (their jacobi radius) and distinguish between bound and unbound clusters. we find that only 5647 (79%) of the clusters from our previous catalogue are compatible with bound open clusters, dropping to just 11% of clusters within 250 pc. 3530 open clusters are in a strongly cut high quality sample. the moving groups in our sample show different trends in their size as a function of age and mass, suggesting that they are unbound and undergoing different dynamical processes. our cluster mass measurements constitute the largest catalogue of milky way cluster masses to date, which we also use for further science. firstly, we inferred the mass-dependent completeness limit of the open cluster census, showing that the census is complete within 1.8 kpc only for objects heavier than 230 m$_\odot$. next, we derived a completeness-corrected age and mass function for our open cluster catalogue, including estimating that the milky way contains a total of $1.3 \times 10^5$ open clusters, only ~4% of which are currently known. finally, we show that most open clusters have mass functions compatible with the kroupa initial mass function. we demonstrate jacobi radii for distinguishing between bound and unbound star clusters, and publish an updated star cluster catalogue with masses and improved cluster classifications. (abridged)",2024-03-08
"77","single family algebra operation on zdds leads to exponential blow-up","kengo nakamura, masaaki nishino, shuhei denzumi","data structures and algorithms","zero-suppressed binary decision diagram (zdd) is a data structure to represent a family of (sub)sets compactly, and it can be used as a succinct index for a family of sets. to build zdd representing a desired family of sets, there are many transformation operations that take zdds as inputs and output zdd representing the resultant family after performing operations such as set union and intersection. however, except for some basic operations, the worst-time complexity of taking such transformation on zdds has not been extensively studied, and some contradictory statements about it have arisen in the literature. in this paper, we show that many transformation operations on zdds cannot be performed in worst-case polynomial time with respect to the size of input zdds. this refutes some of the folklore circulated in past literature and resolves an open problem raised by knuth. our results are stronger in that such blow-up of computational time occurs even when the ordering, which has a significant impact on the efficiency of treating zdds, is reasonable.",2024-03-08
"78","data-dependent lsh for the earth mover's distance","rajesh jayaram, erik waingarten, tian zhang","data structures and algorithms","we give new data-dependent locality sensitive hashing schemes (lsh) for the earth mover's distance ($\mathsf{emd}$), and as a result, improve the best approximation for nearest neighbor search under $\mathsf{emd}$ by a quadratic factor. here, the metric $\mathsf{emd}_s(\mathbb{r}^d,\ell_p)$ consists of sets of $s$ vectors in $\mathbb{r}^d$, and for any two sets $x,y$ of $s$ vectors the distance $\mathsf{emd}(x,y)$ is the minimum cost of a perfect matching between $x,y$, where the cost of matching two vectors is their $\ell_p$ distance. previously, andoni, indyk, and krauthgamer gave a (data-independent) locality-sensitive hashing scheme for $\mathsf{emd}_s(\mathbb{r}^d,\ell_p)$ when $p \in [1,2]$ with approximation $o(\log^2 s)$. by being data-dependent, we improve the approximation to $\tilde{o}(\log s)$. our main technical contribution is to show that for any distribution $\mu$ supported on the metric $\mathsf{emd}_s(\mathbb{r}^d, \ell_p)$, there exists a data-dependent lsh for dense regions of $\mu$ which achieves approximation $\tilde{o}(\log s)$, and that the data-independent lsh actually achieves a $\tilde{o}(\log s)$-approximation outside of those dense regions. finally, we show how to ""glue"" together these two hashing schemes without any additional loss in the approximation. beyond nearest neighbor search, our data-dependent lsh also gives optimal (distributional) sketches for the earth mover's distance. by known sketching lower bounds, this implies that our lsh is optimal (up to $\mathrm{poly}(\log \log s)$ factors) among those that collide close points with constant probability.",2024-03-08
"79","a basic lower bound for property testing","eldar fischer","data structures and algorithms","an $\epsilon$-test for any non-trivial property (one for which there are both satisfying inputs and inputs of large distance from the property) should use a number of queries that is at least inversely proportional in $\epsilon$. however, to the best of our knowledge there is no reference proof for this intuition. such a proof is provided here. it is written so as to not require any prior knowledge of the related literature, and in particular does not use yao's method.",2024-03-08
"80","np-completeness for the space-optimality of double-array tries","hideo bannai, keisuke goto, shunsuke kanda, dominik köppl","data structures and algorithms","indexing a set of strings for prefix search or membership queries is a fundamental task with many applications such as information retrieval or database systems. a classic abstract data type for modelling such an index is a trie. due to the fundamental nature of this problem, it has sparked much interest, leading to a variety of trie implementations with different characteristics. a trie implementation that has been well-used in practice is the double-array (trie) consisting of merely two integer arrays. while a traversal takes constant time per node visit, the needed space consumption in computer words can be as large as the product of the number of nodes and the alphabet size. despite that several heuristics have been proposed on lowering the space requirements, we are unaware of any theoretical guarantees. in this paper, we study the decision problem whether there exists a double-array of a given size. to this end, we first draw a connection to the sparse matrix compression problem, which makes our problem np-complete for alphabet sizes linear to the number of nodes. we further propose a reduction from the restricted directed hamiltonian path problem, leading to np-completeness even for logarithmic-sized alphabets.",2024-03-07
"81","a mixed-integer conic program for the moving-target traveling salesman problem based on a graph of convex sets","allen george philip, zhongqiang ren, sivakumar rathinam, howie choset","robotics","this paper introduces a new formulation that finds the optimum for the moving-target traveling salesman problem (mt-tsp), which seeks to find a shortest path for an agent, that starts at a depot, visits a set of moving targets exactly once within their assigned time-windows, and returns to the depot. the formulation relies on the key idea that when the targets move along lines, their trajectories become convex sets within the space-time coordinate system. the problem then reduces to finding the shortest path within a graph of convex sets, subject to some speed constraints. we compare our formulation with the current state-of-the-art mixed integer conic program (micp) solver for the mt-tsp. the experimental results show that our formulation outperforms the micp for instances with up to 20 targets, with up to two orders of magnitude reduction in runtime, and up to a 60\% tighter optimality gap. we also show that the solution cost from the convex relaxation of our formulation provides significantly tighter lower bounds for the mt-tsp than the ones from the micp.",2024-03-07
"82","towards robust data-driven automated recovery of symbolic conservation laws from limited data","tracey oellerich, maria emelianenko","numerical analysis","conservation laws are an inherent feature in many systems modeling real world phenomena, in particular, those modeling biological and chemical systems. if the form of the underlying dynamical system is known, linear algebra and algebraic geometry methods can be used to identify the conservation laws. our work focuses on using data-driven methods to identify the conservation law(s) in the absence of the knowledge of system dynamics. building in part upon the ideas proposed in [arxiv:1811.00961], we develop a robust data-driven computational framework that automates the process of identifying the number and type of the conservation law(s) while keeping the amount of required data to a minimum. we demonstrate that due to relative stability of singular vectors to noise we are able to reconstruct correct conservation laws without the need for excessive parameter tuning. while we focus primarily on biological examples, the framework proposed herein is suitable for a variety of data science applications and can be coupled with other machine learning approaches.",2024-03-07
"83","improved lower bound for differentially private facility location","pasin manurangsi","data structures and algorithms","we consider the differentially private (dp) facility location problem in the so called super-set output setting proposed by gupta et al. [soda 2010]. the current best known expected approximation ratio for an $\epsilon$-dp algorithm is $o\left(\frac{\log n}{\sqrt{\epsilon}}\right)$ due to cohen-addad et al. [aistats 2022] where $n$ denote the size of the metric space, meanwhile the best known lower bound is $\omega(1/\sqrt{\epsilon})$ [neurips 2019]. in this short note, we give a lower bound of $\tilde{\omega}\left(\min\left\{\log n, \sqrt{\frac{\log n}{\epsilon}}\right\}\right)$ on the expected approximation ratio of any $\epsilon$-dp algorithm, which is the first evidence that the approximation ratio has to grow with the size of the metric space.",2024-03-07
"84","corrective or backfire: characterizing and predicting user response to social correction","bing he, yingchen ma, mustaque ahamad, srijan kumar","social and information networks","online misinformation poses a global risk with harmful implications for society. ordinary social media users are known to actively reply to misinformation posts with counter-misinformation messages, which is shown to be effective in containing the spread of misinformation. such a practice is defined as ""social correction"". nevertheless, it remains unknown how users respond to social correction in real-world scenarios, especially, will it have a corrective or backfire effect on users. investigating this research question is pivotal for developing and refining strategies that maximize the efficacy of social correction initiatives. to fill this gap, we conduct an in-depth study to characterize and predict the user response to social correction in a data-driven manner through the lens of x (formerly twitter), where the user response is instantiated as the reply that is written toward a counter-misinformation message. particularly, we first create a novel dataset with 55, 549 triples of misinformation tweets, counter-misinformation replies, and responses to counter-misinformation replies, and then curate a taxonomy to illustrate different kinds of user responses. next, fine-grained statistical analysis of reply linguistic and engagement features as well as repliers' user attributes is conducted to illustrate the characteristics that are significant in determining whether a reply will have a corrective or backfire effect. finally, we build a user response prediction model to identify whether a social correction will be corrective, neutral, or have a backfire effect, which achieves a promising f1 score of 0.816. our work enables stakeholders to monitor and predict user responses effectively, thus guiding the use of social correction to maximize their corrective impact and minimize backfire effects. the code and data is accessible on this https url.",2024-03-07
"85","specifying and verifying the convergence stairs of the collatz program","ali ebnenasir","discrete mathematics","this paper presents an algorithmic method that, given a positive integer $j$, generates the $j$-th convergence stair containing all natural numbers from where the collatz conjecture holds by exactly $j$ applications of the collatz function. to this end, we present a novel formulation of the collatz conjecture as a concurrent program, and provide the general case specification of the $j$-th convergence stair for any $j > 0$. the proposed specifications provide a layered and linearized orientation of collatz numbers organized in an infinite set of infinite binary trees. to the best of our knowledge, this is the first time that such a general specification is provided, which can have significant applications in analyzing and testing the behaviors of complex non-linear systems. we have implemented this method as a software tool that generates the collatz numbers of individual stairs. we also show that starting from any value in any convergence stair the conjecture holds. however, to prove the conjecture, one has to show that every natural number will appear in some stair; i.e., the union of all stairs is equal to the set of natural numbers, which remains an open problem.",2024-02-29
"86","sq lower bounds for non-gaussian component analysis with weaker assumptions","ilias diakonikolas, daniel kane, lisheng ren, yuxin sun","machine learning","we study the complexity of non-gaussian component analysis (ngca) in the statistical query (sq) model. prior work developed a general methodology to prove sq lower bounds for this task that have been applicable to a wide range of contexts. in particular, it was known that for any univariate distribution $a$ satisfying certain conditions, distinguishing between a standard multivariate gaussian and a distribution that behaves like $a$ in a random hidden direction and like a standard gaussian in the orthogonal complement, is sq-hard. the required conditions were that (1) $a$ matches many low-order moments with the standard univariate gaussian, and (2) the chi-squared norm of $a$ with respect to the standard gaussian is finite. while the moment-matching condition is necessary for hardness, the chi-squared condition was only required for technical reasons. in this work, we establish that the latter condition is indeed not necessary. in particular, we prove near-optimal sq lower bounds for ngca under the moment-matching condition only. our result naturally generalizes to the setting of a hidden subspace. leveraging our general sq lower bound, we obtain near-optimal sq lower bounds for a range of concrete estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.",2024-03-07
"87","a sub-quadratic time algorithm for robust sparse mean estimation","ankit pensia","data structures and algorithms","we study the algorithmic problem of sparse mean estimation in the presence of adversarial outliers. specifically, the algorithm observes a \emph{corrupted} set of samples from $\mathcal{n}(\mu,\mathbf{i}_d)$, where the unknown mean $\mu \in \mathbb{r}^d$ is constrained to be $k$-sparse. a series of prior works has developed efficient algorithms for robust sparse mean estimation with sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2 \mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of contamination. in particular, the fastest runtime of existing algorithms is quadratic ($\omega(d^2)$), which can be prohibitive in high dimensions. this quadratic barrier in the runtime stems from the reliance of these algorithms on the sample covariance matrix, which is of size $d^2$. our main contribution is an algorithm for robust sparse mean estimation which runs in \emph{subquadratic} time using $\mathrm{poly}(k,\log d,1/\epsilon)$ samples. we also provide analogous results for robust sparse pca. our results build on algorithmic advances in detecting weak correlations, a generalized version of the light-bulb problem by valiant.",2024-03-07
"88","mmplace: robust place recognition with intermediate frequency signal of low-cost single-chip millimeter wave radar","chengzhen meng, yifan duan, chenming he, dequan wang, xiaoran fan, yanyong zhang","robotics","place recognition is crucial for tasks like loop-closure detection and re-localization. single-chip millimeter wave radar (single-chip radar in short) emerges as a low-cost sensor option for place recognition, with the advantage of insensitivity to degraded visual environments. however, it encounters two challenges. firstly, sparse point cloud from single-chip radar leads to poor performance when using current place recognition methods, which assume much denser data. secondly, its performance significantly declines in scenarios involving rotational and lateral variations, due to limited overlap in its field of view (fov). we propose mmplace, a robust place recognition system to address these challenges. specifically, mmplace transforms intermediate frequency (if) signal into range azimuth heatmap and employs a spatial encoder to extract features. additionally, to improve the performance in scenarios involving rotational and lateral variations, mmplace employs a rotating platform and concatenates heatmaps in a rotation cycle, effectively expanding the system's fov. we evaluate mmplace's performance on the millisonic dataset, which is collected on the university of science and technology of china (ustc) campus, the city roads surrounding the campus, and an underground parking garage. the results demonstrate that mmplace outperforms point cloud-based methods and achieves 87.37% recall@1 in scenarios involving rotational and lateral variations.",2024-03-07
"89","on $[1,2]$-domination in interval and circle graphs","mohsen alambardar meybodi, abolfazl poureidi","computational complexity","a subset $s$ of vertices in a graph $g=(v, e)$ is dominating set if each vertex in $v(g)\setminus s$ is adjacent to at least one vertex in $s$. chellali et al. in 2013, by restricting the number of neighbors in $s$ of a vertex outside $s$, introduced the concept of $[1,j]$-dominating set. a set $d \subseteq v$ of a graph $g = (v, e)$ is called $[1,j]$-dominating set of $g$ if every vertex not in $d$ has at least one neighbor and at most $j$ neighbors in $d$. the minimum $[1,j]$-domination problem is the problem of finding the minimum set $d$. given a positive integer $k$ and a graph $g = (v, e)$, the $[1,j]$-domination decision problem is to decide whether $g$ has $[1,j]$-dominating set of cardinality at most $k$. a polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic dominating set problem which is np-hard in split graphs. this result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. in this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $np$-complete.",2024-03-07
"90","time-aware projections: truly node-private graph statistics under continual observation","palak jain, adam smith, connor wagaman","data structures and algorithms","we describe the first algorithms that satisfy the standard notion of node-differential privacy in the continual release setting (i.e., without an assumed promise on input streams). previous work addresses node-private continual release by assuming an unenforced promise on the maximum degree in a graph; indeed, the algorithms from these works exhibit blatant privacy violations when the degree bound is not met. our algorithms are accurate on sparse graphs, for several fundamental graph problems: counting edges, triangles, other subgraphs, and connected components; and releasing degree histograms. our unconditionally private algorithms generally have optimal error, up to polylogarithmic factors and lower-order terms. we provide general transformations that take a base algorithm for the continual release setting, which need only be private for streams satisfying a promised degree bound, and produce an algorithm that is unconditionally private yet mimics the base algorithm when the stream meets the degree bound (and adds only linear overhead to the time and space complexity of the base algorithm). to do so, we design new projection algorithms for graph streams, based on the batch-model techniques of day et al. 2016 and blocki et al. 2013, which modify the stream to limit its degree. our main technical innovation is to show that the projections are stable -- meaning that similar input graphs have similar projections -- when the input stream satisfies a privately testable safety condition. our transformation then follows a novel online variant of the propose-test-release framework (dwork and lei, 2009), privately testing the safety condition before releasing output at each step.",2024-03-07
"91","optimizing inventory placement for a downstream online matching problem","boris epstein, will ma (columbia university)","data structures and algorithms","we study the inventory placement problem of splitting $q$ units of a single item across warehouses, in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. this is a challenging problem both in theory, because the downstream matching problem itself is computationally hard, and in practice, because the fulfillment team is constantly updating its algorithm and the placement team cannot directly evaluate how a placement decision would perform. we compare the performance of three placement procedures based on optimizing surrogate functions that have been studied and applied: offline, myopic, and fluid placement. on the theory side, we show that optimizing inventory placement for the offline surrogate leads to a $(1-(1-1/d)^d)/2$-approximation for the joint placement and fulfillment problem. we assume $d$ is an upper bound on how many warehouses can serve any demand location and that stochastic arrivals satisfy either temporal or spatial independence. the crux of our theoretical contribution is to use randomized rounding to derive a tight $(1-(1-1/d)^d)$-approximation for the integer programming problem of optimizing the offline surrogate. we use statistical learning to show that rounding after optimizing a sample-average offline surrogate, which is necessary due to the exponentially-sized support, does indeed have vanishing loss. on the experimental side, we extract real-world sequences of customer orders from publicly-available this http url data and evaluate different combinations of placement and fulfillment procedures. optimizing the offline surrogate performs best overall, even compared to simulation procedures, corroborating our theory.",2024-03-07
"92","algorithms and complexity for path covers of temporal dags: when is dilworth dynamic?","dibyayan chakraborty, antoine dailly, florent foucaud, ralf klasing","data structures and algorithms","in this paper, we study a dynamic analogue of the path cover problem, which can be solved in polynomial-time in directed acyclic graphs. a temporal digraph has an arc set that changes over discrete time-steps, if the underlying digraph (the union of all the arc sets) is acyclic, then we have a temporal dag. a temporal path is a directed path in the underlying digraph, such that the time-steps of arcs are strictly increasing along the path. two temporal paths are temporally disjoint if they do not occupy any vertex at the same time. a temporal (resp. temporally disjoint) path cover is a collection of (resp. temporally disjoint) temporal paths that covers all vertices. in this paper, we study the computational complexities of the problems of finding a temporal (disjoint) path cover with minimum cardinality, denoted as temporal path cover (tpc) and temporally disjoint path cover (td-pc). we show that both problems are np-hard even when the underlying dag is planar, bipartite, subcubic, and there are only two arc-disjoint time-steps. moreover, td-pc remains np-hard even on temporal oriented trees. in contrast, we show that tpc is polynomial-time solvable on temporal oriented trees by a reduction to clique cover for (static undirected) weakly chordal graphs (a subclass of perfect graphs for which clique cover admits an efficient algorithm). this highlights an interesting algorithmic difference between the two problems. although it is np-hard on temporal oriented trees, td-pc becomes polynomial-time solvable on temporal oriented lines and temporal rooted directed trees. we also show that tpc (resp. td-pc) admits an xp (resp. fpt) time algorithm with respect to parameter tmax + tw, where tmax is the maximum time-step, and tw is the treewidth of the underlying static undirected graph.",2024-03-07
"93","a simple and near-optimal algorithm for directed expander decompositions","aurelio l. sulser, maximilian probst gutenberg","data structures and algorithms","in this work, we present the first algorithm to compute expander decompositions in an $m$-edge directed graph with near-optimal time $\tilde{o}(m)$. further, our algorithm can maintain such a decomposition in a dynamic graph and again obtains near-optimal update times. our result improves over previous algorithms of bernstein-probst gutenberg-saranurak (focs 2020), hua-kyng-probst gutenberg-wu (soda 2023) that only obtained algorithms optimal up to subpolynomial factors. at the same time, our algorithm is much simpler and more accessible than previous work. in order to obtain our new algorithm, we present a new push-pull-relabel flow framework that generalizes the classic push-relabel flow algorithm of goldberg-tarjan (jacm 1988), which was later dynamized for computing expander decompositions in undirected graphs by henzinger-rao-wang (siam j. comput. 2020), saranurak-wang (soda 2019). we then show that the flow problems formulated in recent work of hua-kyng-probst gutenberg-wu (soda 2023) to decompose directed graphs can be solved much more efficiently in the push-pull-relabel flow framework.",2024-03-07
"94","conflict and fairness in resource allocation","susobhan bandopadhyay, aritra banik, sushmita gupta, pallavi jain, abhishek sahu, saket saurabh, prafullkumar tale","computer science and game theory","in the standard model of fair allocation of resources to agents, every agent has some utility for every resource, and the goal is to assign resources to agents so that the agents' welfare is maximized. motivated by job scheduling, interest in this problem dates back to the work of deuermeyer et al. [siam j. on algebraic discrete methods'82]. recent works consider the compatibility between resources and assign only mutually compatible resources to an agent. we study a fair allocation problem in which we are given a set of agents, a set of resources, a utility function for every agent over a set of resources, and a {\it conflict graph} on the set of resources (where an edge denotes incompatibility). the goal is to assign resources to the agents such that $(i)$ the set of resources allocated to an agent are compatible with each other, and $(ii)$ the minimum satisfaction of an agent is maximized, where the satisfaction of an agent is the sum of the utility of the assigned resources. chiarelli et al. [algorithmica'22] explore this problem from the classical complexity perspective to draw the boundary between the cases that are polynomial-time solvable and those that are \np-hard. in this article, we study the parameterized complexity of the problem (and its variants) by considering several natural and structural parameters.",2024-03-07
"95","switching classes: characterization and computation","dhanyamol antony, yixin cao, sagartanu pal, r.b. sandeep","data structures and algorithms","in a graph, the switching operation reverses adjacencies between a subset of vertices and the others. for a hereditary graph class $\mathcal{g}$, we are concerned with the maximum subclass and the minimum superclass of $\mathcal{g}$ that are closed under switching. we characterize the maximum subclass for many important classes $\mathcal{g}$, and prove that it is finite when $\mathcal{g}$ is minor-closed and omits at least one graph. for several graph classes, we develop polynomial-time algorithms to recognize the minimum superclass. we also show that the recognition of the superclass is np-complete for $h$-free graphs when $h$ is a sufficiently long path or cycle, and it cannot be solved in subexponential time assuming the exponential time hypothesis.",2024-03-07
"96","equivalence testing: the power of bounded adaptivity","diptarka chakraborty, sourav chakraborty, gunjan kumar, kuldeep s. meel","data structures and algorithms","equivalence testing, a fundamental problem in the field of distribution testing, seeks to infer if two unknown distributions on $[n]$ are the same or far apart in the total variation distance. conditional sampling has emerged as a powerful query model and has been investigated by theoreticians and practitioners alike, leading to the design of optimal algorithms albeit in a sequential setting (also referred to as adaptive tester). given the profound impact of parallel computing over the past decades, there has been a strong desire to design algorithms that enable high parallelization. despite significant algorithmic advancements over the last decade, parallelizable techniques (also termed non-adaptive testers) have $\tilde{o}(\log^{12}n)$ query complexity, a prohibitively large complexity to be of practical usage. therefore, the primary challenge is whether it is possible to design algorithms that enable high parallelization while achieving efficient query complexity. our work provides an affirmative answer to the aforementioned challenge: we present a highly parallelizable tester with a query complexity of $\tilde{o}(\log n)$, achieved through a single round of adaptivity, marking a significant stride towards harmonizing parallelizability and efficiency in equivalence testing.",2024-03-07
"97","fl-guard: a holistic framework for run-time detection and recovery of negative federated learning","hong lin, lidan shou, ke chen, gang chen, sai wu","machine learning","federated learning (fl) is a promising approach for learning a model from data distributed on massive clients without exposing data privacy. it works effectively in the ideal federation where clients share homogeneous data distribution and learning behavior. however, fl may fail to function appropriately when the federation is not ideal, amid an unhealthy state called negative federated learning (nfl), in which most clients gain no benefit from participating in fl. many studies have tried to address nfl. however, their solutions either (1) predetermine to prevent nfl in the entire learning life-cycle or (2) tackle nfl in the aftermath of numerous learning rounds. thus, they either (1) indiscriminately incur extra costs even if fl can perform well without such costs or (2) waste numerous learning rounds. additionally, none of the previous work takes into account the clients who may be unwilling/unable to follow the proposed nfl solutions when using those solutions to upgrade an fl system in use. this paper introduces fl-guard, a holistic framework that can be employed on any fl system for tackling nfl in a run-time paradigm. that is, to dynamically detect nfl at the early stage (tens of rounds) of learning and then to activate recovery measures when necessary. specifically, we devise a cost-effective nfl detection mechanism, which relies on an estimation of performance gain on clients. only when nfl is detected, we activate the nfl recovery process, in which each client learns in parallel an adapted model when training the global model. extensive experiment results confirm the effectiveness of fl-guard in detecting nfl and recovering from nfl to a healthy learning state. we also show that fl-guard is compatible with previous nfl solutions and robust against clients unwilling/unable to take any recovery measures.",2024-03-07
"98","optimal scheduling of graph states via path decompositions","samuel j. elman, jason gavriel, ryan l. mann","quantum physics","we study the optimal scheduling of graph states in measurement-based quantum computation, establishing an equivalence between measurement schedules and path decompositions of graphs. we define the spatial cost of a measurement schedule based on the number of simultaneously active qubits and prove that an optimal measurement schedule corresponds to a path decomposition of minimal width. our analysis shows that approximating the spatial cost of a graph is $\textsf{np}$-hard, while for graphs with bounded spatial cost, we establish an efficient algorithm for computing an optimal measurement schedule.",2024-03-07
"99","understanding biology in the age of artificial intelligence","elsa lawrence, adham el-shazly, srijit seal, chaitanya k joshi, pietro liò, shantanu singh, andreas bender, pietro sormanni, matthew greenig","artificial intelligence","modern life sciences research is increasingly relying on artificial intelligence approaches to model biological systems, primarily centered around the use of machine learning (ml) models. although ml is undeniably useful for identifying patterns in large, complex data sets, its widespread application in biological sciences represents a significant deviation from traditional methods of scientific inquiry. as such, the interplay between these models and scientific understanding in biology is a topic with important implications for the future of scientific research, yet it is a subject that has received little attention. here, we draw from an epistemological toolkit to contextualize recent applications of ml in biological sciences under modern philosophical theories of understanding, identifying general principles that can guide the design and application of ml systems to model biological phenomena and advance scientific knowledge. we propose that conceptions of scientific understanding as information compression, qualitative intelligibility, and dependency relation modelling provide a useful framework for interpreting ml-mediated understanding of biological systems. through a detailed analysis of two key application areas of ml in modern biological research - protein structure prediction and single cell rna-sequencing - we explore how these features have thus far enabled ml systems to advance scientific understanding of their target phenomena, how they may guide the development of future ml models, and the key obstacles that remain in preventing ml from achieving its potential as a tool for biological discovery. consideration of the epistemological features of ml applications in biology will improve the prospects of these methods to solve important problems and advance scientific understanding of living systems.",2024-03-06
"100","semi-supervised dialogue abstractive summarization via high-quality pseudolabel selection","jianfeng he, hang su, jason cai, igor shalyminov, hwanjun song, saab mansour","computation and language","semi-supervised dialogue summarization (ssds) leverages model-generated summaries to reduce reliance on human-labeled data and improve the performance of summarization models. while addressing label noise, previous works on semi-supervised learning primarily focus on natural language understanding tasks, assuming each sample has a unique label. however, these methods are not directly applicable to ssds, as it is a generative task, and each dialogue can be summarized in different ways. in this work, we propose a novel scoring approach, sicf, which encapsulates three primary dimensions of summarization model quality: semantic invariance (indicative of model confidence), coverage (factual recall), and faithfulness (factual precision). using the sicf score, we select unlabeled dialogues with high-quality generated summaries to train summarization models. comprehensive experiments on three public datasets demonstrate the effectiveness of sicf scores in uncertainty estimation and semi-supervised learning for dialogue summarization tasks. our code is available at \url{this https url}.",2024-03-06
"101","a sierpinski triangle data structure for efficient array value update and prefix sum calculation","brent harrison, jason necaise, andrew projansky, james d. whitfield","data structures and algorithms","the binary indexed tree, or fenwick tree, is a data structure that can efficiently update values and calculate prefix sums in an array. it allows both of these operations to be performed in $o(\log_2 n)$ time. here we present a novel data structure resembling the sierpinski triangle, which accomplishes these operations with the same memory usage in $o(\log_3 n)$ time instead. we show this order to be optimal by making use of a connection to quantum computing.",2024-03-06
"102","on outer bi-lipschitz extensions of linear johnson-lindenstrauss embeddings of subsets of $\mathbb{r}^n$","rafael chiclana, mark a. iwen, mark philip roach","metric geometry","the celebrated johnson-lindenstrauss lemma states that for all $\varepsilon \in (0,1)$ and finite sets $x \subseteq \mathbb{r}^n$ with $n>1$ elements, there exists a matrix $\phi \in \mathbb{r}^{m \times n}$ with $m=\mathcal{o}(\varepsilon^{-2}\log n)$ such that \[ (1 - \varepsilon) \|x-y\|_2 \leq \|\phi x-\phi y\|_2 \leq (1+\varepsilon)\| x- y\|_2 \quad \forall\, x, y \in x.\] herein we consider terminal embedding results which have recently been introduced in the computer science literature as stronger extensions of the johnson-lindenstrauss lemma for finite sets. after a short survey of this relatively recent line of work, we extend the theory of terminal embeddings to hold for arbitrary (e.g., infinite) subsets $x \subseteq \mathbb{r}^n$, and then specialize our generalized results to the case where $x$ is a low-dimensional compact submanifold of $\mathbb{r}^n$. in particular, we prove the following generalization of the johnson-lindenstrauss lemma: for all $\varepsilon \in (0,1)$ and $x\subseteq\mathbb{r}^n$, there exists a terminal embedding $f: \mathbb{r}^n \longrightarrow \mathbb{r}^{m}$ such that $$(1 - \varepsilon) \| x - y \|_2 \leq \left\| f(x) - f(y) \right\|_2 \leq (1 + \varepsilon) \| x - y \|_2 \quad \forall \, x \in x ~{\rm and}~ \forall \, y \in \mathbb{r}^n.$$ crucially, we show that the dimension $m$ of the range of $f$ above is optimal up to multiplicative constants, satisfying $m=\mathcal{o}(\varepsilon^{-2} \omega^2(s_x))$, where $\omega(s_x)$ is the gaussian width of the set of unit secants of $x$, $s_x=\overline{\{(x-y)/\|x-y\|_2 \colon x \neq y \in x\}}$. furthermore, our proofs are constructive and yield algorithms for computing a general class of terminal embeddings $f$, an instance of which is demonstrated herein to allow for more accurate compressive nearest neighbor classification than standard linear johnson-lindenstrauss embeddings do in practice.",2024-03-06
"103","on htlc-based protocols for multi-party cross-chain swaps","emily clark, chloe georgiou, katelyn poon, marek chrobak","data structures and algorithms","in his 2018 paper, herlihy introduced an atomic protocol for multi-party asset swaps across different blockchains. his model represents an asset swap by a directed graph whose nodes are the participating parties and edges represent asset transfers, and rational behavior of the participants is captured by a preference relation between a protocol's outcomes. asset transfers between parties are achieved using smart contracts. these smart contracts are quite involved and they require storage and processing of a large number of paths in the swap digraph, limiting practical significance of his protocol. his paper also describes a different protocol that uses only standard hash time-lock contracts (htlc's), but this simpler protocol applies only to some special types of digraphs. he left open the question whether there is a simple and efficient protocol for cross-chain asset swaps in arbitrary digraphs. motivated by this open problem, we conducted a comprehensive study of \emph{htlc-based protocols}, in which all asset transfers are implemented with htlcs. our main contribution is a full characterization of swap digraphs that have such protocols.",2024-03-06
"104","black-box $k$-to-$1$-pca reductions: theory and applications","arun jambulapati, syamantak kumar, jerry li, shourya pandey, ankit pensia, kevin tian","numerical analysis","the $k$-principal component analysis ($k$-pca) problem is a fundamental algorithmic primitive that is widely-used in data analysis and dimensionality reduction applications. in statistical settings, the goal of $k$-pca is to identify a top eigenspace of the covariance matrix of a distribution, which we only have implicit access to via samples. motivated by these implicit settings, we analyze black-box deflation methods as a framework for designing $k$-pca algorithms, where we model access to the unknown target matrix via a black-box $1$-pca oracle which returns an approximate top eigenvector, under two popular notions of approximation. despite being arguably the most natural reduction-based approach to $k$-pca algorithm design, such black-box methods, which recursively call a $1$-pca oracle $k$ times, were previously poorly-understood. our main contribution is significantly sharper bounds on the approximation parameter degradation of deflation methods for $k$-pca. for a quadratic form notion of approximation we term epca (energy pca), we show deflation methods suffer no parameter loss. for an alternative well-studied approximation notion we term cpca (correlation pca), we tightly characterize the parameter regimes where deflation methods are feasible. moreover, we show that in all feasible regimes, $k$-cpca deflation algorithms suffer no asymptotic parameter loss for any constant $k$. we apply our framework to obtain state-of-the-art $k$-pca algorithms robust to dataset contamination, improving prior work both in sample complexity and approximation quality.",2024-03-06
"105","digitality as a ""longue durèe"" historical phenomenon","salvatore spina","computers and society","the digital age introduced the digital ecological niche (den), revolutionizing human interactions. the advent of digital history (dhy) has marked a methodological shift in historical studies, tracing its roots to babbage and lovelace's 19th-century work on ""coding"" as a foundational communication process, fostering a new interaction paradigm between humans and machines, termed ""person2persons2machines."" this evolution, through digitization and informatization, builds upon ancient coding practices but was significantly advanced by babbage and lovelace's contributions to mathematical linguistic systems, laying the groundwork for computer science. this field, central to 20th-century mainframe interaction through programming languages and formalization, situates digital history within a broader historical context. here, coding and mathematical methodologies empower historians with advanced technologies for historical data preservation and analysis. nonetheless, the extent to which computation and turing machines can fully understand and interpret history remains a subject of debate.",2024-03-06
"106","parameterized algorithms for balanced cluster edge modification problems","jayakrishnan madathil, kitty meeks","data structures and algorithms","we introduce cluster edge modification problems with constraints on the size of the clusters and study their complexity. a graph $g$ is a cluster graph if every connected component of $g$ is a clique. in a typical cluster edge modification problem such as the widely studied cluster editing, we are given a graph $g$ and a non-negative integer $k$ as input, and we have to decide if we can turn $g$ into a cluster graph by way of at most $k$ edge modifications -- that is, by adding or deleting edges. in this paper, we study the parameterized complexity of such problems, but with an additional constraint: the size difference between any two connected components of the resulting cluster graph should not exceed a given threshold. depending on which modifications are permissible -- only adding edges, only deleting edges, both adding and deleting edges -- we have three different computational problems. we show that all three problems, when parameterized by $k$, admit single-exponential time fpt algorithms and polynomial kernels. our problems may be thought of as the size-constrained or balanced counterparts of the typical cluster edge modification problems, similar to the well-studied size-constrained or balanced counterparts of other clustering problems such as $k$-means clustering.",2024-03-06
"107","in the search of optimal tree networks: hardness and heuristics","maxim buzdalov, pavel martynov, sergey pankratov, vitaly aksenov, stefan schmid","networking and internet architecture","demand-aware communication networks are networks whose topology is optimized toward the traffic they need to serve. these networks have recently been enabled by novel optical communication technologies and are investigated intensively in the context of datacenters. in this work, we consider networks with one of the most common topologies~ -- a binary tree. we show that finding an optimal demand-aware binary tree network is np-hard. then, we propose optimization algorithms that generate efficient binary tree networks on real-life and synthetic workloads.",2024-03-06
"108","largest common subgraph of two forests","dieter rautenbach, florian werner","data structures and algorithms","a common subgraph of two graphs $g_1$ and $g_2$ is a graph that is isomorphic to subgraphs of $g_1$ and $g_2$. in the largest common subgraph problem the task is to determine a common subgraph for two given graphs $g_1$ and $g_2$ that is of maximum possible size ${\rm lcs}(g_1,g_2)$. this natural problem generalizes the well-studied graph isomorphism problem, has many applications, and remains np-hard even restricted to unions of paths. we present a simple $4$-approximation algorithm for forests, and, for every fixed $\epsilon\in (0,1)$, we show that, for two given forests $f_1$ and $f_2$ of order at most $n$, one can determine in polynomial time a common subgraph $f$ of $f_1$ and $f_2$ with at least ${\rm lcs}(f_1,f_2)-\epsilon n$ edges. restricted to instances with ${\rm lcs}(f_1,f_2)\geq cn$ for some fixed positive $c$, this yields a polynomial time approximation scheme. our approach relies on the approximation of the given forests by structurally simpler forests that are composed of copies of only $o(\log (n))$ different starlike rooted trees and iterative quantizations of the options for the solutions.",2024-03-06
"109","wildest dreams: reproducible research in privacy-preserving neural network training","tanveer khan, mindaugas budzys, khoa nguyen, antonis michalas","cryptography and security","machine learning (ml), addresses a multitude of complex issues in multiple disciplines, including social sciences, finance, and medical research. ml models require substantial computing power and are only as powerful as the data utilized. due to high computational cost of ml methods, data scientists frequently use machine learning-as-a-service (mlaas) to outsource computation to external servers. however, when working with private information, like financial data or health records, outsourcing the computation might result in privacy issues. recent advances in privacy-preserving techniques (ppts) have enabled ml training and inference over protected data through the use of privacy-preserving machine learning (ppml). however, these techniques are still at a preliminary stage and their application in real-world situations is demanding. in order to comprehend discrepancy between theoretical research suggestions and actual applications, this work examines the past and present of ppml, focusing on homomorphic encryption (he) and secure multi-party computation (smpc) applied to ml. this work primarily focuses on the ml model's training phase, where maintaining user data privacy is of utmost importance. we provide a solid theoretical background that eases the understanding of current approaches and their limitations. in addition, we present a sok of the most recent ppml frameworks for model training and provide a comprehensive comparison in terms of the unique properties and performances on standard benchmarks. also, we reproduce the results for some of the papers and examine at what level existing works in the field provide support for open science. we believe our work serves as a valuable contribution by raising awareness about the current gap between theoretical advancements and real-world applications in ppml, specifically regarding open-source availability, reproducibility, and usability.",2024-03-06
"110","studying ecg signals using nonlinear oscillators and genetic algorithm","sourav chowdhury, apratim ghosal, suparna roychowhury, indranath chaudhuri","medical physics","cardiovascular diseases are the leading cause of death and disability in the world and thus their detection is extremely important as early as possible so that it can be prognosed and managed appropriately. hence, electrophysiological models dealing with cardiac conduction are critically important in the field of interdisciplinary sciences. the primary aim of this paper is to reproduce a normal sinus rhythm ecg waveform which will act as the baseline for fitting and then fit any clinical ecg waveform that does not deviate much from normal sinus rhythm. to reproduce the ecg, we modeled the pacemaker complex using three coupled van der pol (vdp) oscillators with appropriate delays to generate the action potentials. these action potentials are responsible for the excitation of the non-pacemaker cells of the atria and ventricles whose electrical activity gets recorded as the ecg signal. the ecg signal is composed of a periodic set of individual waves corresponding to atrial and ventricular contraction and relaxation. these waves are modeled with the help of four fitzhugh-nagumo (fhn) equations with impulses corresponding to the action potentials generated by the pacemaker cells. after the successful reproduction of a normal sinus rhythm ecg, we have developed a framework where we have used genetic algorithm (ga) to fit a given clinical ecg data with parameters belonging to the above mentioned system of delay differential equations (ddes). the ga framework has enabled us to fit ecg data representing different cardiac conditions reasonably well. we aim to use this work to get a better understanding of the cardiac conduction system and cardiovascular diseases which will help humanity in the future.",2024-03-06
"111","graph visualization for blockchain data","marcell dietl, andre gemünd, daniel oeltz, felix m. thiele, christian werner","data structures and algorithms","in this report, we introduce a novel approach to visualize extremely large graphs efficiently. our method combines two force-directed algorithms, kamada-kawai and forceatlas2, to handle different graph components based on their node count. additionally, we suggest utilizing the fast multipole method to enhance the speed of forceatlas2. although initially designed for analyzing bitcoin transaction graphs, for which we present results here, this algorithm can also be applied to other crypto currency transaction graphs or graphs from diverse domains.",2024-03-06
"112","double exponential lower bound for telephone broadcast","prafullkumar tale","data structures and algorithms","consider the telephone broadcast problem in which an input is a connected graph $g$ on $n$ vertices, a source vertex $s \in v(g)$, and a positive integer $t$. the objective is to decide whether there is a broadcast protocol from $s$ that ensures that all the vertices of $g$ get the message in at most $t$ rounds. we consider the broadcast protocol where, in a round, any node aware of the message can forward it to at most one of its neighbors. as the number of nodes aware of the message can at most double at each round, for a non-trivial instance we have $n \le 2^t$. hence, the brute force algorithm that checks all the permutations of the vertices runs in time $2^{2^{\calo(t)}} \cdot n^{\calo(1)}$. as our first result, we prove this simple algorithm is the best possible in the following sense. telephone broadcast does not admit an algorithm running in time $2^{2^{o(t)}} \cdot n^{\calo(1)}$, unless the ð fails. to the best of our knowledge, this is only the fourth example of \np-complete problem that admits a double exponential lower bound when parameterized by the solution size. it also resolves the question by fomin, fraigniaud, and golovach [wg 2023]. in the same article, the authors asked whether the problem is \fpt\ when parameterized by the feedback vertex set number of the graph. we answer this question in the negative. telephone broadcast, when restricted to graphs of the feedback vertex number one, and hence treewidth of two, is \np-\complete. we find this a relatively rare example of problems that admit a polynomial-time algorithm on trees but is \np-\complete\ on graphs of treewidth two.",2024-03-06
"113","undergraduate data science education: who has the microphone and what are they saying?","mine dogucu, sinem demirci, harry bendekgey, federica zoe ricci, catalina m. medina","other statistics","the presence of data science has been profound in the scientific community in almost every discipline. an important part of the data science education expansion has been at the undergraduate level. we conducted a systematic literature review to (1) specify current evidence and knowledge gaps in undergraduate data science education and (2) inform policymakers and data science educators/practitioners about the present status of data science education research. the majority of the publications in data science education that met our search criteria were available open-access. our results indicate that data science education research lacks empirical data and reproducibility. not all disciplines contribute equally to the field of data science education. computer science and data science as a separate field emerge as the leading contributors to the literature. in contrast, fields such as statistics, mathematics, as well as other fields closely related to data science exhibit a limited presence in studies. we recommend that federal agencies and researchers 1) invest in empirical data science education research; 2) diversify research efforts to enrich the spectrum of types of studies; 3) encourage scholars in key data science fields that are currently underrepresented in the literature to contribute more to research and publications.",2024-03-06
"114","fine-grained privacy guarantees for coverage problems","laxman dhulipala, george z. li","data structures and algorithms","we introduce a new notion of neighboring databases for coverage problems such as max cover and set cover under differential privacy. in contrast to the standard privacy notion for these problems, which is analogous to node-privacy in graphs, our new definition gives a more fine-grained privacy guarantee, which is analogous to edge-privacy. we illustrate several scenarios of set cover and max cover where our privacy notion is desired one for the application. our main result is an $\epsilon$-edge differentially private algorithm for max cover which obtains an $(1-1/e-\eta,\tilde{o}(k/\epsilon))$-approximation with high probability. furthermore, we show that this result is nearly tight: we give a lower bound show that an additive error of $\omega(k/\epsilon)$ is necessary under edge-differential privacy. via group privacy properties, this implies a new algorithm for $\epsilon$-node differentially private max cover which obtains an $(1-1/e-\eta,\tilde{o}(fk/\epsilon))$-approximation, where $f$ is the maximum degree of an element in the set system. when $f\ll k$, this improves over the best known algorithm for max cover under pure (node) differential privacy, which obtains an $(1-1/e,\tilde{o}(k^2/\epsilon))$-approximation.",2024-03-05
"115","deep configuration performance learning: a systematic survey and taxonomy","jingzhi gong, tao chen","software engineering","performance is arguably the most crucial attribute that reflects the behavior of a configurable software system. however, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. as such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. in this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. our results summarize the key topics and statistics on how the configuration data is prepared; how the deep configuration performance learning model is built; how the model is evaluated and how they are exploited in different tasks related to software configuration. we also identify the good practice and the potentially problematic phenomena from the studies surveyed, together with insights on future opportunities for the field. to promote open science, all the raw results of this survey can be accessed at our repository: this https url.",2024-03-05
"116","maintaining light spanners via minimal updates","hadi khodabandeh, david eppstein","computational geometry","we study the problem of maintaining a lightweight bounded-degree $(1+\varepsilon)$-spanner of a dynamic point set in a $d$-dimensional euclidean space, where $\varepsilon>0$ and $d$ are arbitrary constants. in our fully-dynamic setting, points are allowed to be inserted as well as deleted, and our objective is to maintain a $(1+\varepsilon)$-spanner that has constant bounds on its maximum degree and its lightness (the ratio of its weight to that of the minimum spanning tree), while minimizing the recourse, which is the number of edges added or removed by each point insertion or deletion. we present a fully-dynamic algorithm that handles point insertion with amortized constant recourse and point deletion with amortized $o(\log\delta)$ recourse, where $\delta$ is the aspect ratio of the point set.",2024-03-05
"117","shuffling momentum gradient algorithm for convex optimization","trang h. tran, quoc tran-dinh, lam m. nguyen","optimization and control","the stochastic gradient descent method (sgd) and its stochastic variants have become methods of choice for solving finite-sum optimization problems arising from machine learning and data science thanks to their ability to handle large-scale applications and big datasets. in the last decades, researchers have made substantial effort to study the theoretical performance of sgd and its shuffling variants. however, only limited work has investigated its shuffling momentum variants, including shuffling heavy-ball momentum schemes for non-convex problems and nesterov's momentum for convex settings. in this work, we extend the analysis of the shuffling momentum gradient method developed in [tran et al (2021)] to both finite-sum convex and strongly convex optimization problems. we provide the first analysis of shuffling momentum-based methods for the strongly convex setting, attaining a convergence rate of $o(1/nt^2)$, where $n$ is the number of samples and $t$ is the number of training epochs. our analysis is a state-of-the-art, matching the best rates of existing shuffling stochastic gradient algorithms in the literature.",2024-03-05
"118","jovian sodium nebula and io plasma torus s$^+$ and brightnesses 2017 -- 2023: insights into volcanic vs. sublimation supply","jeffrey p. morgenthaler (1), carl a. schmidt (2), marissa f. vogt (1), nicholas m. schneider (3), max marconi (1) ((1) planetary science institute, (2) center for space physics boston university, (3) university of colorado, boulder)","earth and planetary astrophysics","we present first results derived from the largest collection of contemporaneously recorded jovian sodium nebula and io plasma torus (ipt) in [s ii] 673.1 nm images assembled to date. the data were recorded by the planetary science institute's io input/output observatory (ioio) and provide important context to io geologic and atmospheric studies as well as the juno mission and supporting observations. enhancements in the observed emission are common, typically lasting 1 -- 3 months, such that the average flux of material from io is determined by the enhancements, not any quiescent state. the enhancements are not seen at periodicities associated with modulation in solar insolation of io's surface, thus physical process(es) other than insolation-driven sublimation must ultimately drive the bulk of io's atmospheric escape. we suggest that geologic activity, likely involving volcanic plumes, drives escape.",2024-03-05
"119","the exchange problem","mohit garg, suneel sarswat","data structures and algorithms","auctions are widely used in exchanges to match buy and sell requests. once the buyers and sellers place their requests, the exchange determines how these requests are to be matched. the two most popular objectives used while determining the matching are maximizing volume at a uniform price and maximizing volume with dynamic pricing. in this work, we study the algorithmic complexity of the problems arising from these matching tasks. we present a linear time algorithm for uniform price matching which is an improvement over the previous algorithms that take $o(n\log n)$ time to match $n$ requests. for dynamic price matching, we establish a lower bound of $\omega(n \log n)$ on the running time, thereby proving that the currently known best algorithm is time-optimal.",2024-03-05
"120","cover edge-based novel triangle counting","david a. bader, fuhuan li, zhihui du, palina pauliuchenka, oliver alvarado rodriguez, anant gupta, sai sri vastav minnal, valmik nahata, anya ganeshan, ahmet gundogdu, jason lew","data structures and algorithms","listing and counting triangles in graphs is a key algorithmic kernel for network analyses, including community detection, clustering coefficients, k-trusses, and triangle centrality. in this paper, we propose the novel concept of a cover-edge set that can be used to find triangles more efficiently. leveraging the breadth-first search (bfs) method, we can quickly generate a compact cover-edge set. novel sequential and parallel triangle counting algorithms that employ cover-edge sets are presented. the novel sequential algorithm performs competitively with the fastest previous approaches on both real and synthetic graphs, such as those from the graph500 benchmark and the mit/amazon/ieee graph challenge. we implement 22 sequential algorithms for performance evaluation and comparison. at the same time, we employ openmp to parallelize 11 sequential algorithms, presenting an in-depth analysis of their parallel performance. furthermore, we develop a distributed parallel algorithm that can asymptotically reduce communication on massive graphs. in our estimate from massive-scale graph500 graphs, our distributed parallel algorithm can reduce the communication on a scale~36 graph by 1156x and on a scale~42 graph by 2368x. comprehensive experiments are conducted on the recently launched intel xeon 8480+ processor and shed light on how graph attributes, such as topology, diameter, and degree distribution, can affect the performance of these algorithms.",2024-03-05
"121","space complexity of euclidean clustering","xiaoyi zhu, yuxiang tian, lingxiao huang, zengfeng huang","computational geometry","the $(k, z)$-clustering problem in euclidean space $\mathbb{r}^d$ has been extensively studied. given the scale of data involved, compression methods for the euclidean $(k, z)$-clustering problem, such as data compression and dimension reduction, have received significant attention in the literature. however, the space complexity of the clustering problem, specifically, the number of bits required to compress the cost function within a multiplicative error $\varepsilon$, remains unclear in existing literature. this paper initiates the study of space complexity for euclidean $(k, z)$-clustering and offers both upper and lower bounds. our space bounds are nearly tight when $k$ is constant, indicating that storing a coreset, a well-known data compression approach, serves as the optimal compression scheme. furthermore, our lower bound result for $(k, z)$-clustering establishes a tight space bound of $\theta( n d )$ for terminal embedding, where $n$ represents the dataset size. our technical approach leverages new geometric insights for principal angles and discrepancy methods, which may hold independent interest.",2024-03-05
"122","hamiltonian property testing","andreas bluhm, matthias c. caro, aadil oufkir","quantum physics","locality is a fundamental feature of many physical time evolutions. assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown hamiltonian from access to the induced time evolution. however, no protocols to rigorously test whether an unknown hamiltonian is local were known. we investigate hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit hamiltonian $h$ is $k$-local or $\varepsilon$-far from all $k$-local hamiltonians, given access to the time evolution along $h$. first, we emphasize the importance of the chosen distance measure: with respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\omega}(2^n / \varepsilon)$, and even coherent testers need $\omega(2^{n/2})$ many queries and $\omega(2^{n/2}/\varepsilon)$ total evolution time. in contrast, when distances are measured according to the normalized frobenius norm, corresponding to an average-case distance, we give a sample-, time-, and computationally efficient incoherent hamiltonian locality testing algorithm based on randomized measurements. in fact, our procedure can be used to simultaneously test a wide class of hamiltonian properties beyond locality. finally, we prove that learning a general hamiltonian remains exponentially hard with this average-case distance, thereby establishing an exponential separation between hamiltonian testing and learning. our work initiates the study of property testing for quantum hamiltonians, demonstrating that a broad class of hamiltonian properties is efficiently testable even with limited quantum capabilities, and positioning hamiltonian testing as an independent area of research alongside hamiltonian learning.",2024-03-05
"123","improving the quality of individual-level online information tracking: challenges of existing approaches and introduction of a new content- and long-tail sensitive academic solution","silke adam, mykola makhortykh, michaela maier, viktor aigenseer, aleksandra urman, teresa gil lopez, clara christner, ernesto de león, roberto ulloa","computers and society","this article evaluates the quality of data collection in individual-level desktop information tracking used in the social sciences and shows that the existing approaches face sampling issues, validity issues due to the lack of content-level data and their disregard of the variety of devices and long-tail consumption patterns as well as transparency and privacy issues. to overcome some of these problems, the article introduces a new academic tracking solution, webtrack, an open source tracking tool maintained by a major european research institution. the design logic, the interfaces and the backend requirements for webtrack, followed by a detailed examination of strengths and weaknesses of the tool, are discussed. finally, using data from 1185 participants, the article empirically illustrates how an improvement in the data collection through webtrack leads to new innovative shifts in the processing of tracking data. as webtrack allows collecting the content people are exposed to on more than classical news platforms, we can strongly improve the detection of politics-related information consumption in tracking data with the application of automated content analysis compared to traditional approaches that rely on the list-based identification of news.",2024-03-05
"124","dynst: dynamic sparse training for resource-constrained spatio-temporal forecasting","hao wu, haomin wen, guibin zhang, yutong xia, kai wang, yuxuan liang, yu zheng, kun wang","artificial intelligence","the ever-increasing sensor service, though opening a precious path and providing a deluge of earth system data for deep-learning-oriented earth science, sadly introduce a daunting obstacle to their industrial level deployment. concretely, earth science systems rely heavily on the extensive deployment of sensors, however, the data collection from sensors is constrained by complex geographical and social factors, making it challenging to achieve comprehensive coverage and uniform deployment. to alleviate the obstacle, traditional approaches to sensor deployment utilize specific algorithms to design and deploy sensors. these methods dynamically adjust the activation times of sensors to optimize the detection process across each sub-region. regrettably, formulating an activation strategy generally based on historical observations and geographic characteristics, which make the methods and resultant models were neither simple nor practical. worse still, the complex technical design may ultimately lead to a model with weak generalizability. in this paper, we introduce for the first time the concept of spatio-temporal data dynamic sparse training and are committed to adaptively, dynamically filtering important sensor distributions. to our knowledge, this is the first proposal (termed dynst) of an industry-level deployment optimization concept at the data level. however, due to the existence of the temporal dimension, pruning of spatio-temporal data may lead to conflicts at different timestamps. to achieve this goal, we employ dynamic merge technology, along with ingenious dimensional mapping to mitigate potential impacts caused by the temporal aspect. during the training process, dynst utilize iterative pruning and sparse training, repeatedly identifying and dynamically removing sensor perception areas that contribute the least to future predictions.",2024-03-05
"125","citizen science and machine learning for research and nature conservation: the case of eurasian lynx, free-ranging rodents and insects","kinga skorupska, rafał stryjek, izabela wierzbowska, piotr bebas, maciej grzeszczuk, piotr gago, jarosław kowalski, maciej krzywicki, jagoda lazarek, wiesław kopeć","human-computer interaction","technology is increasingly used in nature reserves and national parks around the world to support conservation efforts. endangered species, such as the eurasian lynx (lynx lynx), are monitored by a network of automatic photo traps. yet, this method produces vast amounts of data, which needs to be prepared, analyzed and interpreted. therefore, researchers working in this area increasingly need support to process this incoming information. one opportunity is to seek support from volunteer citizen scientists who can help label the data, however, it is challenging to retain their interest. another way is to automate the process with image recognition using convolutional neural networks. during the panel, we will discuss considerations related to nature research and conservation as well as opportunities for the use of citizen science and machine learning to expedite the process of data preparation, labelling and analysis.",2024-03-05
"126","a note on high-probability analysis of algorithms with exponential, sub-gaussian, and general light tails","amit attia, tomer koren","machine learning","this short note describes a simple technique for analyzing probabilistic algorithms that rely on a light-tailed (but not necessarily bounded) source of randomization. we show that the analysis of such an algorithm can be reduced, in a black-box manner and with only a small loss in logarithmic factors, to an analysis of a simpler variant of the same algorithm that uses bounded random variables and often easier to analyze. this approach simultaneously applies to any light-tailed randomization, including exponential, sub-gaussian, and more general fast-decaying distributions, without needing to appeal to specialized concentration inequalities. analyses of a generalized azuma inequality and stochastic optimization with general light-tailed noise are provided to illustrate the technique.",2024-03-05
"127","hints: sensemaking on large collections of documents with hypergraph visualization and intelligent agents","sam yu-te lee, kwan-liu ma","human-computer interaction","sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. in this paper, we present hints, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates large language models (llms) as both a general nlp task solver and an intelligent agent. by leveraging the extraction capability of llms in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. the constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. the system further integrates an llm-based intelligent chatbot agent in the interface to facilitate sensemaking. to demonstrate the generalizability and effectiveness of the hints system, we present two case studies on different domains and a comparative user study. we report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. we find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. we discuss limitations and future work for combining interactive visualization and llms more profoundly to better support corpus analysis.",2024-03-05
"128","neural fractional differential equations","c. coelho, m. fernanda p. costa, l.l. ferrás","machine learning","fractional differential equations (fdes) are essential tools for modelling complex systems in science and engineering. they extend the traditional concepts of differentiation and integration to non-integer orders, enabling a more precise representation of processes characterised by non-local and memory-dependent behaviours. this property is useful in systems where variables do not respond to changes instantaneously, but instead exhibit a strong memory of past interactions. having this in mind, and drawing inspiration from neural ordinary differential equations (neural odes), we propose the neural fde, a novel deep neural network architecture that adjusts a fde to the dynamics of data. this work provides a comprehensive overview of the numerical method employed in neural fdes and the neural fde architecture. the numerical outcomes suggest that, despite being more computationally demanding, the neural fde may outperform the neural ode in modelling systems with memory or dependencies on past states, and it can effectively be applied to learn more intricate dynamical systems.",2024-03-05
"129","distributed openmp offloading of openmc on intel gpu max accelerators","yehonatan fridman, guy tamir, uri steinitz, gal oren","distributed, parallel, and cluster computing","monte carlo (mc) simulations play a pivotal role in diverse scientific and engineering domains, with applications ranging from nuclear physics to materials science. harnessing the computational power of high-performance computing (hpc) systems, especially graphics processing units (gpus), has become essential for accelerating mc simulations. this paper focuses on the adaptation and optimization of the openmc neutron and photon transport monte carlo code for intel gpus, specifically the intel data center max 1100 gpu (codename ponte vecchio, pvc), through distributed openmp offloading. building upon prior work by tramm j.r., et al. (2022), which laid the groundwork for gpu adaptation, our study meticulously extends the openmc code's capabilities to intel gpus. we present a comprehensive benchmarking and scaling analysis, comparing performance on intel max gpus to state-of-the-art cpu execution (intel xeon platinum 8480+ processor, codename 4th generation sapphire rapids). the results demonstrate a remarkable acceleration factor compared to cpu execution, showcasing the gpu-adapted code's superiority over its cpu counterpart as computational load increases.",2024-03-05
"130","dgap: efficient dynamic graph analysis on persistent memory","abdullah al raqibul islam, dong dai","data structures and algorithms","dynamic graphs, featuring continuously updated vertices and edges, have grown in importance for numerous real-world applications. to accommodate this, graph frameworks, particularly their internal data structures, must support both persistent graph updates and rapid graph analysis simultaneously, leading to complex designs to orchestrate `fast but volatile' and `persistent but slow' storage devices. emerging persistent memory technologies, such as optane dcpmm, offer a promising alternative to simplify the designs by providing data persistence, low latency, and high iops together. in light of this, we propose dgap, a framework for efficient dynamic graph analysis on persistent memory. unlike traditional dynamic graph frameworks, which combine multiple graph data structures (e.g., edge list or adjacency list) to achieve the required performance, dgap utilizes a single mutable compressed sparse row (csr) graph structure with new designs for persistent memory to construct the framework. specifically, dgap introduces a \textit{per-section edge log} to reduce write amplification on persistent memory; a \textit{per-thread undo log} to enable high-performance, crash-consistent rebalancing operations; and a data placement schema to minimize in-place updates on persistent memory. our extensive evaluation results demonstrate that dgap can achieve up to $3.2\times$ better graph update performance and up to $3.77\times$ better graph analysis performance compared to state-of-the-art dynamic graph frameworks for persistent memory, such as xpgraph, llama, and graphone.",2024-03-05
"131","the influence of validation data on logical and scientific interpretations of forensic expert opinions","steven p. lund, hari iyer","applications","forensic experts use specialized training and knowledge to enable other members of the judicial system to make better informed and more just decisions. factfinders, in particular, are tasked with judging how much weight to give to experts' reports and opinions. many references describe assessing evidential weight from the perspective of a forensic expert. some recognize that stakeholders are each responsible for evaluating their own weight of evidence. morris (1971, 1974, 1977) provided a general framework for recipients to update their own uncertainties after learning an expert's opinion. although this framework is normative under bayesian axioms and several forensic scholars advocate the use of bayesian reasoning, few resources describe its application in forensic science. this paper addresses this gap by examining how recipients can combine principles of science and bayesian reasoning to evaluate their own likelihood ratios for expert opinions. this exercise helps clarify how an expert's role depends on whether one envisions recipients to be logical and scientific or deferential. illustrative examples with an expert's opinion expressed as a categorical conclusion, likelihood ratio, or range of likelihood ratios, or with likelihood ratios from multiple experts, each reveal the importance and influence of validation data for logical recipients' interpretations.",2024-03-05
"132","algorithms for galois words: detection, factorization, and rotation","diptarama hendrian, dominik köppl, ryo yoshinaka, ayumi shinohara","data structures and algorithms","lyndon words are extensively studied in combinatorics on words -- they play a crucial role on upper bounding the number of runs a word can have [bannai+, siam j. comput.'17]. we can determine lyndon words, factorize a word into lyndon words in lexicographically decreasing order, and find the lyndon rotation of a word, all in linear time within constant additional working space. a recent research interest emerged from the question of what happens when we change the lexicographic order, which is at the heart of the definition of lyndon words. in particular, the alternating order, where the order of all odd positions becomes reversed, has been recently proposed. while a lyndon word is, among all its cyclic rotations, the smallest one with respect to the lexicographic order, a galois word exhibits the same property by exchanging the lexicographic order with the alternating order. unfortunately, this exchange has a large impact on the properties galois words exhibit, which makes it a nontrivial task to translate results from lyndon words to galois words. up until now, it has only been conjectured that linear-time algorithms with constant additional working space in the spirit of duval's algorithm are possible for computing the galois factorization or the galois rotation. here, we affirm this conjecture as follows. given a word $t$ of length $n$, we can determine whether $t$ is a galois word, in $o(n)$ time with constant additional working space. within the same complexities, we can also determine the galois rotation of $t$, and compute the galois factorization of $t$ online. the last result settles open problem~1 in [dolce et al., tcs'2019] for galois words.",2024-03-05
"133","on approximate fully-dynamic matching and online matrix-vector multiplication","yang p. liu","data structures and algorithms","we study connections between the problem of fully dynamic $(1-\epsilon)$-approximate maximum bipartite matching, and the dual $(1+\epsilon)$-approximate vertex cover problem, with the online matrix-vector ($\mathsf{omv}$) conjecture which has recently been used in several fine-grained hardness reductions. we prove that there is an online algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $n^{1-c}\epsilon^{-c}$ time for constants $c, c > 0$ for fully dynamic updates if and only if the $\mathsf{omv}$ conjecture is false. similarly, we prove that there is an online algorithm that maintains a $(1-\epsilon)$-approximate maximum matching in amortized $n^{1-c}\epsilon^{-c}$ time if and only if there is a nontrivial algorithm for another dynamic problem, which we call dynamic approximate $\mathsf{omv}$, that has seemingly no matching structure. this provides some evidence against achieving amortized sublinear update times for approximate fully dynamic matching and vertex cover. leveraging these connections, we obtain faster algorithms for approximate fully dynamic matching in both the online and offline settings. 1. we give a randomized algorithm that with high probability maintains a $(1-\epsilon)$-approximate bipartite matching and $(1+\epsilon)$-approximate vertex cover in fully dynamic graphs, in amortized $o(\epsilon^{-o(1)} \frac{n}{2^{\omega(\sqrt{\log n})}})$ update time. our algorithm leverages fast algorithms for $\mathsf{omv}$ due to larsen-williams [soda 2017]. 2. we give a randomized offline algorithm for $(1-\epsilon)$-approximate maximum matching with amortized runtime $o(n^{.58}\epsilon^{-o(1)})$ by using fast matrix multiplication, significantly improving over the runtimes achieved via online algorithms. we also give an offline algorithm that maintains a $(1+\epsilon)$-approximate vertex cover in amortized $o(n^{.723}\epsilon^{-o(1)})$ time.",2024-03-05
"134","born accessible data science and visualization courses: challenges of developing curriculum to be taught by blind instructors to blind students","jooyoung seo, sile o'modhrain, yilin xia, sanchita kamath, bongshin lee, james m. coughlan","human-computer interaction","while recent years have seen a growing interest in accessible visualization tools and techniques for blind people, little attention is paid to the learning opportunities and teaching strategies of data science and visualization tailored for blind individuals. whereas the former focuses on the accessibility issues of data visualization tools, the latter is concerned with the learnability of concepts and skills for data science and visualization. in this paper, we present novel approaches to teaching data science and visualization to blind students in an online setting. taught by blind instructors, nine blind learners having a wide range of professional backgrounds participated in a two-week summer course. we describe the course design, teaching strategies, and learning outcomes. we also discuss the challenges and opportunities of teaching data science and visualization to blind students. our work contributes to the growing body of knowledge on accessible data science and visualization education, and provides insights into the design of online courses for blind students.",2024-03-05
"135","a tutorial on the pretrain-finetune paradigm for natural language processing","yu wang","computation and language","the pretrain-finetune paradigm represents a transformative approach in natural language processing (nlp). this paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. this efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. we first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. we demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. to this end, we have provided open access to all our code and datasets. the tutorial is particularly valuable for quantitative researchers in psychology, offering them an insightful guide into this innovative approach.",2024-03-04
"136","end-to-end variational quantum sensing","benjamin maclellan, piotr roztocki, stefanie czischek, roger g. melko","quantum physics","harnessing quantum correlations can enable sensing beyond the classical limits of precision, with the realization of such sensors poised for transformative impacts across science and engineering. real devices, however, face the accumulated impacts of noise effects, architecture constraints, and finite sampling rates, making the design and success of practical quantum sensors challenging. numerical and theoretical frameworks that support the optimization and analysis of imperfections from one end of a sensing protocol through to the other (i.e., from probe state preparation through to parameter estimation) are thus crucial for translating quantum advantage into widespread practice. here, we present an end-to-end variational framework for quantum sensing protocols, where parameterized quantum circuits and neural networks form trainable, adaptive models for quantum sensor dynamics and estimation, respectively. the framework is general and can be adapted towards arbitrary qubit architectures, as we demonstrate with experimentally-relevant ansätze for trapped-ion and photonic systems, and enables to directly quantify the impacts that noisy state preparation/measurement and finite data sampling have on parameter estimation. end-to-end variational frameworks can thus underpin powerful design and analysis tools for realizing quantum advantage in practical, robust sensors.",2024-03-04
"137","nimcso: a nim package for compositional space optimization","adam m. krajewski, arindam debnath, wesley f. reinhart, allison m. beese, zi-kui liu","materials science","nimcso is a high-performance tool implementing several methods for selecting components (data dimensions) in compositional datasets, which optimize the data availability and density for applications such as machine learning. making said choice is a combinatorically hard problem for complex compositions existing in highly dimensional spaces due to the interdependency of components being present. such spaces are encountered, for instance, in materials science, where datasets on compositionally complex materials (ccms) often span 20-45 chemical elements, 5-10 processing types, and several temperature regimes, for up to 60 total data dimensions. at its core, nimcso leverages the metaprogramming ability of the nim language (this http url) to optimize itself at the compile time, both in terms of speed and memory handling, to the specific problem statement and dataset at hand based on a human-readable configuration file. as demonstrated in this paper, nimcso reaches the physical limits of the hardware (l1 cache latency) and can outperform an efficient native python implementation over 400 times in terms of speed and 50 times in terms of memory usage (not counting interpreter), while also outperforming numpy implementation 35 and 17 times, respectively, when checking a candidate solution. it is designed to be both (1) a user-ready tool, implementing two efficient brute-force approaches (for handling up to 25 dimensions), a custom search algorithm (for up to 40 dimensions), and a genetic algorithm (for any dimensionality), and (2) a scaffold for building even more elaborate methods in the future, including heuristics going beyond data availability. all configuration is done with a simple human-readable yaml config file and plain text data files, making it easy to modify the search method and its parameters with no knowledge of programming and only basic command line skills.",2024-03-04
"138","contract design for pandora's box","martin hoefer, conrad schecker, kevin schewior","computer science and game theory","we study a natural application of contract design to search problems with probabilistic prior and exploration costs. these problems have a plethora of applications and are expressed concisely within the pandora's box model. its optimal solution is the ingenious index policy proposed originally by weitzman in 1979. in our principal-agent setting, the search task is delegated to an agent. the agent performs a sequential exploration of $n$ boxes, suffers the exploration cost for each inspected box, and selects the content (called the prize) of one inspected box as outcome. agent and principal obtain an individual value based on the selected prize. to influence the search, the principal a-priori designs a contract with a non-negative payment to the agent for each potential prize. the goal of the principal to maximize her expected reward, i.e., value minus payment. we show how to compute optimal contracts for the principal in several scenarios. a popular and important subclass are linear contracts, and we show how to compute optimal linear contracts in polynomial time. for general contracts, we consider the standard assumption that the agent suffers cost but obtains value only from the transfers by the principal. interestingly, a suitable adaptation of the index policy results in an optimal contract here. more generally, for general contracts with non-zero agent values for outcomes we show how to compute an optimal contract in two cases: (1) when each box has only one prize with non-zero value for principal and agent, (2) for i.i.d. boxes with a single prize with positive value for the principal. these results show that optimal contracts can be highly non-trivial, and their design goes significantly beyond the application or re-interpretation of the index policy.",2024-03-04
"139","statistical query lower bounds for learning truncated gaussians","ilias diakonikolas, daniel m. kane, thanasis pittas, nikos zarifis","data structures and algorithms","we study the problem of estimating the mean of an identity covariance gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{c}$ of sets. specifically, for a fixed but unknown truncation set $s \subseteq \mathbb{r}^d$, we are given access to samples from the distribution $\mathcal{n}(\boldsymbol{ \mu}, \mathbf{ i})$ truncated to the set $s$. the goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. our main result is a statistical query (sq) lower bound suggesting a super-polynomial information-computation gap for this task. in more detail, we show that the complexity of any sq algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{c}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. concretely, our sq lower bound applies when $\mathcal{c}$ is a union of a bounded number of rectangles whose vc dimension and gaussian surface are small. as a corollary of our construction, it also follows that the complexity of the previously known algorithm for this task is qualitatively best possible.",2024-03-04
"140","astronomy in colombia: a bibliometric perspective","sofía guevara-montoya, felipe ortiz-ferreira, maría paula silva-arévalo, paola a. niño-muñoz, jaime e. forero-romero","digital libraries","in colombia, astronomical research is experiencing accelerated growth. in order to better understand its evolution and current state, we conducted a bibliometric study using data from the astrophysics data system (ads) and web of science (wos). in ads, we identified 422 peer-reviewed publications from 1980, the year of the first publication, until 2023, which was the cutoff date for our study. among the 25 colombian institutions identified as participants in at least one publication, the contributions of four universities stand out: universidad de los andes, universidad nacional de colombia, universidad industrial de santander, and universidad de antioquia, with 104, 78, 68, and 67 publications, respectively. by cross-referencing information from ads and wos, we found that the areas with the greatest impact in publications are threefold: high-energy and fundamental physics, stars and stellar physics, and galaxies and cosmology. globally, according to wos, colombia ranks 52nd in the number of peer-reviewed publications between 2019 and 2023, and fifth in latin america. additionally, we identified three highly cited publications (top 1% worldwide) belonging to the field of observational cosmology. when analyzing countries with equal or greater bibliographic production, we estimate that colombian production is approximately four times lower than expected considering its population and gdp.",2024-03-04
"141","constraint satisfaction problems with advice","suprovat ghoshal, konstantin makarychev, yury makarychev","data structures and algorithms","we initiate the study of algorithms for constraint satisfaction problems with ml oracle advice. we introduce two models of advice and then design an approximation algorithm for max cut and max 2-lin in these models.",2024-03-04
"142","matching algorithms in the sparse stochastic block model","anna brandenberger, byron chin, nathan s. sheffield, divya shyamal","data structures and algorithms","the stochastic block model (sbm) is a generalization of the erdős--rényi model of random graphs that describes the interaction of a finite number of distinct communities. in sparse erdős--rényi graphs, it is known that a linear-time algorithm of karp and sipser achieves near-optimal matching sizes asymptotically almost surely, giving a law-of-large numbers for the matching sizes of such graphs in terms of solutions to an ode. we provide an extension of this analysis, identifying broad ranges of stochastic block model parameters for which the karp--sipser algorithm achieves near-optimal matching sizes, but demonstrating that it cannot perform optimally on general sbm instances. we also consider the problem of constructing a matching online, in which the vertices of one half of a bipartite stochastic block model arrive one-at-a-time, and must be matched as they arrive. we show that the competitive ratio lower bound of 0.837 found by mastin and jaillet for the erdős--rényi case is tight whenever the expected degrees in all communities are equal. we propose several linear-time algorithms for online matching in the general stochastic block model, but prove that despite very good experimental performance, none of these achieve online asymptotic optimality.",2024-03-04
"143","bipartite graph variational auto-encoder with fair latent representation to account for sampling bias in ecological networks","emre anakok, pierre barbillon, colin fontaine, elisa thebault","machine learning","we propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. we adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. we translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. by incorporating the hilbert-schmidt independence criterion (hsic) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. finally, we show how our approach can change our understanding of ecological networks when applied to the spipoll data set, a citizen science monitoring program of plant-pollinator interactions to which many observers contribute, making it prone to sampling bias.",2024-03-04
"144","faster mem-finding in $o (r + \bar{r} + g)$ space","travis gagie","data structures and algorithms","suppose we are given a text $t [1..n]$, a straight-line program with $g$ rules for $t$ and an assignment of tags to the characters in $t$ such that the burrows-wheeler transform of $t$ has $r$ runs, the burrows-wheeler transform of the reverse of $t$ has $\bar{r}$ runs and the tag array -- the list of tags in the lexicographic order of the suffixes starting at the characters the tags are assigned to -- has $t$ runs. if the alphabet size is at most polylogarithmic in $n$ then there is an $o (r + \bar{r} + g + t)$-space index for $t$ such that when we are given a pattern $p [1..m]$ we can compute the maximal exact matches (mems) of $p$ with respect to $t$ in $o (m)$ time plus $o (\log n)$ time per mem and then list the distinct tags assigned to the first characters of occurrences of that mem in constant time per tag listed, all correctly with high probability.",2024-03-04
"145","ab initio path integral monte carlo simulations of warm dense two-component systems without fixed nodes: structural properties","tobias dornheim, sebastian schwalbe, maximilian böhme, zhandos moldabekov, jan vorberger, panagiotis tolias","computational physics","we present extensive new \emph{ab initio} path integral monte carlo (pimc) results for a variety of structural properties of warm dense hydrogen and beryllium. to deal with the fermion sign problem -- an exponential computational bottleneck due to the antisymmetry of the electronic thermal density matrix -- we employ the recently proposed [\textit{j.~chem.~phys.}~\textbf{157}, 094112 (2022); \textbf{159}, 164113 (2023)] $\xi$-extrapolation method and find excellent agreement with exact direct pimc reference data where available. this opens up the intriguing possibility to study a gamut of properties of light elements and potentially material mixtures over a substantial part of the warm dense matter regime, with direct relevance for astrophysics, material science, and inertial confinement fusion research.",2024-03-04
"146","towards deterministic algorithms for constant-depth factors of constant-depth circuits","mrinal kumar, varun ramanathan, ramprasad saptharishi, ben lee volk","computational complexity","we design a deterministic subexponential time algorithm that takes as input a multivariate polynomial $f$ computed by a constant-depth circuit over rational numbers, and outputs a list $l$ of circuits (of unbounded depth and possibly with division gates) that contains all irreducible factors of $f$ computable by constant-depth circuits. this list $l$ might also include circuits that are spurious: they either do not correspond to factors of $f$ or are not even well-defined, e.g. the input to a division gate is a sub-circuit that computes the identically zero polynomial. the key technical ingredient of our algorithm is a notion of the pseudo-resultant of $f$ and a factor $g$, which serves as a proxy for the resultant of $g$ and $f/g$, with the advantage that the circuit complexity of the pseudo-resultant is comparable to that of the circuit complexity of $f$ and $g$. this notion, which might be of independent interest, together with the recent results of limaye, srinivasan and tavenas, helps us derandomize one key step of multivariate polynomial factorization algorithms - that of deterministically finding a good starting point for newton iteration for the case when the input polynomial as well as the irreducible factor of interest have small constant-depth circuits.",2024-03-04
"147","random generation of git graphs","julien courtiel (greyc), martin pépin (greyc)","data structures and algorithms","version control systems, such as git and mercurial, manage the history of a project as a directed acyclic graph encoding the various divergences and synchronizations happening in its life cycle. a popular workflow in the industry, called the feature branch workflow, constrains these graphs to be of a particular shape: a unique main branch, and non-interfering feature branches. here we focus on the uniform random generation of those graphs with n vertices, including k on the main branch, for which we provide three algorithms, for three different use-cases. the first, based on rejection, is efficient when aiming for small values of k (more precisely whenever k = o($\sqrt$ n)). the second takes as input any number k of commits in the main branch, but requires costly precalculation. the last one is a boltzmann generator and enables us to generate very large graphs while targeting a constant k/n ratio. all these algorithms are linear in the size of their outputs.",2024-03-04
"148","the canadian traveller problem on outerplanar graphs","laurent beaudou, pierre bergé, vsevolod chernyshev, antoine dailly, yan gerard, aurélie lagoutte, vincent limouzy, lucas pastor","data structures and algorithms","we study the pspace-complete $k$-canadian traveller problem, where a weighted graph $g=(v,e,\omega)$ with a source $s\in v$ and a target $t\in v$ are given. this problem also has a hidden input $e_* \subsetneq e$ of cardinality at most $k$ representing blocked edges. the objective is to travel from $s$ to $t$ with the minimum distance. at the beginning of the walk, the blockages $e_*$ are unknown: the traveller discovers that an edge is blocked when visiting one of its endpoints. online algorithms, also called strategies, have been proposed for this problem and assessed with the competitive ratio, i.e. the ratio between the distance actually traversed by the traveller divided by the distance we would have traversed knowing the blockages in advance. even though the optimal competitive ratio is $2k+1$ even on unit-weighted planar graphs of treewidth 2, we design a polynomial-time strategy achieving competitive ratio $9$ on unit-weighted outerplanar graphs. this value $9$ also stands as a lower bound for this family of graphs as we prove that, for any $\varepsilon > 0$, no strategy can achieve a competitive ratio $9-\varepsilon$. finally, we show that it is not possible to achieve a constant competitive ratio (independent of $g$ and $k$) on weighted outerplanar graphs.",2024-03-04
"149","schema-based query optimisation for graph databases","chandan sharma (tyrex), pierre genevès (tyrex), nils gesbert (tyrex), nabil layaïda (tyrex)","databases","recursive graph queries are increasingly popular for extracting information from interconnected data found in various domains such as social networks, life sciences, and business analytics. graph data often come with schema information that describe how nodes and edges are organized. we propose a type inference mechanism that enriches recursive graph queries with relevant structural information contained in a graph schema. we show that this schema information can be useful in order to improve the performance when evaluating acylic recursive graph queries. furthermore, we prove that the proposed method is sound and complete, ensuring that the semantics of the query is preserved during the schema-enrichment process.",2024-03-04
"150","fully polynomial-time algorithms parameterized by vertex integrity using fast matrix multiplication","matthias bentert, klaus heeger, tomohiro koana","data structures and algorithms","we study the computational complexity of several polynomial-time-solvable graph problems parameterized by vertex integrity, a measure of a graph's vulnerability to vertex removal in terms of connectivity. vertex integrity is the smallest number $\iota$ such that there is a set $s$ of $\iota' \le \iota$ vertices such that every connected component of $g-s$ contains at most $\iota-\iota'$ vertices. it is known that the vertex integrity lies between the well-studied parameters vertex cover number and tree-depth. alon and yuster [esa 2007] designed algorithms for graphs with small vertex cover number using fast matrix multiplications. we demonstrate that fast matrix multiplication can also be effectively used when parameterizing by vertex integrity $\iota$ by developing efficient algorithms for problems including an $o(\iota^{\omega-1}n)$-time algorithm for computing the girth of a graph, randomized $o(\iota^{\omega - 1}n)$-time algorithms for maximum matching and for finding any induced four-vertex subgraph except for a clique or an independent set, and an $o(\iota^{(\omega-1)/2}n^2) \subseteq o(\iota^{0.687} n^2)$-time algorithm for all-pairs shortest paths. these algorithms can be faster than previous algorithms parameterized by tree-depth, for which fast matrix multiplication is not known to be effective.",2024-03-04
